{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 4: Superposition-Enhanced Experience Replay\n",
    "\n",
    "**Notebook:** `04_superposition_replay.ipynb`  \n",
    "**Phase:** 4 of 9  \n",
    "**Purpose:** Implement quantum superposition-inspired experience replay for improved sample efficiency  \n",
    "**Author:** Saurabh Jalendra  \n",
    "**Institution:** BITS Pilani (WILP Division)  \n",
    "**Date:** November 2025\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Setup & Imports](#1-setup--imports)\n",
    "2. [Superposition Concept](#2-superposition-concept)\n",
    "3. [Amplitude-Based Weighting](#3-amplitude-based-weighting)\n",
    "4. [Interference Effects](#4-interference-effects)\n",
    "5. [Superposition Replay Buffer](#5-superposition-replay-buffer)\n",
    "6. [Integration with World Model](#6-integration-with-world-model)\n",
    "7. [Experiments](#7-experiments)\n",
    "8. [Comparison with Standard Replay](#8-comparison-with-standard-replay)\n",
    "9. [Visualizations](#9-visualizations)\n",
    "10. [Summary](#10-summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Cell: Imports and Configuration\n",
    "Purpose: Import packages and set up environment\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Tuple, Optional, Union, Any, NamedTuple\n",
    "from dataclasses import dataclass, field\n",
    "from collections import deque\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "\n",
    "import gymnasium as gym\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import seaborn as sns\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "PROJECT_ROOT = Path.cwd().parent\n",
    "sys.path.insert(0, str(PROJECT_ROOT / \"src\"))\n",
    "\n",
    "from utils import set_seed, get_device, MetricLogger, COLORS\n",
    "\n",
    "SEED = 42\n",
    "set_seed(SEED)\n",
    "DEVICE = get_device()\n",
    "\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"PyTorch: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Cell: World Model Components (from Phase 2)\n",
    "Purpose: Import world model architecture\n",
    "\"\"\"\n",
    "\n",
    "# Core world model components (condensed from previous phases)\n",
    "class RSSMState(NamedTuple):\n",
    "    deter: torch.Tensor\n",
    "    stoch: torch.Tensor\n",
    "    @property\n",
    "    def combined(self): return torch.cat([self.deter, self.stoch], dim=-1)\n",
    "\n",
    "class MLPEncoder(nn.Module):\n",
    "    def __init__(self, obs_dim, hidden_dims, latent_dim):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        in_dim = obs_dim\n",
    "        for hd in hidden_dims:\n",
    "            layers.extend([nn.Linear(in_dim, hd), nn.ELU()])\n",
    "            in_dim = hd\n",
    "        layers.append(nn.Linear(in_dim, latent_dim))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "    def forward(self, obs): return self.network(obs)\n",
    "\n",
    "class MLPDecoder(nn.Module):\n",
    "    def __init__(self, latent_dim, hidden_dims, obs_dim):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        in_dim = latent_dim\n",
    "        for hd in hidden_dims:\n",
    "            layers.extend([nn.Linear(in_dim, hd), nn.ELU()])\n",
    "            in_dim = hd\n",
    "        layers.append(nn.Linear(in_dim, obs_dim))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "    def forward(self, latent): return self.network(latent)\n",
    "\n",
    "class RSSM(nn.Module):\n",
    "    def __init__(self, stoch_dim, deter_dim, hidden_dim, action_dim, embed_dim, min_std=0.1):\n",
    "        super().__init__()\n",
    "        self.stoch_dim, self.deter_dim, self.min_std = stoch_dim, deter_dim, min_std\n",
    "        self.input_proj = nn.Sequential(nn.Linear(stoch_dim + action_dim, hidden_dim), nn.ELU())\n",
    "        self.gru = nn.GRUCell(hidden_dim, deter_dim)\n",
    "        self.prior_net = nn.Sequential(nn.Linear(deter_dim, hidden_dim), nn.ELU(), nn.Linear(hidden_dim, 2*stoch_dim))\n",
    "        self.posterior_net = nn.Sequential(nn.Linear(deter_dim + embed_dim, hidden_dim), nn.ELU(), nn.Linear(hidden_dim, 2*stoch_dim))\n",
    "    \n",
    "    def initial_state(self, batch_size, device):\n",
    "        return RSSMState(torch.zeros(batch_size, self.deter_dim, device=device),\n",
    "                        torch.zeros(batch_size, self.stoch_dim, device=device))\n",
    "    \n",
    "    def _get_dist(self, stats):\n",
    "        mean, std = torch.chunk(stats, 2, dim=-1)\n",
    "        return Normal(mean, F.softplus(std) + self.min_std)\n",
    "    \n",
    "    def imagine_step(self, prev_state, action):\n",
    "        x = self.input_proj(torch.cat([prev_state.stoch, action], dim=-1))\n",
    "        deter = self.gru(x, prev_state.deter)\n",
    "        prior_dist = self._get_dist(self.prior_net(deter))\n",
    "        return RSSMState(deter, prior_dist.rsample()), prior_dist\n",
    "    \n",
    "    def observe_step(self, prev_state, action, embed):\n",
    "        prior_state, prior_dist = self.imagine_step(prev_state, action)\n",
    "        posterior_dist = self._get_dist(self.posterior_net(torch.cat([prior_state.deter, embed], dim=-1)))\n",
    "        return RSSMState(prior_state.deter, posterior_dist.rsample()), prior_dist, posterior_dist\n",
    "\n",
    "class RewardPredictor(nn.Module):\n",
    "    def __init__(self, state_dim, hidden_dims):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        in_dim = state_dim\n",
    "        for hd in hidden_dims:\n",
    "            layers.extend([nn.Linear(in_dim, hd), nn.ELU()])\n",
    "            in_dim = hd\n",
    "        layers.append(nn.Linear(in_dim, 1))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "    def forward(self, state): return self.network(state).squeeze(-1)\n",
    "\n",
    "class ContinuePredictor(nn.Module):\n",
    "    def __init__(self, state_dim, hidden_dims):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        in_dim = state_dim\n",
    "        for hd in hidden_dims:\n",
    "            layers.extend([nn.Linear(in_dim, hd), nn.ELU()])\n",
    "            in_dim = hd\n",
    "        layers.append(nn.Linear(in_dim, 1))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "    def forward(self, state): return self.network(state).squeeze(-1)\n",
    "\n",
    "class WorldModel(nn.Module):\n",
    "    def __init__(self, obs_dim, action_dim, stoch_dim=32, deter_dim=256, hidden_dim=256,\n",
    "                 encoder_hidden=[256,256], decoder_hidden=[256,256], predictor_hidden=[256,256]):\n",
    "        super().__init__()\n",
    "        self.state_dim = stoch_dim + deter_dim\n",
    "        self.encoder = MLPEncoder(obs_dim, encoder_hidden, hidden_dim)\n",
    "        self.rssm = RSSM(stoch_dim, deter_dim, hidden_dim, action_dim, hidden_dim)\n",
    "        self.reward_predictor = RewardPredictor(self.state_dim, predictor_hidden)\n",
    "        self.continue_predictor = ContinuePredictor(self.state_dim, predictor_hidden)\n",
    "        self.decoder = MLPDecoder(self.state_dim, decoder_hidden, obs_dim)\n",
    "    \n",
    "    def initial_state(self, batch_size):\n",
    "        return self.rssm.initial_state(batch_size, next(self.parameters()).device)\n",
    "    \n",
    "    def observe(self, obs, action, prev_state):\n",
    "        return self.rssm.observe_step(prev_state, action, self.encoder(obs))\n",
    "    \n",
    "    def predict(self, state):\n",
    "        combined = state.combined\n",
    "        return self.decoder(combined), self.reward_predictor(combined), self.continue_predictor(combined)\n",
    "    \n",
    "    def forward(self, obs_seq, action_seq):\n",
    "        B, T, _ = obs_seq.shape\n",
    "        state = self.initial_state(B)\n",
    "        priors, posteriors, recon_obs, pred_rewards, pred_continues = [], [], [], [], []\n",
    "        for t in range(T):\n",
    "            state, prior, posterior = self.observe(obs_seq[:, t], action_seq[:, t], state)\n",
    "            recon, reward, cont = self.predict(state)\n",
    "            priors.append(prior); posteriors.append(posterior)\n",
    "            recon_obs.append(recon); pred_rewards.append(reward); pred_continues.append(cont)\n",
    "        return {'recon_obs': torch.stack(recon_obs, 1), 'pred_rewards': torch.stack(pred_rewards, 1),\n",
    "                'pred_continues': torch.stack(pred_continues, 1), 'priors': priors, 'posteriors': posteriors}\n",
    "\n",
    "print(\"World model components loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Superposition Concept\n",
    "\n",
    "In quantum mechanics, superposition allows a system to exist in multiple states simultaneously.\n",
    "For experience replay, we implement this as:\n",
    "\n",
    "1. **Parallel sampling**: Sample multiple trajectories simultaneously\n",
    "2. **Amplitude weighting**: Assign quantum-like amplitudes to each sample\n",
    "3. **Interference**: Allow positive/negative interference between samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Cell: Superposition Concept Diagram\n",
    "Purpose: Visualize the superposition-enhanced replay concept\n",
    "\"\"\"\n",
    "\n",
    "def create_superposition_diagram(figsize=(14, 8)):\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    ax.set_xlim(0, 14)\n",
    "    ax.set_ylim(0, 8)\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # Standard replay\n",
    "    ax.text(3, 7.5, 'Standard Replay', ha='center', fontsize=12, fontweight='bold')\n",
    "    for i in range(3):\n",
    "        rect = mpatches.FancyBboxPatch((1+i*1.5, 6), 1.2, 0.8, boxstyle=\"round,pad=0.02\",\n",
    "                                       facecolor=COLORS['baseline'], alpha=0.6)\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(1.6+i*1.5, 6.4, f'τ{i+1}', ha='center', va='center', fontsize=10, color='white')\n",
    "    ax.annotate('', xy=(5.5, 6.4), xytext=(4.8, 6.4), arrowprops=dict(arrowstyle='->', lw=2))\n",
    "    rect = mpatches.FancyBboxPatch((5.6, 6), 1.2, 0.8, boxstyle=\"round,pad=0.02\",\n",
    "                                   facecolor=COLORS['baseline'], alpha=0.9)\n",
    "    ax.add_patch(rect)\n",
    "    ax.text(6.2, 6.4, 'Batch', ha='center', va='center', fontsize=10, color='white', fontweight='bold')\n",
    "    \n",
    "    # Superposition replay\n",
    "    ax.text(3, 4.5, 'Superposition Replay', ha='center', fontsize=12, fontweight='bold', color=COLORS['superposition'])\n",
    "    \n",
    "    # Parallel samples with amplitudes\n",
    "    amplitudes = [0.5, 0.3, 0.7, 0.4, 0.6]\n",
    "    for i, amp in enumerate(amplitudes):\n",
    "        rect = mpatches.FancyBboxPatch((0.5+i*1.3, 3), 1.0, 0.8, boxstyle=\"round,pad=0.02\",\n",
    "                                       facecolor=COLORS['superposition'], alpha=amp)\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(1.0+i*1.3, 3.4, f'α{i+1}', ha='center', va='center', fontsize=9, color='white')\n",
    "        ax.text(1.0+i*1.3, 2.6, f'{amp}', ha='center', va='center', fontsize=8)\n",
    "    \n",
    "    # Interference\n",
    "    ax.annotate('', xy=(8, 3.4), xytext=(7, 3.4), arrowprops=dict(arrowstyle='->', lw=2, color=COLORS['superposition']))\n",
    "    ax.text(7.5, 3.8, 'Interference', ha='center', fontsize=9, style='italic')\n",
    "    \n",
    "    # Weighted combination\n",
    "    rect = mpatches.FancyBboxPatch((8.2, 3), 2, 0.8, boxstyle=\"round,pad=0.02\",\n",
    "                                   facecolor=COLORS['superposition'], alpha=0.9)\n",
    "    ax.add_patch(rect)\n",
    "    ax.text(9.2, 3.4, 'Σ αᵢ|τᵢ⟩', ha='center', va='center', fontsize=11, color='white', fontweight='bold')\n",
    "    \n",
    "    # Benefits\n",
    "    ax.text(11.5, 6.5, 'Benefits:', fontsize=11, fontweight='bold')\n",
    "    benefits = ['• Better exploration', '• Sample efficiency', '• Rare event focus', '• Smooth gradients']\n",
    "    for i, b in enumerate(benefits):\n",
    "        ax.text(11.5, 5.8-i*0.5, b, fontsize=9)\n",
    "    \n",
    "    ax.set_title('Superposition-Enhanced Experience Replay', fontsize=14, fontweight='bold')\n",
    "    return fig\n",
    "\n",
    "fig = create_superposition_diagram()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Amplitude-Based Weighting\n",
    "\n",
    "Assign quantum-like amplitudes to experiences based on their importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Cell: Amplitude Calculator\n",
    "Purpose: Compute quantum-inspired amplitudes for experiences\n",
    "\"\"\"\n",
    "\n",
    "class AmplitudeCalculator:\n",
    "    \"\"\"\n",
    "    Calculate quantum-inspired amplitudes for experience weighting.\n",
    "    \n",
    "    Amplitudes are computed based on:\n",
    "    - TD error (surprise/novelty)\n",
    "    - Reward magnitude\n",
    "    - Recency\n",
    "    - State entropy\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    td_weight : float\n",
    "        Weight for TD error component\n",
    "    reward_weight : float\n",
    "        Weight for reward magnitude\n",
    "    recency_weight : float\n",
    "        Weight for recency (newer = higher)\n",
    "    entropy_weight : float\n",
    "        Weight for state entropy\n",
    "    temperature : float\n",
    "        Softmax temperature for amplitude normalization\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        td_weight: float = 0.4,\n",
    "        reward_weight: float = 0.3,\n",
    "        recency_weight: float = 0.2,\n",
    "        entropy_weight: float = 0.1,\n",
    "        temperature: float = 1.0\n",
    "    ):\n",
    "        self.td_weight = td_weight\n",
    "        self.reward_weight = reward_weight\n",
    "        self.recency_weight = recency_weight\n",
    "        self.entropy_weight = entropy_weight\n",
    "        self.temperature = temperature\n",
    "    \n",
    "    def compute_td_priority(\n",
    "        self,\n",
    "        rewards: np.ndarray,\n",
    "        gamma: float = 0.99\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Compute TD-error based priority.\n",
    "        Uses reward variance as proxy for TD error.\n",
    "        \"\"\"\n",
    "        # Compute reward changes\n",
    "        reward_diff = np.abs(np.diff(rewards, prepend=rewards[0]))\n",
    "        return reward_diff / (reward_diff.max() + 1e-8)\n",
    "    \n",
    "    def compute_reward_priority(self, rewards: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Compute reward magnitude priority.\n",
    "        Higher absolute rewards get higher priority.\n",
    "        \"\"\"\n",
    "        abs_rewards = np.abs(rewards)\n",
    "        return abs_rewards / (abs_rewards.max() + 1e-8)\n",
    "    \n",
    "    def compute_recency_priority(self, length: int, decay: float = 0.99) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Compute recency-based priority.\n",
    "        More recent experiences get higher priority.\n",
    "        \"\"\"\n",
    "        indices = np.arange(length)\n",
    "        return decay ** (length - 1 - indices)\n",
    "    \n",
    "    def compute_entropy_priority(self, observations: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Compute state entropy priority.\n",
    "        Higher entropy (more diverse) states get higher priority.\n",
    "        \"\"\"\n",
    "        # Use observation variance as proxy for entropy\n",
    "        obs_var = np.var(observations, axis=-1)\n",
    "        return obs_var / (obs_var.max() + 1e-8)\n",
    "    \n",
    "    def compute_amplitudes(\n",
    "        self,\n",
    "        observations: np.ndarray,\n",
    "        rewards: np.ndarray\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Compute quantum-inspired amplitudes for a trajectory.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        observations : np.ndarray\n",
    "            Trajectory observations (T, obs_dim)\n",
    "        rewards : np.ndarray\n",
    "            Trajectory rewards (T,)\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray\n",
    "            Amplitudes (T,) that sum to 1 (like probability amplitudes squared)\n",
    "        \"\"\"\n",
    "        length = len(rewards)\n",
    "        \n",
    "        # Compute individual priorities\n",
    "        td_priority = self.compute_td_priority(rewards)\n",
    "        reward_priority = self.compute_reward_priority(rewards)\n",
    "        recency_priority = self.compute_recency_priority(length)\n",
    "        entropy_priority = self.compute_entropy_priority(observations)\n",
    "        \n",
    "        # Weighted combination\n",
    "        combined = (\n",
    "            self.td_weight * td_priority +\n",
    "            self.reward_weight * reward_priority +\n",
    "            self.recency_weight * recency_priority +\n",
    "            self.entropy_weight * entropy_priority\n",
    "        )\n",
    "        \n",
    "        # Softmax normalization (temperature-scaled)\n",
    "        amplitudes = np.exp(combined / self.temperature)\n",
    "        amplitudes = amplitudes / amplitudes.sum()\n",
    "        \n",
    "        return amplitudes\n",
    "\n",
    "\n",
    "# Test amplitude calculator\n",
    "amp_calc = AmplitudeCalculator()\n",
    "\n",
    "test_obs = np.random.randn(50, 4)\n",
    "test_rewards = np.sin(np.linspace(0, 4*np.pi, 50)) + np.random.randn(50) * 0.1\n",
    "\n",
    "amplitudes = amp_calc.compute_amplitudes(test_obs, test_rewards)\n",
    "\n",
    "print(f\"Amplitudes shape: {amplitudes.shape}\")\n",
    "print(f\"Amplitudes sum: {amplitudes.sum():.4f}\")\n",
    "print(f\"Max amplitude: {amplitudes.max():.4f}\")\n",
    "print(f\"Min amplitude: {amplitudes.min():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Cell: Visualize Amplitudes\n",
    "Purpose: Plot amplitude distribution for a trajectory\n",
    "\"\"\"\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "\n",
    "# Rewards\n",
    "ax = axes[0, 0]\n",
    "ax.plot(test_rewards, color=COLORS['baseline'], linewidth=2)\n",
    "ax.set_xlabel('Step')\n",
    "ax.set_ylabel('Reward')\n",
    "ax.set_title('Trajectory Rewards')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Amplitudes\n",
    "ax = axes[0, 1]\n",
    "ax.bar(range(len(amplitudes)), amplitudes, color=COLORS['superposition'], alpha=0.7)\n",
    "ax.set_xlabel('Step')\n",
    "ax.set_ylabel('Amplitude')\n",
    "ax.set_title('Quantum-Inspired Amplitudes')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Amplitude vs Reward\n",
    "ax = axes[1, 0]\n",
    "ax.scatter(test_rewards, amplitudes, c=range(len(amplitudes)), cmap='viridis', alpha=0.7)\n",
    "ax.set_xlabel('Reward')\n",
    "ax.set_ylabel('Amplitude')\n",
    "ax.set_title('Amplitude vs Reward')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Cumulative amplitude\n",
    "ax = axes[1, 1]\n",
    "ax.plot(np.cumsum(amplitudes), color=COLORS['superposition'], linewidth=2)\n",
    "ax.axhline(y=0.5, color='gray', linestyle='--', alpha=0.5)\n",
    "ax.set_xlabel('Step')\n",
    "ax.set_ylabel('Cumulative Amplitude')\n",
    "ax.set_title('Cumulative Amplitude (Sampling CDF)')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "fig.suptitle('Amplitude-Based Experience Weighting', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Interference Effects\n",
    "\n",
    "Implement constructive and destructive interference between sampled experiences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Cell: Interference Calculator\n",
    "Purpose: Implement quantum interference-like effects\n",
    "\"\"\"\n",
    "\n",
    "class InterferenceCalculator:\n",
    "    \"\"\"\n",
    "    Calculate interference effects between parallel samples.\n",
    "    \n",
    "    Implements:\n",
    "    - Constructive interference: Similar experiences reinforce each other\n",
    "    - Destructive interference: Contradictory experiences cancel out\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    interference_strength : float\n",
    "        Strength of interference effects (0 = none, 1 = full)\n",
    "    similarity_threshold : float\n",
    "        Threshold for considering experiences similar\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        interference_strength: float = 0.3,\n",
    "        similarity_threshold: float = 0.7\n",
    "    ):\n",
    "        self.interference_strength = interference_strength\n",
    "        self.similarity_threshold = similarity_threshold\n",
    "    \n",
    "    def compute_similarity(\n",
    "        self,\n",
    "        obs1: np.ndarray,\n",
    "        obs2: np.ndarray\n",
    "    ) -> float:\n",
    "        \"\"\"\n",
    "        Compute similarity between two observations.\n",
    "        Uses cosine similarity.\n",
    "        \"\"\"\n",
    "        obs1_flat = obs1.flatten()\n",
    "        obs2_flat = obs2.flatten()\n",
    "        norm1 = np.linalg.norm(obs1_flat)\n",
    "        norm2 = np.linalg.norm(obs2_flat)\n",
    "        if norm1 < 1e-8 or norm2 < 1e-8:\n",
    "            return 0.0\n",
    "        return np.dot(obs1_flat, obs2_flat) / (norm1 * norm2)\n",
    "    \n",
    "    def compute_phase(\n",
    "        self,\n",
    "        rewards: np.ndarray,\n",
    "        actions: np.ndarray\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Compute phase angles for samples.\n",
    "        Phase is determined by action-reward relationship.\n",
    "        \"\"\"\n",
    "        # Use action direction and reward sign to determine phase\n",
    "        action_signal = np.mean(actions, axis=-1) if actions.ndim > 1 else actions\n",
    "        reward_sign = np.sign(rewards)\n",
    "        \n",
    "        # Phase in [0, 2π]\n",
    "        phase = np.pi * (1 + action_signal * reward_sign) / 2\n",
    "        return phase\n",
    "    \n",
    "    def apply_interference(\n",
    "        self,\n",
    "        samples: List[Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]],\n",
    "        amplitudes: List[np.ndarray]\n",
    "    ) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Apply interference effects to parallel samples.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        samples : List[Tuple]\n",
    "            List of (obs, action, reward, continue) samples\n",
    "        amplitudes : List[np.ndarray]\n",
    "            Amplitudes for each sample\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Tuple[np.ndarray, ...]\n",
    "            Interfered (obs, actions, rewards, continues, weights)\n",
    "        \"\"\"\n",
    "        num_samples = len(samples)\n",
    "        \n",
    "        # Compute pairwise similarities and phases\n",
    "        interference_matrix = np.ones((num_samples, num_samples))\n",
    "        \n",
    "        for i in range(num_samples):\n",
    "            for j in range(i + 1, num_samples):\n",
    "                obs_i, _, rewards_i, _ = samples[i]\n",
    "                obs_j, _, rewards_j, _ = samples[j]\n",
    "                \n",
    "                # Compute similarity\n",
    "                sim = self.compute_similarity(obs_i.mean(axis=0), obs_j.mean(axis=0))\n",
    "                \n",
    "                # Determine interference type based on reward correlation\n",
    "                reward_corr = np.corrcoef(rewards_i, rewards_j)[0, 1]\n",
    "                \n",
    "                if np.isnan(reward_corr):\n",
    "                    reward_corr = 0.0\n",
    "                \n",
    "                # Constructive if similar and same reward direction\n",
    "                # Destructive if similar but opposite rewards\n",
    "                if sim > self.similarity_threshold:\n",
    "                    if reward_corr > 0:\n",
    "                        # Constructive interference\n",
    "                        interference = 1.0 + self.interference_strength * sim\n",
    "                    else:\n",
    "                        # Destructive interference\n",
    "                        interference = 1.0 - self.interference_strength * sim * abs(reward_corr)\n",
    "                else:\n",
    "                    interference = 1.0\n",
    "                \n",
    "                interference_matrix[i, j] = interference\n",
    "                interference_matrix[j, i] = interference\n",
    "        \n",
    "        # Apply interference to amplitudes\n",
    "        modified_amplitudes = []\n",
    "        for i, amp in enumerate(amplitudes):\n",
    "            interference_factor = interference_matrix[i].mean()\n",
    "            modified_amplitudes.append(amp * interference_factor)\n",
    "        \n",
    "        # Combine samples with modified amplitudes\n",
    "        total_amp = sum(a.sum() for a in modified_amplitudes)\n",
    "        weights = [a / total_amp for a in modified_amplitudes]\n",
    "        \n",
    "        # Weighted combination\n",
    "        obs_combined = sum(w.reshape(-1, 1) * s[0] for w, s in zip(weights, samples))\n",
    "        act_combined = sum(w.reshape(-1, 1) * s[1] for w, s in zip(weights, samples))\n",
    "        rew_combined = sum(w * s[2] for w, s in zip(weights, samples))\n",
    "        cont_combined = sum(w * s[3] for w, s in zip(weights, samples))\n",
    "        \n",
    "        final_weights = sum(weights) / num_samples\n",
    "        \n",
    "        return obs_combined, act_combined, rew_combined, cont_combined, final_weights\n",
    "\n",
    "\n",
    "print(\"Interference calculator defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Superposition Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Cell: Superposition Replay Buffer\n",
    "Purpose: Implement the complete superposition-enhanced replay buffer\n",
    "\"\"\"\n",
    "\n",
    "@dataclass\n",
    "class Episode:\n",
    "    observations: np.ndarray\n",
    "    actions: np.ndarray\n",
    "    rewards: np.ndarray\n",
    "    dones: np.ndarray\n",
    "    amplitudes: Optional[np.ndarray] = None\n",
    "    \n",
    "    def __len__(self): return len(self.observations)\n",
    "\n",
    "\n",
    "class SuperpositionReplayBuffer:\n",
    "    \"\"\"\n",
    "    Quantum superposition-inspired experience replay buffer.\n",
    "    \n",
    "    Features:\n",
    "    - Amplitude-based sampling (prioritized by quantum-inspired weights)\n",
    "    - Parallel trajectory sampling (superposition of experiences)\n",
    "    - Interference effects between similar experiences\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    capacity : int\n",
    "        Maximum number of episodes to store\n",
    "    parallel_samples : int\n",
    "        Number of parallel trajectories to sample (superposition width)\n",
    "    interference_strength : float\n",
    "        Strength of interference effects\n",
    "    amplitude_temperature : float\n",
    "        Temperature for amplitude calculation\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        capacity: int = 1000,\n",
    "        parallel_samples: int = 4,\n",
    "        interference_strength: float = 0.2,\n",
    "        amplitude_temperature: float = 1.0\n",
    "    ):\n",
    "        self.capacity = capacity\n",
    "        self.parallel_samples = parallel_samples\n",
    "        \n",
    "        self.episodes: List[Episode] = []\n",
    "        self.total_steps = 0\n",
    "        \n",
    "        self.amplitude_calc = AmplitudeCalculator(\n",
    "            temperature=amplitude_temperature\n",
    "        )\n",
    "        self.interference_calc = InterferenceCalculator(\n",
    "            interference_strength=interference_strength\n",
    "        )\n",
    "    \n",
    "    def add_episode(self, episode: Episode) -> None:\n",
    "        \"\"\"\n",
    "        Add an episode and compute its amplitudes.\n",
    "        \"\"\"\n",
    "        # Compute amplitudes for the episode\n",
    "        amplitudes = self.amplitude_calc.compute_amplitudes(\n",
    "            episode.observations, episode.rewards\n",
    "        )\n",
    "        episode.amplitudes = amplitudes\n",
    "        \n",
    "        if len(self.episodes) >= self.capacity:\n",
    "            removed = self.episodes.pop(0)\n",
    "            self.total_steps -= len(removed)\n",
    "        \n",
    "        self.episodes.append(episode)\n",
    "        self.total_steps += len(episode)\n",
    "    \n",
    "    def _sample_single_sequence(\n",
    "        self,\n",
    "        seq_len: int\n",
    "    ) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Sample a single sequence with amplitude-based prioritization.\n",
    "        \"\"\"\n",
    "        valid_episodes = [ep for ep in self.episodes if len(ep) >= seq_len]\n",
    "        \n",
    "        # Sample episode weighted by total episode amplitude\n",
    "        episode_weights = np.array([ep.amplitudes.sum() for ep in valid_episodes])\n",
    "        episode_weights = episode_weights / episode_weights.sum()\n",
    "        \n",
    "        ep_idx = np.random.choice(len(valid_episodes), p=episode_weights)\n",
    "        episode = valid_episodes[ep_idx]\n",
    "        \n",
    "        # Sample start position weighted by amplitudes\n",
    "        max_start = len(episode) - seq_len\n",
    "        start_weights = episode.amplitudes[:max_start + 1]\n",
    "        start_weights = start_weights / start_weights.sum()\n",
    "        \n",
    "        start = np.random.choice(max_start + 1, p=start_weights)\n",
    "        \n",
    "        obs = episode.observations[start:start + seq_len]\n",
    "        actions = episode.actions[start:start + seq_len]\n",
    "        rewards = episode.rewards[start:start + seq_len]\n",
    "        continues = 1.0 - episode.dones[start:start + seq_len]\n",
    "        amplitudes = episode.amplitudes[start:start + seq_len]\n",
    "        \n",
    "        return obs, actions, rewards, continues, amplitudes\n",
    "    \n",
    "    def sample_superposition(\n",
    "        self,\n",
    "        batch_size: int,\n",
    "        seq_len: int,\n",
    "        apply_interference: bool = True\n",
    "    ) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Sample a batch with superposition-enhanced replay.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        batch_size : int\n",
    "            Number of sequences in batch\n",
    "        seq_len : int\n",
    "            Length of each sequence\n",
    "        apply_interference : bool\n",
    "            Whether to apply interference effects\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Tuple[np.ndarray, ...]\n",
    "            (observations, actions, rewards, continues, weights)\n",
    "        \"\"\"\n",
    "        obs_batch, act_batch, rew_batch, cont_batch, weight_batch = [], [], [], [], []\n",
    "        \n",
    "        for _ in range(batch_size):\n",
    "            # Sample parallel sequences (superposition)\n",
    "            parallel_samples = []\n",
    "            parallel_amplitudes = []\n",
    "            \n",
    "            for _ in range(self.parallel_samples):\n",
    "                obs, act, rew, cont, amp = self._sample_single_sequence(seq_len)\n",
    "                parallel_samples.append((obs, act, rew, cont))\n",
    "                parallel_amplitudes.append(amp)\n",
    "            \n",
    "            if apply_interference and len(parallel_samples) > 1:\n",
    "                # Apply interference effects\n",
    "                obs, act, rew, cont, weights = self.interference_calc.apply_interference(\n",
    "                    parallel_samples, parallel_amplitudes\n",
    "                )\n",
    "            else:\n",
    "                # Simple amplitude-weighted average\n",
    "                total_amp = sum(a.sum() for a in parallel_amplitudes)\n",
    "                weights = [a / total_amp for a in parallel_amplitudes]\n",
    "                \n",
    "                obs = sum(w.reshape(-1, 1) * s[0] for w, s in zip(weights, parallel_samples))\n",
    "                act = sum(w.reshape(-1, 1) * s[1] for w, s in zip(weights, parallel_samples))\n",
    "                rew = sum(w * s[2] for w, s in zip(weights, parallel_samples))\n",
    "                cont = sum(w * s[3] for w, s in zip(weights, parallel_samples))\n",
    "                weights = sum(weights) / len(weights)\n",
    "            \n",
    "            obs_batch.append(obs)\n",
    "            act_batch.append(act)\n",
    "            rew_batch.append(rew)\n",
    "            cont_batch.append(cont)\n",
    "            weight_batch.append(weights)\n",
    "        \n",
    "        return (\n",
    "            np.stack(obs_batch),\n",
    "            np.stack(act_batch),\n",
    "            np.stack(rew_batch),\n",
    "            np.stack(cont_batch),\n",
    "            np.stack(weight_batch)\n",
    "        )\n",
    "    \n",
    "    def sample_standard(\n",
    "        self,\n",
    "        batch_size: int,\n",
    "        seq_len: int\n",
    "    ) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Standard sampling (no superposition) for comparison.\n",
    "        \"\"\"\n",
    "        valid = [ep for ep in self.episodes if len(ep) >= seq_len]\n",
    "        obs_b, act_b, rew_b, cont_b = [], [], [], []\n",
    "        \n",
    "        for _ in range(batch_size):\n",
    "            ep = valid[np.random.randint(len(valid))]\n",
    "            start = np.random.randint(0, len(ep) - seq_len + 1)\n",
    "            obs_b.append(ep.observations[start:start+seq_len])\n",
    "            act_b.append(ep.actions[start:start+seq_len])\n",
    "            rew_b.append(ep.rewards[start:start+seq_len])\n",
    "            cont_b.append(1.0 - ep.dones[start:start+seq_len])\n",
    "        \n",
    "        return np.stack(obs_b), np.stack(act_b), np.stack(rew_b), np.stack(cont_b)\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.episodes)\n",
    "\n",
    "\n",
    "print(\"Superposition replay buffer defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Cell: Data Collection\n",
    "Purpose: Collect episodes for testing\n",
    "\"\"\"\n",
    "\n",
    "def collect_episodes(env_name: str, num_episodes: int, seed: int = 42):\n",
    "    env = gym.make(env_name)\n",
    "    episodes = []\n",
    "    for ep_idx in tqdm(range(num_episodes), desc=\"Collecting\"):\n",
    "        obs_l, act_l, rew_l, done_l = [], [], [], []\n",
    "        obs, _ = env.reset(seed=seed+ep_idx)\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = env.action_space.sample()\n",
    "            obs_l.append(obs)\n",
    "            if isinstance(env.action_space, gym.spaces.Discrete):\n",
    "                act_oh = np.zeros(env.action_space.n)\n",
    "                act_oh[action] = 1.0\n",
    "                act_l.append(act_oh)\n",
    "            else:\n",
    "                act_l.append(action)\n",
    "            obs, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            rew_l.append(reward)\n",
    "            done_l.append(float(done))\n",
    "        episodes.append(Episode(\n",
    "            np.array(obs_l, np.float32), np.array(act_l, np.float32),\n",
    "            np.array(rew_l, np.float32), np.array(done_l, np.float32)\n",
    "        ))\n",
    "    env.close()\n",
    "    return episodes\n",
    "\n",
    "# Collect and add to buffer\n",
    "print(\"Collecting training data...\")\n",
    "episodes = collect_episodes(\"CartPole-v1\", num_episodes=100, seed=SEED)\n",
    "\n",
    "# Create superposition buffer\n",
    "super_buffer = SuperpositionReplayBuffer(\n",
    "    capacity=1000,\n",
    "    parallel_samples=4,\n",
    "    interference_strength=0.2,\n",
    "    amplitude_temperature=1.0\n",
    ")\n",
    "\n",
    "for ep in episodes:\n",
    "    super_buffer.add_episode(ep)\n",
    "\n",
    "print(f\"Superposition buffer: {len(super_buffer)} episodes, {super_buffer.total_steps} steps\")\n",
    "\n",
    "# Test sampling\n",
    "obs, act, rew, cont, weights = super_buffer.sample_superposition(batch_size=8, seq_len=20)\n",
    "print(f\"Sample shapes: obs={obs.shape}, weights={weights.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Integration with World Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Cell: Loss Functions\n",
    "Purpose: World model loss computation\n",
    "\"\"\"\n",
    "\n",
    "def compute_world_model_loss(output, obs_seq, reward_seq, continue_seq, \n",
    "                            kl_weight=1.0, free_nats=1.0, sample_weights=None):\n",
    "    recon_loss = F.mse_loss(output['recon_obs'], obs_seq, reduction='none')\n",
    "    reward_loss = F.mse_loss(output['pred_rewards'], reward_seq, reduction='none')\n",
    "    continue_loss = F.binary_cross_entropy_with_logits(\n",
    "        output['pred_continues'], continue_seq, reduction='none'\n",
    "    )\n",
    "    \n",
    "    # Apply sample weights if provided (superposition weighting)\n",
    "    if sample_weights is not None:\n",
    "        weights = torch.tensor(sample_weights, device=obs_seq.device, dtype=torch.float32)\n",
    "        if weights.dim() == 2:  # (batch, seq)\n",
    "            recon_loss = (recon_loss.mean(-1) * weights).mean()\n",
    "            reward_loss = (reward_loss * weights).mean()\n",
    "            continue_loss = (continue_loss * weights).mean()\n",
    "        else:\n",
    "            recon_loss = recon_loss.mean()\n",
    "            reward_loss = reward_loss.mean()\n",
    "            continue_loss = continue_loss.mean()\n",
    "    else:\n",
    "        recon_loss = recon_loss.mean()\n",
    "        reward_loss = reward_loss.mean()\n",
    "        continue_loss = continue_loss.mean()\n",
    "    \n",
    "    kl_losses = []\n",
    "    for prior, posterior in zip(output['priors'], output['posteriors']):\n",
    "        kl = torch.distributions.kl_divergence(posterior, prior).sum(-1)\n",
    "        kl = torch.clamp(kl, min=free_nats).mean()\n",
    "        kl_losses.append(kl)\n",
    "    kl_loss = torch.stack(kl_losses).mean()\n",
    "    \n",
    "    total = kl_weight * kl_loss + recon_loss + reward_loss + continue_loss\n",
    "    return {'total': total, 'kl': kl_loss, 'recon': recon_loss, \n",
    "            'reward': reward_loss, 'continue': continue_loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Cell: Superposition Trainer\n",
    "Purpose: Training loop with superposition replay\n",
    "\"\"\"\n",
    "\n",
    "class SuperpositionTrainer:\n",
    "    def __init__(self, model, buffer, lr=3e-4, device=DEVICE):\n",
    "        self.model = model\n",
    "        self.buffer = buffer\n",
    "        self.device = device\n",
    "        self.optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "        self.logger = MetricLogger(name=\"superposition\")\n",
    "    \n",
    "    def train_step(self, batch_size=32, seq_len=20, kl_weight=1.0, use_superposition=True):\n",
    "        self.model.train()\n",
    "        \n",
    "        if use_superposition:\n",
    "            obs, act, rew, cont, weights = self.buffer.sample_superposition(batch_size, seq_len)\n",
    "        else:\n",
    "            obs, act, rew, cont = self.buffer.sample_standard(batch_size, seq_len)\n",
    "            weights = None\n",
    "        \n",
    "        obs = torch.tensor(obs, dtype=torch.float32, device=self.device)\n",
    "        act = torch.tensor(act, dtype=torch.float32, device=self.device)\n",
    "        rew = torch.tensor(rew, dtype=torch.float32, device=self.device)\n",
    "        cont = torch.tensor(cont, dtype=torch.float32, device=self.device)\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        output = self.model(obs, act)\n",
    "        losses = compute_world_model_loss(output, obs, rew, cont, kl_weight, sample_weights=weights)\n",
    "        losses['total'].backward()\n",
    "        nn.utils.clip_grad_norm_(self.model.parameters(), 100.0)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        result = {k: v.item() for k, v in losses.items()}\n",
    "        self.logger.log(**result)\n",
    "        return result\n",
    "    \n",
    "    def train(self, num_steps=5000, batch_size=32, seq_len=20, log_every=100, \n",
    "              kl_weight=1.0, use_superposition=True):\n",
    "        desc = \"Superposition Training\" if use_superposition else \"Standard Training\"\n",
    "        pbar = tqdm(range(num_steps), desc=desc)\n",
    "        for step in pbar:\n",
    "            self.train_step(batch_size, seq_len, kl_weight, use_superposition)\n",
    "            if step % log_every == 0:\n",
    "                pbar.set_postfix({\n",
    "                    'total': f\"{self.logger.get_mean('total', 100):.4f}\",\n",
    "                    'recon': f\"{self.logger.get_mean('recon', 100):.4f}\"\n",
    "                })\n",
    "        return self.logger.to_dataframe()\n",
    "\n",
    "print(\"Superposition trainer defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Cell: Train with Superposition Replay\n",
    "Purpose: Train world model with superposition-enhanced replay\n",
    "\"\"\"\n",
    "\n",
    "# Create model\n",
    "set_seed(SEED)\n",
    "super_model = WorldModel(\n",
    "    obs_dim=4, action_dim=2, stoch_dim=32, deter_dim=128, hidden_dim=128,\n",
    "    encoder_hidden=[128, 128], decoder_hidden=[128, 128], predictor_hidden=[128, 128]\n",
    ").to(DEVICE)\n",
    "\n",
    "print(f\"Model Parameters: {sum(p.numel() for p in super_model.parameters()):,}\")\n",
    "\n",
    "# Train with superposition\n",
    "super_trainer = SuperpositionTrainer(super_model, super_buffer, lr=3e-4, device=DEVICE)\n",
    "\n",
    "print(\"\\nTraining with superposition replay...\")\n",
    "super_history = super_trainer.train(\n",
    "    num_steps=5000, batch_size=32, seq_len=20, log_every=100,\n",
    "    kl_weight=1.0, use_superposition=True\n",
    ")\n",
    "\n",
    "print(f\"\\nSuperposition Training complete!\")\n",
    "print(f\"  Final loss: {super_history['total'].iloc[-100:].mean():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Cell: Train Standard Baseline\n",
    "Purpose: Train with standard replay for comparison\n",
    "\"\"\"\n",
    "\n",
    "set_seed(SEED)\n",
    "standard_model = WorldModel(\n",
    "    obs_dim=4, action_dim=2, stoch_dim=32, deter_dim=128, hidden_dim=128,\n",
    "    encoder_hidden=[128, 128], decoder_hidden=[128, 128], predictor_hidden=[128, 128]\n",
    ").to(DEVICE)\n",
    "\n",
    "standard_trainer = SuperpositionTrainer(standard_model, super_buffer, lr=3e-4, device=DEVICE)\n",
    "\n",
    "print(\"\\nTraining with standard replay...\")\n",
    "standard_history = standard_trainer.train(\n",
    "    num_steps=5000, batch_size=32, seq_len=20, log_every=100,\n",
    "    kl_weight=1.0, use_superposition=False\n",
    ")\n",
    "\n",
    "print(f\"\\nStandard Training complete!\")\n",
    "print(f\"  Final loss: {standard_history['total'].iloc[-100:].mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Comparison with Standard Replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Cell: Statistical Comparison\n",
    "Purpose: Compare superposition vs standard replay\n",
    "\"\"\"\n",
    "\n",
    "def compare_methods(hist1, hist2, name1, name2, window=100):\n",
    "    final1 = hist1['total'].iloc[-window:].values\n",
    "    final2 = hist2['total'].iloc[-window:].values\n",
    "    \n",
    "    stat, p_value = stats.mannwhitneyu(final1, final2, alternative='two-sided')\n",
    "    pooled_std = np.sqrt((final1.std()**2 + final2.std()**2) / 2)\n",
    "    cohens_d = (final1.mean() - final2.mean()) / pooled_std if pooled_std > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        'method1': name1, 'method2': name2,\n",
    "        'mean_loss_1': final1.mean(), 'mean_loss_2': final2.mean(),\n",
    "        'std_loss_1': final1.std(), 'std_loss_2': final2.std(),\n",
    "        'p_value': p_value, 'cohens_d': cohens_d,\n",
    "        'significant': p_value < 0.05\n",
    "    }\n",
    "\n",
    "comparison = compare_methods(super_history, standard_history, \"Superposition\", \"Standard\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMPARISON: Superposition vs Standard Replay\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nFinal Loss:\")\n",
    "print(f\"  Superposition: {comparison['mean_loss_1']:.4f} +/- {comparison['std_loss_1']:.4f}\")\n",
    "print(f\"  Standard:      {comparison['mean_loss_2']:.4f} +/- {comparison['std_loss_2']:.4f}\")\n",
    "print(f\"\\nStatistics:\")\n",
    "print(f\"  p-value: {comparison['p_value']:.4f}\")\n",
    "print(f\"  Cohen's d: {comparison['cohens_d']:.3f}\")\n",
    "print(f\"  Significant: {comparison['significant']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Cell: Comparison Plots\n",
    "Purpose: Visualize superposition vs standard\n",
    "\"\"\"\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "window = 50\n",
    "\n",
    "# Total loss\n",
    "ax = axes[0, 0]\n",
    "ax.plot(super_history['total'].rolling(window).mean(), \n",
    "       color=COLORS['superposition'], label='Superposition', linewidth=2)\n",
    "ax.plot(standard_history['total'].rolling(window).mean(),\n",
    "       color=COLORS['baseline'], label='Standard', linewidth=2)\n",
    "ax.set_xlabel('Step'); ax.set_ylabel('Loss'); ax.set_title('Total Loss')\n",
    "ax.legend(); ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Reconstruction loss\n",
    "ax = axes[0, 1]\n",
    "ax.plot(super_history['recon'].rolling(window).mean(),\n",
    "       color=COLORS['superposition'], label='Superposition', linewidth=2)\n",
    "ax.plot(standard_history['recon'].rolling(window).mean(),\n",
    "       color=COLORS['baseline'], label='Standard', linewidth=2)\n",
    "ax.set_xlabel('Step'); ax.set_ylabel('Loss'); ax.set_title('Reconstruction Loss')\n",
    "ax.legend(); ax.grid(True, alpha=0.3)\n",
    "\n",
    "# KL loss\n",
    "ax = axes[1, 0]\n",
    "ax.plot(super_history['kl'].rolling(window).mean(),\n",
    "       color=COLORS['superposition'], label='Superposition', linewidth=2)\n",
    "ax.plot(standard_history['kl'].rolling(window).mean(),\n",
    "       color=COLORS['baseline'], label='Standard', linewidth=2)\n",
    "ax.set_xlabel('Step'); ax.set_ylabel('Loss'); ax.set_title('KL Loss')\n",
    "ax.legend(); ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Final distribution\n",
    "ax = axes[1, 1]\n",
    "ax.hist(super_history['total'].iloc[-200:], bins=30, alpha=0.6, \n",
    "       color=COLORS['superposition'], label='Superposition')\n",
    "ax.hist(standard_history['total'].iloc[-200:], bins=30, alpha=0.6,\n",
    "       color=COLORS['baseline'], label='Standard')\n",
    "ax.set_xlabel('Loss'); ax.set_ylabel('Frequency'); ax.set_title('Final Loss Distribution')\n",
    "ax.legend(); ax.grid(True, alpha=0.3)\n",
    "\n",
    "fig.suptitle('Superposition vs Standard Replay Comparison', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Cell: Save and Summary\n",
    "Purpose: Save results and display summary\n",
    "\"\"\"\n",
    "\n",
    "# Save results\n",
    "results_dir = PROJECT_ROOT / \"experiments\" / \"results\" / \"superposition\"\n",
    "results_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "torch.save({'model_state_dict': super_model.state_dict(), 'config': {'parallel_samples': 4}},\n",
    "           results_dir / \"cartpole_superposition.pt\")\n",
    "super_history.to_csv(results_dir / \"superposition_history.csv\", index=False)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"PHASE 4 COMPLETE: SUPERPOSITION-ENHANCED REPLAY\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\n[1] Components Implemented\")\n",
    "print(\"    - AmplitudeCalculator: Quantum-inspired experience weighting\")\n",
    "print(\"    - InterferenceCalculator: Constructive/destructive interference\")\n",
    "print(\"    - SuperpositionReplayBuffer: Full quantum-inspired replay\")\n",
    "print(f\"\\n[2] Configuration\")\n",
    "print(f\"    - Parallel samples: 4\")\n",
    "print(f\"    - Interference strength: 0.2\")\n",
    "print(f\"\\n[3] Results\")\n",
    "print(f\"    - Superposition loss: {comparison['mean_loss_1']:.4f}\")\n",
    "print(f\"    - Standard loss: {comparison['mean_loss_2']:.4f}\")\n",
    "print(f\"    - p-value: {comparison['p_value']:.4f}\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"NEXT: Phase 5 - Gate-Enhanced Neural Layers\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
