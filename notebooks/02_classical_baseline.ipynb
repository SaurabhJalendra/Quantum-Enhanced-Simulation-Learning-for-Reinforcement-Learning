{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 2: Classical Baseline World Model\n",
    "\n",
    "**Notebook:** `02_classical_baseline.ipynb`  \n",
    "**Phase:** 2 of 9  \n",
    "**Purpose:** Implement DreamerV3-style RSSM world model with classical training  \n",
    "**Author:** Saurabh Jalendra  \n",
    "**Institution:** BITS Pilani (WILP Division)  \n",
    "**Date:** November 2025\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Setup & Imports](#1-setup--imports)\n",
    "2. [World Model Architecture](#2-world-model-architecture)\n",
    "3. [RSSM Implementation](#3-rssm-implementation)\n",
    "4. [Training Components](#4-training-components)\n",
    "5. [Replay Buffer](#5-replay-buffer)\n",
    "6. [Training Loop](#6-training-loop)\n",
    "7. [Experiments](#7-experiments)\n",
    "8. [Visualizations](#8-visualizations)\n",
    "9. [Save & Export](#9-save--export)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Cell: Imports and Configuration\n",
    "Purpose: Import all required packages and set up environment\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Tuple, Optional, Union, Any, NamedTuple\n",
    "from dataclasses import dataclass, field\n",
    "from collections import deque\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Scientific computing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal, Categorical, OneHotCategorical\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# RL Environment\n",
    "import gymnasium as gym\n",
    "\n",
    "# Visualization\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import seaborn as sns\n",
    "\n",
    "# Progress bar\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Add src to path\n",
    "PROJECT_ROOT = Path.cwd().parent\n",
    "sys.path.insert(0, str(PROJECT_ROOT / \"src\"))\n",
    "\n",
    "# Import our utilities\n",
    "from utils import set_seed, get_device, MetricLogger, timer, COLORS\n",
    "\n",
    "# Configuration\n",
    "SEED = 42\n",
    "set_seed(SEED)\n",
    "DEVICE = get_device()\n",
    "\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"Gymnasium: {gym.__version__}\")\n",
    "print(f\"Project Root: {PROJECT_ROOT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. World Model Architecture\n",
    "\n",
    "The world model follows the DreamerV3 RSSM architecture:\n",
    "- **Encoder**: Converts observations to latent representations\n",
    "- **RSSM**: Recurrent State-Space Model for temporal modeling\n",
    "- **Dynamics Predictor**: Predicts next latent state\n",
    "- **Reward Predictor**: Predicts rewards\n",
    "- **Continue Predictor**: Predicts episode termination\n",
    "- **Decoder**: Reconstructs observations from latent states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Cell: Architecture Diagram\n",
    "Purpose: Visualize the world model architecture\n",
    "\"\"\"\n",
    "\n",
    "def create_architecture_diagram(figsize: Tuple[int, int] = (14, 10)) -> plt.Figure:\n",
    "    \"\"\"\n",
    "    Create a visual diagram of the RSSM world model architecture.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    plt.Figure\n",
    "        The matplotlib figure\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    ax.set_xlim(0, 14)\n",
    "    ax.set_ylim(0, 10)\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # Component colors\n",
    "    colors = {\n",
    "        'input': '#3498db',\n",
    "        'encoder': '#2ecc71',\n",
    "        'rssm': '#9b59b6',\n",
    "        'predictor': '#e74c3c',\n",
    "        'decoder': '#f39c12',\n",
    "        'output': '#1abc9c',\n",
    "    }\n",
    "    \n",
    "    # Draw components\n",
    "    components = [\n",
    "        # (x, y, w, h, label, color_key)\n",
    "        (1, 7, 2.5, 1.2, 'Observation\\n(o_t)', 'input'),\n",
    "        (1, 5, 2.5, 1.2, 'Encoder\\n(CNN/MLP)', 'encoder'),\n",
    "        (5, 4, 4, 3, 'RSSM\\n\\nDeterministic: h_t\\nStochastic: z_t', 'rssm'),\n",
    "        (10.5, 7, 2.5, 1.2, 'Dynamics\\nPredictor', 'predictor'),\n",
    "        (10.5, 5, 2.5, 1.2, 'Reward\\nPredictor', 'predictor'),\n",
    "        (10.5, 3, 2.5, 1.2, 'Continue\\nPredictor', 'predictor'),\n",
    "        (10.5, 1, 2.5, 1.2, 'Decoder', 'decoder'),\n",
    "        (1, 3, 2.5, 1.2, 'Action\\n(a_t)', 'input'),\n",
    "    ]\n",
    "    \n",
    "    for x, y, w, h, label, color_key in components:\n",
    "        rect = mpatches.FancyBboxPatch(\n",
    "            (x, y), w, h,\n",
    "            boxstyle=\"round,pad=0.05\",\n",
    "            facecolor=colors[color_key],\n",
    "            edgecolor='black',\n",
    "            linewidth=2,\n",
    "            alpha=0.85\n",
    "        )\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(x + w/2, y + h/2, label, ha='center', va='center',\n",
    "               fontsize=10, fontweight='bold', color='white')\n",
    "    \n",
    "    # Draw arrows\n",
    "    arrows = [\n",
    "        ((2.25, 7), (2.25, 6.3)),  # obs -> encoder\n",
    "        ((3.6, 5.6), (4.9, 5.5)),  # encoder -> rssm\n",
    "        ((2.25, 4.1), (4.9, 5)),   # action -> rssm\n",
    "        ((9.1, 5.5), (10.4, 7.5)), # rssm -> dynamics\n",
    "        ((9.1, 5.5), (10.4, 5.6)), # rssm -> reward\n",
    "        ((9.1, 5.5), (10.4, 3.6)), # rssm -> continue\n",
    "        ((9.1, 5.5), (10.4, 1.6)), # rssm -> decoder\n",
    "        ((7, 7.1), (7, 8)),        # rssm recurrence (up)\n",
    "        ((7, 8), (5.5, 8)),        # recurrence (left)\n",
    "        ((5.5, 8), (5.5, 7.1)),    # recurrence (down into rssm)\n",
    "    ]\n",
    "    \n",
    "    for start, end in arrows:\n",
    "        ax.annotate(\n",
    "            '', xy=end, xytext=start,\n",
    "            arrowprops=dict(arrowstyle='->', color='black', lw=1.5)\n",
    "        )\n",
    "    \n",
    "    # Labels\n",
    "    ax.text(5.5, 8.3, 'h_{t-1}', fontsize=10, ha='center')\n",
    "    \n",
    "    ax.set_title('DreamerV3-Style RSSM World Model Architecture', \n",
    "                fontsize=16, fontweight='bold', pad=20)\n",
    "    \n",
    "    # Legend\n",
    "    legend_elements = [\n",
    "        mpatches.Patch(facecolor=colors['input'], label='Input'),\n",
    "        mpatches.Patch(facecolor=colors['encoder'], label='Encoder'),\n",
    "        mpatches.Patch(facecolor=colors['rssm'], label='RSSM'),\n",
    "        mpatches.Patch(facecolor=colors['predictor'], label='Predictors'),\n",
    "        mpatches.Patch(facecolor=colors['decoder'], label='Decoder'),\n",
    "    ]\n",
    "    ax.legend(handles=legend_elements, loc='lower right', fontsize=9)\n",
    "    \n",
    "    return fig\n",
    "\n",
    "\n",
    "fig = create_architecture_diagram()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Cell: Encoder Network\n",
    "Purpose: Implement observation encoder (MLP for state-based environments)\n",
    "\"\"\"\n",
    "\n",
    "class MLPEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    MLP encoder for state-based observations.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    obs_dim : int\n",
    "        Dimension of observation space\n",
    "    hidden_dims : List[int]\n",
    "        List of hidden layer dimensions\n",
    "    latent_dim : int\n",
    "        Dimension of latent representation\n",
    "    activation : str\n",
    "        Activation function ('relu', 'elu', 'tanh')\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        obs_dim: int,\n",
    "        hidden_dims: List[int] = [256, 256],\n",
    "        latent_dim: int = 256,\n",
    "        activation: str = 'elu'\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.obs_dim = obs_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        # Build layers\n",
    "        layers = []\n",
    "        in_dim = obs_dim\n",
    "        \n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.append(nn.Linear(in_dim, hidden_dim))\n",
    "            if activation == 'relu':\n",
    "                layers.append(nn.ReLU())\n",
    "            elif activation == 'elu':\n",
    "                layers.append(nn.ELU())\n",
    "            elif activation == 'tanh':\n",
    "                layers.append(nn.Tanh())\n",
    "            in_dim = hidden_dim\n",
    "        \n",
    "        layers.append(nn.Linear(in_dim, latent_dim))\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        \"\"\"Initialize network weights.\"\"\"\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.orthogonal_(m.weight, gain=np.sqrt(2))\n",
    "                nn.init.zeros_(m.bias)\n",
    "    \n",
    "    def forward(self, obs: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Encode observation to latent representation.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        obs : torch.Tensor\n",
    "            Observation tensor of shape (batch, obs_dim)\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            Latent representation of shape (batch, latent_dim)\n",
    "        \"\"\"\n",
    "        return self.network(obs)\n",
    "\n",
    "\n",
    "# Test encoder\n",
    "encoder = MLPEncoder(obs_dim=4, hidden_dims=[64, 64], latent_dim=32).to(DEVICE)\n",
    "test_obs = torch.randn(8, 4, device=DEVICE)\n",
    "test_latent = encoder(test_obs)\n",
    "print(f\"Encoder: {4} -> {test_latent.shape[-1]}\")\n",
    "print(f\"Parameters: {sum(p.numel() for p in encoder.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Cell: Decoder Network\n",
    "Purpose: Implement observation decoder\n",
    "\"\"\"\n",
    "\n",
    "class MLPDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    MLP decoder for reconstructing observations from latent states.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    latent_dim : int\n",
    "        Dimension of latent representation\n",
    "    hidden_dims : List[int]\n",
    "        List of hidden layer dimensions\n",
    "    obs_dim : int\n",
    "        Dimension of observation space\n",
    "    activation : str\n",
    "        Activation function\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        latent_dim: int,\n",
    "        hidden_dims: List[int] = [256, 256],\n",
    "        obs_dim: int = 4,\n",
    "        activation: str = 'elu'\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.latent_dim = latent_dim\n",
    "        self.obs_dim = obs_dim\n",
    "        \n",
    "        layers = []\n",
    "        in_dim = latent_dim\n",
    "        \n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.append(nn.Linear(in_dim, hidden_dim))\n",
    "            if activation == 'relu':\n",
    "                layers.append(nn.ReLU())\n",
    "            elif activation == 'elu':\n",
    "                layers.append(nn.ELU())\n",
    "            elif activation == 'tanh':\n",
    "                layers.append(nn.Tanh())\n",
    "            in_dim = hidden_dim\n",
    "        \n",
    "        layers.append(nn.Linear(in_dim, obs_dim))\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.orthogonal_(m.weight, gain=np.sqrt(2))\n",
    "                nn.init.zeros_(m.bias)\n",
    "    \n",
    "    def forward(self, latent: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Decode latent representation to observation.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        latent : torch.Tensor\n",
    "            Latent tensor of shape (batch, latent_dim)\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            Reconstructed observation of shape (batch, obs_dim)\n",
    "        \"\"\"\n",
    "        return self.network(latent)\n",
    "\n",
    "\n",
    "# Test decoder\n",
    "decoder = MLPDecoder(latent_dim=32, hidden_dims=[64, 64], obs_dim=4).to(DEVICE)\n",
    "test_recon = decoder(test_latent)\n",
    "print(f\"Decoder: {test_latent.shape[-1]} -> {test_recon.shape[-1]}\")\n",
    "print(f\"Parameters: {sum(p.numel() for p in decoder.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. RSSM Implementation\n",
    "\n",
    "The Recurrent State-Space Model (RSSM) is the core of the world model.\n",
    "It maintains both deterministic (h) and stochastic (z) state components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Cell: RSSM State\n",
    "Purpose: Define the RSSM state structure\n",
    "\"\"\"\n",
    "\n",
    "class RSSMState(NamedTuple):\n",
    "    \"\"\"\n",
    "    RSSM state containing deterministic and stochastic components.\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    deter : torch.Tensor\n",
    "        Deterministic state (hidden state from GRU)\n",
    "    stoch : torch.Tensor\n",
    "        Stochastic state (sampled from posterior/prior)\n",
    "    \"\"\"\n",
    "    deter: torch.Tensor  # (batch, deter_dim)\n",
    "    stoch: torch.Tensor  # (batch, stoch_dim)\n",
    "    \n",
    "    @property\n",
    "    def combined(self) -> torch.Tensor:\n",
    "        \"\"\"Get combined state for downstream networks.\"\"\"\n",
    "        return torch.cat([self.deter, self.stoch], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Cell: RSSM Core\n",
    "Purpose: Implement the Recurrent State-Space Model\n",
    "\"\"\"\n",
    "\n",
    "class RSSM(nn.Module):\n",
    "    \"\"\"\n",
    "    Recurrent State-Space Model for world model dynamics.\n",
    "    \n",
    "    Implements the DreamerV3-style RSSM with:\n",
    "    - GRU for deterministic dynamics\n",
    "    - Gaussian distributions for stochastic states\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    stoch_dim : int\n",
    "        Dimension of stochastic state\n",
    "    deter_dim : int\n",
    "        Dimension of deterministic state\n",
    "    hidden_dim : int\n",
    "        Hidden dimension for MLPs\n",
    "    action_dim : int\n",
    "        Dimension of action space\n",
    "    embed_dim : int\n",
    "        Dimension of observation embedding\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        stoch_dim: int = 32,\n",
    "        deter_dim: int = 256,\n",
    "        hidden_dim: int = 256,\n",
    "        action_dim: int = 2,\n",
    "        embed_dim: int = 256,\n",
    "        min_std: float = 0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.stoch_dim = stoch_dim\n",
    "        self.deter_dim = deter_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.embed_dim = embed_dim\n",
    "        self.min_std = min_std\n",
    "        \n",
    "        # Input projection (action + stochastic -> hidden)\n",
    "        self.input_proj = nn.Sequential(\n",
    "            nn.Linear(stoch_dim + action_dim, hidden_dim),\n",
    "            nn.ELU()\n",
    "        )\n",
    "        \n",
    "        # Recurrent cell (GRU)\n",
    "        self.gru = nn.GRUCell(hidden_dim, deter_dim)\n",
    "        \n",
    "        # Prior network (deterministic -> stochastic prior)\n",
    "        self.prior_net = nn.Sequential(\n",
    "            nn.Linear(deter_dim, hidden_dim),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(hidden_dim, 2 * stoch_dim)  # mean and std\n",
    "        )\n",
    "        \n",
    "        # Posterior network (deterministic + embed -> stochastic posterior)\n",
    "        self.posterior_net = nn.Sequential(\n",
    "            nn.Linear(deter_dim + embed_dim, hidden_dim),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(hidden_dim, 2 * stoch_dim)  # mean and std\n",
    "        )\n",
    "        \n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        \"\"\"Initialize weights.\"\"\"\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.orthogonal_(m.weight, gain=1.0)\n",
    "                nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.GRUCell):\n",
    "                for name, param in m.named_parameters():\n",
    "                    if 'weight' in name:\n",
    "                        nn.init.orthogonal_(param)\n",
    "                    elif 'bias' in name:\n",
    "                        nn.init.zeros_(param)\n",
    "    \n",
    "    def initial_state(self, batch_size: int, device: torch.device) -> RSSMState:\n",
    "        \"\"\"\n",
    "        Get initial RSSM state.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        batch_size : int\n",
    "            Batch size\n",
    "        device : torch.device\n",
    "            Device for tensors\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        RSSMState\n",
    "            Initial state with zeros\n",
    "        \"\"\"\n",
    "        return RSSMState(\n",
    "            deter=torch.zeros(batch_size, self.deter_dim, device=device),\n",
    "            stoch=torch.zeros(batch_size, self.stoch_dim, device=device)\n",
    "        )\n",
    "    \n",
    "    def _get_dist(self, stats: torch.Tensor) -> Normal:\n",
    "        \"\"\"\n",
    "        Get distribution from network output.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        stats : torch.Tensor\n",
    "            Network output of shape (batch, 2 * stoch_dim)\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Normal\n",
    "            Gaussian distribution\n",
    "        \"\"\"\n",
    "        mean, std = torch.chunk(stats, 2, dim=-1)\n",
    "        std = F.softplus(std) + self.min_std\n",
    "        return Normal(mean, std)\n",
    "    \n",
    "    def imagine_step(\n",
    "        self,\n",
    "        prev_state: RSSMState,\n",
    "        action: torch.Tensor\n",
    "    ) -> Tuple[RSSMState, Normal]:\n",
    "        \"\"\"\n",
    "        Imagine one step forward using prior (no observation).\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        prev_state : RSSMState\n",
    "            Previous RSSM state\n",
    "        action : torch.Tensor\n",
    "            Action taken, shape (batch, action_dim)\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Tuple[RSSMState, Normal]\n",
    "            Next state and prior distribution\n",
    "        \"\"\"\n",
    "        # Project input\n",
    "        x = self.input_proj(torch.cat([prev_state.stoch, action], dim=-1))\n",
    "        \n",
    "        # Update deterministic state\n",
    "        deter = self.gru(x, prev_state.deter)\n",
    "        \n",
    "        # Get prior distribution\n",
    "        prior_stats = self.prior_net(deter)\n",
    "        prior_dist = self._get_dist(prior_stats)\n",
    "        \n",
    "        # Sample stochastic state\n",
    "        stoch = prior_dist.rsample()\n",
    "        \n",
    "        return RSSMState(deter=deter, stoch=stoch), prior_dist\n",
    "    \n",
    "    def observe_step(\n",
    "        self,\n",
    "        prev_state: RSSMState,\n",
    "        action: torch.Tensor,\n",
    "        embed: torch.Tensor\n",
    "    ) -> Tuple[RSSMState, Normal, Normal]:\n",
    "        \"\"\"\n",
    "        Observe one step forward using posterior (with observation).\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        prev_state : RSSMState\n",
    "            Previous RSSM state\n",
    "        action : torch.Tensor\n",
    "            Action taken\n",
    "        embed : torch.Tensor\n",
    "            Observation embedding\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Tuple[RSSMState, Normal, Normal]\n",
    "            Next state, prior distribution, and posterior distribution\n",
    "        \"\"\"\n",
    "        # Get prior prediction\n",
    "        prior_state, prior_dist = self.imagine_step(prev_state, action)\n",
    "        \n",
    "        # Get posterior distribution using observation\n",
    "        posterior_stats = self.posterior_net(\n",
    "            torch.cat([prior_state.deter, embed], dim=-1)\n",
    "        )\n",
    "        posterior_dist = self._get_dist(posterior_stats)\n",
    "        \n",
    "        # Sample from posterior\n",
    "        stoch = posterior_dist.rsample()\n",
    "        \n",
    "        return RSSMState(deter=prior_state.deter, stoch=stoch), prior_dist, posterior_dist\n",
    "\n",
    "\n",
    "# Test RSSM\n",
    "rssm = RSSM(\n",
    "    stoch_dim=32,\n",
    "    deter_dim=128,\n",
    "    hidden_dim=128,\n",
    "    action_dim=2,\n",
    "    embed_dim=32\n",
    ").to(DEVICE)\n",
    "\n",
    "batch_size = 8\n",
    "state = rssm.initial_state(batch_size, DEVICE)\n",
    "action = torch.randn(batch_size, 2, device=DEVICE)\n",
    "embed = torch.randn(batch_size, 32, device=DEVICE)\n",
    "\n",
    "# Test imagine step\n",
    "next_state, prior = rssm.imagine_step(state, action)\n",
    "print(f\"Imagine step - Deter: {next_state.deter.shape}, Stoch: {next_state.stoch.shape}\")\n",
    "\n",
    "# Test observe step\n",
    "next_state, prior, posterior = rssm.observe_step(state, action, embed)\n",
    "print(f\"Observe step - Deter: {next_state.deter.shape}, Stoch: {next_state.stoch.shape}\")\n",
    "print(f\"RSSM Parameters: {sum(p.numel() for p in rssm.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Cell: Predictor Networks\n",
    "Purpose: Implement reward and continue predictors\n",
    "\"\"\"\n",
    "\n",
    "class RewardPredictor(nn.Module):\n",
    "    \"\"\"\n",
    "    Predict rewards from RSSM state.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    state_dim : int\n",
    "        Combined RSSM state dimension (deter + stoch)\n",
    "    hidden_dims : List[int]\n",
    "        Hidden layer dimensions\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        state_dim: int,\n",
    "        hidden_dims: List[int] = [256, 256]\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        layers = []\n",
    "        in_dim = state_dim\n",
    "        \n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(in_dim, hidden_dim),\n",
    "                nn.ELU()\n",
    "            ])\n",
    "            in_dim = hidden_dim\n",
    "        \n",
    "        layers.append(nn.Linear(in_dim, 1))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, state: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Predict reward from combined RSSM state.\"\"\"\n",
    "        return self.network(state).squeeze(-1)\n",
    "\n",
    "\n",
    "class ContinuePredictor(nn.Module):\n",
    "    \"\"\"\n",
    "    Predict episode continuation probability from RSSM state.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    state_dim : int\n",
    "        Combined RSSM state dimension\n",
    "    hidden_dims : List[int]\n",
    "        Hidden layer dimensions\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        state_dim: int,\n",
    "        hidden_dims: List[int] = [256, 256]\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        layers = []\n",
    "        in_dim = state_dim\n",
    "        \n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(in_dim, hidden_dim),\n",
    "                nn.ELU()\n",
    "            ])\n",
    "            in_dim = hidden_dim\n",
    "        \n",
    "        layers.append(nn.Linear(in_dim, 1))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, state: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Predict continuation logit from combined RSSM state.\"\"\"\n",
    "        return self.network(state).squeeze(-1)\n",
    "\n",
    "\n",
    "# Test predictors\n",
    "state_dim = 128 + 32  # deter + stoch\n",
    "reward_pred = RewardPredictor(state_dim, [64, 64]).to(DEVICE)\n",
    "continue_pred = ContinuePredictor(state_dim, [64, 64]).to(DEVICE)\n",
    "\n",
    "test_state = next_state.combined\n",
    "pred_reward = reward_pred(test_state)\n",
    "pred_continue = continue_pred(test_state)\n",
    "\n",
    "print(f\"Predicted reward shape: {pred_reward.shape}\")\n",
    "print(f\"Predicted continue shape: {pred_continue.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Training Components\n",
    "\n",
    "Implement the complete world model and loss functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Cell: Complete World Model\n",
    "Purpose: Combine all components into a complete world model\n",
    "\"\"\"\n",
    "\n",
    "class WorldModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Complete DreamerV3-style World Model.\n",
    "    \n",
    "    Combines encoder, RSSM, predictors, and decoder.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    obs_dim : int\n",
    "        Observation dimension\n",
    "    action_dim : int\n",
    "        Action dimension\n",
    "    stoch_dim : int\n",
    "        Stochastic state dimension\n",
    "    deter_dim : int\n",
    "        Deterministic state dimension\n",
    "    hidden_dim : int\n",
    "        Hidden layer dimension\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        obs_dim: int,\n",
    "        action_dim: int,\n",
    "        stoch_dim: int = 32,\n",
    "        deter_dim: int = 256,\n",
    "        hidden_dim: int = 256,\n",
    "        encoder_hidden: List[int] = [256, 256],\n",
    "        decoder_hidden: List[int] = [256, 256],\n",
    "        predictor_hidden: List[int] = [256, 256]\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.obs_dim = obs_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.stoch_dim = stoch_dim\n",
    "        self.deter_dim = deter_dim\n",
    "        self.state_dim = stoch_dim + deter_dim\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = MLPEncoder(\n",
    "            obs_dim=obs_dim,\n",
    "            hidden_dims=encoder_hidden,\n",
    "            latent_dim=hidden_dim\n",
    "        )\n",
    "        \n",
    "        # RSSM\n",
    "        self.rssm = RSSM(\n",
    "            stoch_dim=stoch_dim,\n",
    "            deter_dim=deter_dim,\n",
    "            hidden_dim=hidden_dim,\n",
    "            action_dim=action_dim,\n",
    "            embed_dim=hidden_dim\n",
    "        )\n",
    "        \n",
    "        # Predictors\n",
    "        self.reward_predictor = RewardPredictor(\n",
    "            state_dim=self.state_dim,\n",
    "            hidden_dims=predictor_hidden\n",
    "        )\n",
    "        self.continue_predictor = ContinuePredictor(\n",
    "            state_dim=self.state_dim,\n",
    "            hidden_dims=predictor_hidden\n",
    "        )\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = MLPDecoder(\n",
    "            latent_dim=self.state_dim,\n",
    "            hidden_dims=decoder_hidden,\n",
    "            obs_dim=obs_dim\n",
    "        )\n",
    "    \n",
    "    def initial_state(self, batch_size: int) -> RSSMState:\n",
    "        \"\"\"Get initial RSSM state.\"\"\"\n",
    "        device = next(self.parameters()).device\n",
    "        return self.rssm.initial_state(batch_size, device)\n",
    "    \n",
    "    def observe(\n",
    "        self,\n",
    "        obs: torch.Tensor,\n",
    "        action: torch.Tensor,\n",
    "        prev_state: RSSMState\n",
    "    ) -> Tuple[RSSMState, Normal, Normal]:\n",
    "        \"\"\"\n",
    "        Process observation through the world model.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        obs : torch.Tensor\n",
    "            Observation tensor\n",
    "        action : torch.Tensor\n",
    "            Action tensor\n",
    "        prev_state : RSSMState\n",
    "            Previous RSSM state\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Tuple[RSSMState, Normal, Normal]\n",
    "            New state, prior, and posterior distributions\n",
    "        \"\"\"\n",
    "        embed = self.encoder(obs)\n",
    "        return self.rssm.observe_step(prev_state, action, embed)\n",
    "    \n",
    "    def imagine(\n",
    "        self,\n",
    "        action: torch.Tensor,\n",
    "        prev_state: RSSMState\n",
    "    ) -> Tuple[RSSMState, Normal]:\n",
    "        \"\"\"\n",
    "        Imagine next state without observation.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        action : torch.Tensor\n",
    "            Action tensor\n",
    "        prev_state : RSSMState\n",
    "            Previous RSSM state\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Tuple[RSSMState, Normal]\n",
    "            New state and prior distribution\n",
    "        \"\"\"\n",
    "        return self.rssm.imagine_step(prev_state, action)\n",
    "    \n",
    "    def predict(\n",
    "        self,\n",
    "        state: RSSMState\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Get predictions from RSSM state.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        state : RSSMState\n",
    "            Current RSSM state\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Tuple[torch.Tensor, torch.Tensor, torch.Tensor]\n",
    "            Reconstructed observation, predicted reward, predicted continue logit\n",
    "        \"\"\"\n",
    "        combined = state.combined\n",
    "        recon_obs = self.decoder(combined)\n",
    "        pred_reward = self.reward_predictor(combined)\n",
    "        pred_continue = self.continue_predictor(combined)\n",
    "        return recon_obs, pred_reward, pred_continue\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        obs_seq: torch.Tensor,\n",
    "        action_seq: torch.Tensor\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Process a sequence of observations and actions.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        obs_seq : torch.Tensor\n",
    "            Observation sequence (batch, seq_len, obs_dim)\n",
    "        action_seq : torch.Tensor\n",
    "            Action sequence (batch, seq_len, action_dim)\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Dict[str, torch.Tensor]\n",
    "            Dictionary with predictions and distributions\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, _ = obs_seq.shape\n",
    "        device = obs_seq.device\n",
    "        \n",
    "        # Initialize\n",
    "        state = self.initial_state(batch_size)\n",
    "        \n",
    "        # Storage\n",
    "        priors = []\n",
    "        posteriors = []\n",
    "        recon_obs_list = []\n",
    "        pred_rewards = []\n",
    "        pred_continues = []\n",
    "        \n",
    "        # Process sequence\n",
    "        for t in range(seq_len):\n",
    "            obs_t = obs_seq[:, t]\n",
    "            action_t = action_seq[:, t]\n",
    "            \n",
    "            # Observe step\n",
    "            state, prior, posterior = self.observe(obs_t, action_t, state)\n",
    "            \n",
    "            # Get predictions\n",
    "            recon_obs, pred_reward, pred_continue = self.predict(state)\n",
    "            \n",
    "            # Store\n",
    "            priors.append(prior)\n",
    "            posteriors.append(posterior)\n",
    "            recon_obs_list.append(recon_obs)\n",
    "            pred_rewards.append(pred_reward)\n",
    "            pred_continues.append(pred_continue)\n",
    "        \n",
    "        return {\n",
    "            'recon_obs': torch.stack(recon_obs_list, dim=1),\n",
    "            'pred_rewards': torch.stack(pred_rewards, dim=1),\n",
    "            'pred_continues': torch.stack(pred_continues, dim=1),\n",
    "            'priors': priors,\n",
    "            'posteriors': posteriors,\n",
    "        }\n",
    "\n",
    "\n",
    "# Test world model\n",
    "world_model = WorldModel(\n",
    "    obs_dim=4,\n",
    "    action_dim=2,\n",
    "    stoch_dim=32,\n",
    "    deter_dim=128,\n",
    "    hidden_dim=128,\n",
    "    encoder_hidden=[64, 64],\n",
    "    decoder_hidden=[64, 64],\n",
    "    predictor_hidden=[64, 64]\n",
    ").to(DEVICE)\n",
    "\n",
    "print(f\"World Model Parameters: {sum(p.numel() for p in world_model.parameters()):,}\")\n",
    "\n",
    "# Test forward pass\n",
    "batch_size, seq_len = 8, 20\n",
    "test_obs_seq = torch.randn(batch_size, seq_len, 4, device=DEVICE)\n",
    "test_action_seq = torch.randn(batch_size, seq_len, 2, device=DEVICE)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = world_model(test_obs_seq, test_action_seq)\n",
    "\n",
    "print(f\"Reconstructed obs shape: {output['recon_obs'].shape}\")\n",
    "print(f\"Predicted rewards shape: {output['pred_rewards'].shape}\")\n",
    "print(f\"Predicted continues shape: {output['pred_continues'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Cell: Loss Functions\n",
    "Purpose: Implement world model training losses\n",
    "\"\"\"\n",
    "\n",
    "def compute_world_model_loss(\n",
    "    model_output: Dict[str, torch.Tensor],\n",
    "    obs_seq: torch.Tensor,\n",
    "    reward_seq: torch.Tensor,\n",
    "    continue_seq: torch.Tensor,\n",
    "    kl_weight: float = 1.0,\n",
    "    recon_weight: float = 1.0,\n",
    "    reward_weight: float = 1.0,\n",
    "    continue_weight: float = 1.0,\n",
    "    free_nats: float = 1.0\n",
    ") -> Dict[str, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Compute world model training loss.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model_output : Dict[str, torch.Tensor]\n",
    "        Output from world model forward pass\n",
    "    obs_seq : torch.Tensor\n",
    "        Ground truth observations (batch, seq_len, obs_dim)\n",
    "    reward_seq : torch.Tensor\n",
    "        Ground truth rewards (batch, seq_len)\n",
    "    continue_seq : torch.Tensor\n",
    "        Ground truth continue flags (batch, seq_len)\n",
    "    kl_weight : float\n",
    "        Weight for KL divergence loss\n",
    "    recon_weight : float\n",
    "        Weight for reconstruction loss\n",
    "    reward_weight : float\n",
    "        Weight for reward prediction loss\n",
    "    continue_weight : float\n",
    "        Weight for continue prediction loss\n",
    "    free_nats : float\n",
    "        Free nats for KL loss (minimum KL)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Dict[str, torch.Tensor]\n",
    "        Dictionary with individual losses and total loss\n",
    "    \"\"\"\n",
    "    # Reconstruction loss (MSE)\n",
    "    recon_loss = F.mse_loss(model_output['recon_obs'], obs_seq)\n",
    "    \n",
    "    # Reward prediction loss (MSE)\n",
    "    reward_loss = F.mse_loss(model_output['pred_rewards'], reward_seq)\n",
    "    \n",
    "    # Continue prediction loss (BCE with logits)\n",
    "    continue_loss = F.binary_cross_entropy_with_logits(\n",
    "        model_output['pred_continues'],\n",
    "        continue_seq\n",
    "    )\n",
    "    \n",
    "    # KL divergence loss\n",
    "    kl_losses = []\n",
    "    for prior, posterior in zip(model_output['priors'], model_output['posteriors']):\n",
    "        kl = torch.distributions.kl_divergence(posterior, prior).sum(-1)\n",
    "        kl = torch.clamp(kl, min=free_nats).mean()\n",
    "        kl_losses.append(kl)\n",
    "    kl_loss = torch.stack(kl_losses).mean()\n",
    "    \n",
    "    # Total loss\n",
    "    total_loss = (\n",
    "        kl_weight * kl_loss +\n",
    "        recon_weight * recon_loss +\n",
    "        reward_weight * reward_loss +\n",
    "        continue_weight * continue_loss\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'total': total_loss,\n",
    "        'kl': kl_loss,\n",
    "        'recon': recon_loss,\n",
    "        'reward': reward_loss,\n",
    "        'continue': continue_loss\n",
    "    }\n",
    "\n",
    "\n",
    "# Test loss computation\n",
    "test_rewards = torch.randn(batch_size, seq_len, device=DEVICE)\n",
    "test_continues = torch.ones(batch_size, seq_len, device=DEVICE)\n",
    "\n",
    "losses = compute_world_model_loss(\n",
    "    output, test_obs_seq, test_rewards, test_continues\n",
    ")\n",
    "\n",
    "print(\"Loss components:\")\n",
    "for name, value in losses.items():\n",
    "    print(f\"  {name}: {value.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Cell: Replay Buffer\n",
    "Purpose: Implement experience replay buffer for training\n",
    "\"\"\"\n",
    "\n",
    "@dataclass\n",
    "class Episode:\n",
    "    \"\"\"\n",
    "    Container for a single episode of experience.\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    observations : np.ndarray\n",
    "        Array of observations (T, obs_dim)\n",
    "    actions : np.ndarray\n",
    "        Array of actions (T, action_dim)\n",
    "    rewards : np.ndarray\n",
    "        Array of rewards (T,)\n",
    "    dones : np.ndarray\n",
    "        Array of done flags (T,)\n",
    "    \"\"\"\n",
    "    observations: np.ndarray\n",
    "    actions: np.ndarray\n",
    "    rewards: np.ndarray\n",
    "    dones: np.ndarray\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.observations)\n",
    "\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"\n",
    "    Replay buffer for storing and sampling episodes.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    capacity : int\n",
    "        Maximum number of episodes to store\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, capacity: int = 1000):\n",
    "        self.capacity = capacity\n",
    "        self.episodes: List[Episode] = []\n",
    "        self.total_steps = 0\n",
    "    \n",
    "    def add_episode(self, episode: Episode) -> None:\n",
    "        \"\"\"\n",
    "        Add an episode to the buffer.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        episode : Episode\n",
    "            Episode to add\n",
    "        \"\"\"\n",
    "        if len(self.episodes) >= self.capacity:\n",
    "            removed = self.episodes.pop(0)\n",
    "            self.total_steps -= len(removed)\n",
    "        \n",
    "        self.episodes.append(episode)\n",
    "        self.total_steps += len(episode)\n",
    "    \n",
    "    def sample_sequences(\n",
    "        self,\n",
    "        batch_size: int,\n",
    "        seq_len: int\n",
    "    ) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Sample random sequences from episodes.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        batch_size : int\n",
    "            Number of sequences to sample\n",
    "        seq_len : int\n",
    "            Length of each sequence\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Tuple[np.ndarray, ...]\n",
    "            (observations, actions, rewards, continues)\n",
    "        \"\"\"\n",
    "        # Filter episodes that are long enough\n",
    "        valid_episodes = [ep for ep in self.episodes if len(ep) >= seq_len]\n",
    "        \n",
    "        if not valid_episodes:\n",
    "            raise ValueError(f\"No episodes with length >= {seq_len}\")\n",
    "        \n",
    "        obs_batch = []\n",
    "        action_batch = []\n",
    "        reward_batch = []\n",
    "        continue_batch = []\n",
    "        \n",
    "        for _ in range(batch_size):\n",
    "            # Random episode\n",
    "            episode = valid_episodes[np.random.randint(len(valid_episodes))]\n",
    "            \n",
    "            # Random start position\n",
    "            max_start = len(episode) - seq_len\n",
    "            start = np.random.randint(0, max_start + 1)\n",
    "            \n",
    "            # Extract sequence\n",
    "            obs_batch.append(episode.observations[start:start + seq_len])\n",
    "            action_batch.append(episode.actions[start:start + seq_len])\n",
    "            reward_batch.append(episode.rewards[start:start + seq_len])\n",
    "            # Continue is 1 - done\n",
    "            continue_batch.append(1.0 - episode.dones[start:start + seq_len])\n",
    "        \n",
    "        return (\n",
    "            np.stack(obs_batch),\n",
    "            np.stack(action_batch),\n",
    "            np.stack(reward_batch),\n",
    "            np.stack(continue_batch)\n",
    "        )\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.episodes)\n",
    "\n",
    "\n",
    "def collect_episodes(\n",
    "    env_name: str,\n",
    "    num_episodes: int = 100,\n",
    "    seed: int = 42\n",
    ") -> List[Episode]:\n",
    "    \"\"\"\n",
    "    Collect episodes from an environment using random actions.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    env_name : str\n",
    "        Gymnasium environment name\n",
    "    num_episodes : int\n",
    "        Number of episodes to collect\n",
    "    seed : int\n",
    "        Random seed\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    List[Episode]\n",
    "        List of collected episodes\n",
    "    \"\"\"\n",
    "    env = gym.make(env_name)\n",
    "    episodes = []\n",
    "    \n",
    "    for ep_idx in tqdm(range(num_episodes), desc=\"Collecting episodes\"):\n",
    "        obs_list = []\n",
    "        action_list = []\n",
    "        reward_list = []\n",
    "        done_list = []\n",
    "        \n",
    "        obs, _ = env.reset(seed=seed + ep_idx)\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            action = env.action_space.sample()\n",
    "            \n",
    "            # Store current transition\n",
    "            obs_list.append(obs)\n",
    "            \n",
    "            # Handle discrete actions - convert to one-hot for network\n",
    "            if isinstance(env.action_space, gym.spaces.Discrete):\n",
    "                action_onehot = np.zeros(env.action_space.n)\n",
    "                action_onehot[action] = 1.0\n",
    "                action_list.append(action_onehot)\n",
    "            else:\n",
    "                action_list.append(action)\n",
    "            \n",
    "            obs, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            reward_list.append(reward)\n",
    "            done_list.append(float(done))\n",
    "        \n",
    "        episodes.append(Episode(\n",
    "            observations=np.array(obs_list, dtype=np.float32),\n",
    "            actions=np.array(action_list, dtype=np.float32),\n",
    "            rewards=np.array(reward_list, dtype=np.float32),\n",
    "            dones=np.array(done_list, dtype=np.float32)\n",
    "        ))\n",
    "    \n",
    "    env.close()\n",
    "    return episodes\n",
    "\n",
    "\n",
    "# Collect episodes from CartPole\n",
    "print(\"Collecting training data...\")\n",
    "episodes = collect_episodes(\"CartPole-v1\", num_episodes=100, seed=SEED)\n",
    "\n",
    "# Create replay buffer\n",
    "replay_buffer = ReplayBuffer(capacity=1000)\n",
    "for ep in episodes:\n",
    "    replay_buffer.add_episode(ep)\n",
    "\n",
    "print(f\"Buffer size: {len(replay_buffer)} episodes, {replay_buffer.total_steps} steps\")\n",
    "\n",
    "# Test sampling\n",
    "obs, actions, rewards, continues = replay_buffer.sample_sequences(batch_size=8, seq_len=20)\n",
    "print(f\"Sample shapes: obs={obs.shape}, actions={actions.shape}, rewards={rewards.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Cell: Trainer Class\n",
    "Purpose: Implement the training loop for the world model\n",
    "\"\"\"\n",
    "\n",
    "class WorldModelTrainer:\n",
    "    \"\"\"\n",
    "    Trainer for the world model.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model : WorldModel\n",
    "        World model to train\n",
    "    replay_buffer : ReplayBuffer\n",
    "        Replay buffer with episodes\n",
    "    lr : float\n",
    "        Learning rate\n",
    "    grad_clip : float\n",
    "        Gradient clipping value\n",
    "    device : torch.device\n",
    "        Device for training\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model: WorldModel,\n",
    "        replay_buffer: ReplayBuffer,\n",
    "        lr: float = 3e-4,\n",
    "        grad_clip: float = 100.0,\n",
    "        device: torch.device = DEVICE\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.replay_buffer = replay_buffer\n",
    "        self.device = device\n",
    "        self.grad_clip = grad_clip\n",
    "        \n",
    "        self.optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "        self.logger = MetricLogger(name=\"world_model\")\n",
    "    \n",
    "    def train_step(\n",
    "        self,\n",
    "        batch_size: int = 32,\n",
    "        seq_len: int = 50,\n",
    "        kl_weight: float = 1.0\n",
    "    ) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Perform a single training step.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        batch_size : int\n",
    "            Batch size\n",
    "        seq_len : int\n",
    "            Sequence length\n",
    "        kl_weight : float\n",
    "            Weight for KL loss\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Dict[str, float]\n",
    "            Dictionary with loss values\n",
    "        \"\"\"\n",
    "        self.model.train()\n",
    "        \n",
    "        # Sample batch\n",
    "        obs, actions, rewards, continues = self.replay_buffer.sample_sequences(\n",
    "            batch_size, seq_len\n",
    "        )\n",
    "        \n",
    "        # Convert to tensors\n",
    "        obs = torch.tensor(obs, dtype=torch.float32, device=self.device)\n",
    "        actions = torch.tensor(actions, dtype=torch.float32, device=self.device)\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32, device=self.device)\n",
    "        continues = torch.tensor(continues, dtype=torch.float32, device=self.device)\n",
    "        \n",
    "        # Forward pass\n",
    "        output = self.model(obs, actions)\n",
    "        \n",
    "        # Compute loss\n",
    "        losses = compute_world_model_loss(\n",
    "            output, obs, rewards, continues, kl_weight=kl_weight\n",
    "        )\n",
    "        \n",
    "        # Backward pass\n",
    "        self.optimizer.zero_grad()\n",
    "        losses['total'].backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        grad_norm = nn.utils.clip_grad_norm_(\n",
    "            self.model.parameters(), self.grad_clip\n",
    "        )\n",
    "        \n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # Log\n",
    "        result = {k: v.item() for k, v in losses.items()}\n",
    "        result['grad_norm'] = grad_norm.item()\n",
    "        \n",
    "        self.logger.log(**result)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def train(\n",
    "        self,\n",
    "        num_steps: int = 10000,\n",
    "        batch_size: int = 32,\n",
    "        seq_len: int = 50,\n",
    "        log_every: int = 100,\n",
    "        kl_weight: float = 1.0\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Train the world model.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        num_steps : int\n",
    "            Number of training steps\n",
    "        batch_size : int\n",
    "            Batch size\n",
    "        seq_len : int\n",
    "            Sequence length\n",
    "        log_every : int\n",
    "            Log every N steps\n",
    "        kl_weight : float\n",
    "            Weight for KL loss\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        pd.DataFrame\n",
    "            Training history\n",
    "        \"\"\"\n",
    "        pbar = tqdm(range(num_steps), desc=\"Training\")\n",
    "        \n",
    "        for step in pbar:\n",
    "            losses = self.train_step(batch_size, seq_len, kl_weight)\n",
    "            \n",
    "            if step % log_every == 0:\n",
    "                pbar.set_postfix({\n",
    "                    'total': f\"{self.logger.get_mean('total', 100):.4f}\",\n",
    "                    'recon': f\"{self.logger.get_mean('recon', 100):.4f}\",\n",
    "                    'kl': f\"{self.logger.get_mean('kl', 100):.4f}\"\n",
    "                })\n",
    "        \n",
    "        return self.logger.to_dataframe()\n",
    "\n",
    "\n",
    "print(\"Trainer class defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Cell: CartPole Experiment\n",
    "Purpose: Train world model on CartPole-v1\n",
    "\"\"\"\n",
    "\n",
    "# Create world model for CartPole\n",
    "cartpole_model = WorldModel(\n",
    "    obs_dim=4,      # CartPole observation dimension\n",
    "    action_dim=2,   # One-hot encoded actions\n",
    "    stoch_dim=32,\n",
    "    deter_dim=128,\n",
    "    hidden_dim=128,\n",
    "    encoder_hidden=[128, 128],\n",
    "    decoder_hidden=[128, 128],\n",
    "    predictor_hidden=[128, 128]\n",
    ").to(DEVICE)\n",
    "\n",
    "print(f\"CartPole World Model Parameters: {sum(p.numel() for p in cartpole_model.parameters()):,}\")\n",
    "\n",
    "# Create trainer\n",
    "trainer = WorldModelTrainer(\n",
    "    model=cartpole_model,\n",
    "    replay_buffer=replay_buffer,\n",
    "    lr=3e-4,\n",
    "    grad_clip=100.0,\n",
    "    device=DEVICE\n",
    ")\n",
    "\n",
    "# Train\n",
    "print(\"\\nTraining on CartPole-v1...\")\n",
    "history = trainer.train(\n",
    "    num_steps=5000,\n",
    "    batch_size=32,\n",
    "    seq_len=20,\n",
    "    log_every=100,\n",
    "    kl_weight=1.0\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining complete! Final losses:\")\n",
    "print(f\"  Total: {history['total'].iloc[-100:].mean():.4f}\")\n",
    "print(f\"  Recon: {history['recon'].iloc[-100:].mean():.4f}\")\n",
    "print(f\"  KL: {history['kl'].iloc[-100:].mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Cell: Loss Curves\n",
    "Purpose: Plot training loss curves\n",
    "\"\"\"\n",
    "\n",
    "def plot_training_curves(history: pd.DataFrame, figsize: Tuple[int, int] = (14, 10)):\n",
    "    \"\"\"\n",
    "    Plot training loss curves.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    history : pd.DataFrame\n",
    "        Training history from trainer\n",
    "    figsize : Tuple[int, int]\n",
    "        Figure size\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=figsize)\n",
    "    \n",
    "    # Smoothing window\n",
    "    window = 50\n",
    "    \n",
    "    # Total loss\n",
    "    ax = axes[0, 0]\n",
    "    ax.plot(history['total'], alpha=0.3, color=COLORS['baseline'])\n",
    "    ax.plot(history['total'].rolling(window).mean(), color=COLORS['baseline'], linewidth=2)\n",
    "    ax.set_xlabel('Step')\n",
    "    ax.set_ylabel('Loss')\n",
    "    ax.set_title('Total Loss')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Reconstruction loss\n",
    "    ax = axes[0, 1]\n",
    "    ax.plot(history['recon'], alpha=0.3, color=COLORS['qaoa'])\n",
    "    ax.plot(history['recon'].rolling(window).mean(), color=COLORS['qaoa'], linewidth=2)\n",
    "    ax.set_xlabel('Step')\n",
    "    ax.set_ylabel('Loss')\n",
    "    ax.set_title('Reconstruction Loss')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # KL loss\n",
    "    ax = axes[1, 0]\n",
    "    ax.plot(history['kl'], alpha=0.3, color=COLORS['superposition'])\n",
    "    ax.plot(history['kl'].rolling(window).mean(), color=COLORS['superposition'], linewidth=2)\n",
    "    ax.set_xlabel('Step')\n",
    "    ax.set_ylabel('Loss')\n",
    "    ax.set_title('KL Divergence Loss')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Gradient norm\n",
    "    ax = axes[1, 1]\n",
    "    ax.plot(history['grad_norm'], alpha=0.3, color=COLORS['gates'])\n",
    "    ax.plot(history['grad_norm'].rolling(window).mean(), color=COLORS['gates'], linewidth=2)\n",
    "    ax.set_xlabel('Step')\n",
    "    ax.set_ylabel('Norm')\n",
    "    ax.set_title('Gradient Norm')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    fig.suptitle('Classical Baseline World Model Training', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "\n",
    "fig = plot_training_curves(history)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Cell: Prediction Accuracy\n",
    "Purpose: Evaluate and visualize prediction accuracy\n",
    "\"\"\"\n",
    "\n",
    "def evaluate_predictions(model: WorldModel, buffer: ReplayBuffer, num_samples: int = 100):\n",
    "    \"\"\"\n",
    "    Evaluate world model prediction accuracy.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model : WorldModel\n",
    "        Trained world model\n",
    "    buffer : ReplayBuffer\n",
    "        Replay buffer with test data\n",
    "    num_samples : int\n",
    "        Number of samples to evaluate\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Dict[str, np.ndarray]\n",
    "        Dictionary with evaluation results\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Sample test data\n",
    "    obs, actions, rewards, continues = buffer.sample_sequences(num_samples, seq_len=20)\n",
    "    \n",
    "    obs = torch.tensor(obs, dtype=torch.float32, device=DEVICE)\n",
    "    actions = torch.tensor(actions, dtype=torch.float32, device=DEVICE)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(obs, actions)\n",
    "    \n",
    "    # Compute errors\n",
    "    recon_error = (output['recon_obs'] - obs).pow(2).mean(dim=-1).cpu().numpy()\n",
    "    reward_error = (output['pred_rewards'].cpu().numpy() - rewards) ** 2\n",
    "    \n",
    "    return {\n",
    "        'recon_error': recon_error,\n",
    "        'reward_error': reward_error,\n",
    "        'true_obs': obs.cpu().numpy(),\n",
    "        'pred_obs': output['recon_obs'].cpu().numpy(),\n",
    "    }\n",
    "\n",
    "\n",
    "# Evaluate\n",
    "eval_results = evaluate_predictions(cartpole_model, replay_buffer)\n",
    "\n",
    "# Plot prediction accuracy\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Reconstruction error over sequence\n",
    "ax = axes[0]\n",
    "mean_error = eval_results['recon_error'].mean(axis=0)\n",
    "std_error = eval_results['recon_error'].std(axis=0)\n",
    "x = np.arange(len(mean_error))\n",
    "\n",
    "ax.plot(x, mean_error, color=COLORS['baseline'], linewidth=2)\n",
    "ax.fill_between(x, mean_error - std_error, mean_error + std_error, \n",
    "               color=COLORS['baseline'], alpha=0.3)\n",
    "ax.set_xlabel('Timestep')\n",
    "ax.set_ylabel('MSE')\n",
    "ax.set_title('Reconstruction Error Over Sequence')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# True vs Predicted observation (single dimension)\n",
    "ax = axes[1]\n",
    "sample_idx = 0\n",
    "dim_idx = 0\n",
    "true = eval_results['true_obs'][sample_idx, :, dim_idx]\n",
    "pred = eval_results['pred_obs'][sample_idx, :, dim_idx]\n",
    "\n",
    "ax.plot(true, label='True', color=COLORS['baseline'], linewidth=2)\n",
    "ax.plot(pred, label='Predicted', color=COLORS['qaoa'], linestyle='--', linewidth=2)\n",
    "ax.set_xlabel('Timestep')\n",
    "ax.set_ylabel('Value')\n",
    "ax.set_title(f'Observation Reconstruction (dim {dim_idx})')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "fig.suptitle('World Model Prediction Accuracy', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Mean reconstruction error: {eval_results['recon_error'].mean():.6f}\")\n",
    "print(f\"Mean reward prediction error: {eval_results['reward_error'].mean():.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Cell: Latent Space Visualization\n",
    "Purpose: Visualize latent representations using t-SNE\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "def visualize_latent_space(\n",
    "    model: WorldModel,\n",
    "    buffer: ReplayBuffer,\n",
    "    num_samples: int = 500\n",
    "):\n",
    "    \"\"\"\n",
    "    Visualize latent representations using t-SNE.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model : WorldModel\n",
    "        Trained world model\n",
    "    buffer : ReplayBuffer\n",
    "        Replay buffer\n",
    "    num_samples : int\n",
    "        Number of samples\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Sample data\n",
    "    obs, actions, rewards, continues = buffer.sample_sequences(num_samples, seq_len=10)\n",
    "    \n",
    "    obs = torch.tensor(obs, dtype=torch.float32, device=DEVICE)\n",
    "    actions = torch.tensor(actions, dtype=torch.float32, device=DEVICE)\n",
    "    \n",
    "    # Get latent representations\n",
    "    latents = []\n",
    "    timesteps = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        batch_size = obs.shape[0]\n",
    "        state = model.initial_state(batch_size)\n",
    "        \n",
    "        for t in range(obs.shape[1]):\n",
    "            state, _, _ = model.observe(obs[:, t], actions[:, t], state)\n",
    "            latents.append(state.combined.cpu().numpy())\n",
    "            timesteps.extend([t] * batch_size)\n",
    "    \n",
    "    latents = np.concatenate(latents, axis=0)\n",
    "    timesteps = np.array(timesteps)\n",
    "    \n",
    "    # Apply t-SNE\n",
    "    print(\"Running t-SNE...\")\n",
    "    tsne = TSNE(n_components=2, perplexity=30, random_state=SEED)\n",
    "    latents_2d = tsne.fit_transform(latents)\n",
    "    \n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    \n",
    "    scatter = ax.scatter(\n",
    "        latents_2d[:, 0], latents_2d[:, 1],\n",
    "        c=timesteps, cmap='viridis',\n",
    "        alpha=0.6, s=10\n",
    "    )\n",
    "    \n",
    "    plt.colorbar(scatter, label='Timestep')\n",
    "    ax.set_xlabel('t-SNE dim 1')\n",
    "    ax.set_ylabel('t-SNE dim 2')\n",
    "    ax.set_title('Latent Space Visualization (colored by timestep)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "\n",
    "fig = visualize_latent_space(cartpole_model, replay_buffer)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Save & Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Cell: Save Model\n",
    "Purpose: Save trained model and training history\n",
    "\"\"\"\n",
    "\n",
    "# Create results directory\n",
    "results_dir = PROJECT_ROOT / \"experiments\" / \"results\" / \"baseline\"\n",
    "results_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save model checkpoint\n",
    "checkpoint = {\n",
    "    'model_state_dict': cartpole_model.state_dict(),\n",
    "    'optimizer_state_dict': trainer.optimizer.state_dict(),\n",
    "    'config': {\n",
    "        'obs_dim': 4,\n",
    "        'action_dim': 2,\n",
    "        'stoch_dim': 32,\n",
    "        'deter_dim': 128,\n",
    "        'hidden_dim': 128,\n",
    "    },\n",
    "    'training_steps': len(history),\n",
    "    'final_loss': history['total'].iloc[-100:].mean(),\n",
    "}\n",
    "\n",
    "checkpoint_path = results_dir / \"cartpole_baseline.pt\"\n",
    "torch.save(checkpoint, checkpoint_path)\n",
    "print(f\"Model saved to: {checkpoint_path}\")\n",
    "\n",
    "# Save training history\n",
    "history_path = results_dir / \"cartpole_training_history.csv\"\n",
    "history.to_csv(history_path, index=False)\n",
    "print(f\"Training history saved to: {history_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Cell: Summary\n",
    "Purpose: Display training summary and next steps\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"PHASE 2 COMPLETE: CLASSICAL BASELINE WORLD MODEL\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\n[1] Components Implemented\")\n",
    "print(\"    - MLPEncoder: Observation to latent\")\n",
    "print(\"    - MLPDecoder: Latent to observation\")\n",
    "print(\"    - RSSM: Recurrent State-Space Model\")\n",
    "print(\"    - RewardPredictor: State to reward\")\n",
    "print(\"    - ContinuePredictor: State to continue probability\")\n",
    "print(\"    - WorldModel: Complete model combining all components\")\n",
    "\n",
    "print(\"\\n[2] Training\")\n",
    "print(f\"    - Environment: CartPole-v1\")\n",
    "print(f\"    - Training steps: {len(history)}\")\n",
    "print(f\"    - Final total loss: {history['total'].iloc[-100:].mean():.4f}\")\n",
    "print(f\"    - Final recon loss: {history['recon'].iloc[-100:].mean():.6f}\")\n",
    "print(f\"    - Model parameters: {sum(p.numel() for p in cartpole_model.parameters()):,}\")\n",
    "\n",
    "print(\"\\n[3] Files Saved\")\n",
    "print(f\"    - Model: {checkpoint_path}\")\n",
    "print(f\"    - History: {history_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"NEXT STEPS: Phase 3 - QAOA-Enhanced Training\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\"\"\n",
    "In Phase 3, we will:\n",
    "  1. Implement QAOA-inspired optimizer\n",
    "  2. Add cost and mixing operators\n",
    "  3. Create alternating optimization schedule\n",
    "  4. Compare with classical baseline\n",
    "  5. Visualize optimization trajectory\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
