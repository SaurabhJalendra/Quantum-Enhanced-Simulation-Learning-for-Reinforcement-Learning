{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 5: Gate-Enhanced Neural Layers\n",
    "\n",
    "**Quantum-Enhanced Simulation Learning for Reinforcement Learning**\n",
    "\n",
    "Author: Saurabh Jalendra  \n",
    "Institution: BITS Pilani (WILP Division)  \n",
    "Date: November 2025\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook implements **quantum gate-inspired neural network layers** that transform\n",
    "classical neural operations using principles from quantum computing gates:\n",
    "\n",
    "### Quantum Gates Implemented\n",
    "\n",
    "1. **Hadamard Gate (H)**: Creates superposition-like feature mixing\n",
    "2. **Rotation Gates (Rx, Ry, Rz)**: Parameterized rotations in feature space\n",
    "3. **CNOT Gate**: Controlled operations creating entanglement-like correlations\n",
    "4. **Phase Gate (S, T)**: Phase shifts for feature modulation\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "- **Unitary-inspired transformations**: Preserve information (approximately)\n",
    "- **Parameterized rotations**: Learnable angles for flexible transformations\n",
    "- **Entanglement-like correlations**: Feature dependencies through controlled ops\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path.cwd().parent\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "import math\n",
    "from typing import Dict, List, Tuple, Optional, Union, Callable\n",
    "from dataclasses import dataclass, field\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from src.utils import set_seed, get_device, MetricLogger, Timer, COLORS\n",
    "\n",
    "# Set seed for reproducibility\n",
    "set_seed(42)\n",
    "device = get_device()\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Quantum Gate Mathematical Foundations\n",
    "\n",
    "### Classical Quantum Gates\n",
    "\n",
    "**Hadamard Gate:**\n",
    "$$H = \\frac{1}{\\sqrt{2}} \\begin{pmatrix} 1 & 1 \\\\ 1 & -1 \\end{pmatrix}$$\n",
    "\n",
    "**Rotation Gates:**\n",
    "$$R_x(\\theta) = \\begin{pmatrix} \\cos(\\theta/2) & -i\\sin(\\theta/2) \\\\ -i\\sin(\\theta/2) & \\cos(\\theta/2) \\end{pmatrix}$$\n",
    "\n",
    "$$R_y(\\theta) = \\begin{pmatrix} \\cos(\\theta/2) & -\\sin(\\theta/2) \\\\ \\sin(\\theta/2) & \\cos(\\theta/2) \\end{pmatrix}$$\n",
    "\n",
    "$$R_z(\\theta) = \\begin{pmatrix} e^{-i\\theta/2} & 0 \\\\ 0 & e^{i\\theta/2} \\end{pmatrix}$$\n",
    "\n",
    "### Classical Adaptations\n",
    "\n",
    "We adapt these to real-valued neural network operations:\n",
    "- Complex exponentials become sinusoidal transformations\n",
    "- 2x2 matrices generalize to arbitrary dimensions\n",
    "- Learnable parameters replace fixed angles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Hadamard-Inspired Layer\n",
    "\n",
    "The Hadamard gate creates equal superposition. We adapt this to neural networks\n",
    "by mixing features through orthogonal-like transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HadamardLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Hadamard-inspired neural network layer.\n",
    "    \n",
    "    Creates superposition-like mixing of features using Hadamard-like\n",
    "    transformations extended to arbitrary dimensions.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dim : int\n",
    "        Input/output dimension (must be power of 2 for true Hadamard)\n",
    "    learnable_scale : bool\n",
    "        Whether to learn scaling factors\n",
    "    normalize : bool\n",
    "        Whether to normalize output\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        learnable_scale: bool = True,\n",
    "        normalize: bool = True\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.normalize = normalize\n",
    "        \n",
    "        # Create Hadamard-like matrix\n",
    "        H = self._create_hadamard_matrix(dim)\n",
    "        self.register_buffer('hadamard', H)\n",
    "        \n",
    "        # Learnable scaling\n",
    "        if learnable_scale:\n",
    "            self.scale = nn.Parameter(torch.ones(dim))\n",
    "        else:\n",
    "            self.register_buffer('scale', torch.ones(dim))\n",
    "        \n",
    "        # Learnable bias\n",
    "        self.bias = nn.Parameter(torch.zeros(dim))\n",
    "    \n",
    "    def _create_hadamard_matrix(self, n: int) -> Tensor:\n",
    "        \"\"\"\n",
    "        Create a Hadamard-like orthogonal matrix.\n",
    "        \n",
    "        For dimensions that are powers of 2, uses true Hadamard construction.\n",
    "        For other dimensions, uses an approximation via QR decomposition.\n",
    "        \"\"\"\n",
    "        # Check if n is power of 2\n",
    "        if n > 0 and (n & (n - 1)) == 0:\n",
    "            # True Hadamard construction via Sylvester's method\n",
    "            H = torch.tensor([[1.0]])\n",
    "            while H.shape[0] < n:\n",
    "                H = torch.cat([\n",
    "                    torch.cat([H, H], dim=1),\n",
    "                    torch.cat([H, -H], dim=1)\n",
    "                ], dim=0)\n",
    "            H = H / math.sqrt(n)\n",
    "        else:\n",
    "            # Approximate with random orthogonal matrix\n",
    "            random_matrix = torch.randn(n, n)\n",
    "            Q, _ = torch.linalg.qr(random_matrix)\n",
    "            H = Q\n",
    "        \n",
    "        return H\n",
    "    \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Apply Hadamard-like transformation.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x : Tensor\n",
    "            Input tensor of shape (batch, ..., dim)\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Tensor\n",
    "            Transformed tensor of same shape\n",
    "        \"\"\"\n",
    "        # Apply Hadamard transformation\n",
    "        y = F.linear(x, self.hadamard)\n",
    "        \n",
    "        # Scale and bias\n",
    "        y = y * self.scale + self.bias\n",
    "        \n",
    "        # Optional normalization\n",
    "        if self.normalize:\n",
    "            y = F.layer_norm(y, (self.dim,))\n",
    "        \n",
    "        return y\n",
    "    \n",
    "    def extra_repr(self) -> str:\n",
    "        return f\"dim={self.dim}, normalize={self.normalize}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Hadamard layer\n",
    "print(\"Testing HadamardLayer...\")\n",
    "\n",
    "# Power of 2 dimension\n",
    "hadamard_64 = HadamardLayer(64).to(device)\n",
    "x = torch.randn(32, 64, device=device)\n",
    "y = hadamard_64(x)\n",
    "print(f\"Input shape: {x.shape}, Output shape: {y.shape}\")\n",
    "\n",
    "# Verify Hadamard matrix properties\n",
    "H = hadamard_64.hadamard\n",
    "HHT = H @ H.T\n",
    "identity_error = torch.norm(HHT - torch.eye(64, device=device)).item()\n",
    "print(f\"Orthogonality error (should be ~0): {identity_error:.6f}\")\n",
    "\n",
    "# Non-power of 2 dimension\n",
    "hadamard_100 = HadamardLayer(100).to(device)\n",
    "x2 = torch.randn(32, 100, device=device)\n",
    "y2 = hadamard_100(x2)\n",
    "print(f\"Non-power-of-2: Input shape: {x2.shape}, Output shape: {y2.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 Rotation Gate Layers\n",
    "\n",
    "Rotation gates perform parameterized rotations in feature space.\n",
    "We implement Rx, Ry, Rz-inspired layers with learnable angles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RotationLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Rotation gate-inspired neural network layer.\n",
    "    \n",
    "    Implements learnable rotations in feature space inspired by\n",
    "    quantum rotation gates (Rx, Ry, Rz).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dim : int\n",
    "        Feature dimension\n",
    "    num_rotations : int\n",
    "        Number of rotation pairs (rotations applied to pairs of features)\n",
    "    rotation_type : str\n",
    "        Type of rotation: 'xy', 'xz', 'yz', or 'all'\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        num_rotations: Optional[int] = None,\n",
    "        rotation_type: str = 'all'\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.num_rotations = num_rotations or (dim // 2)\n",
    "        self.rotation_type = rotation_type\n",
    "        \n",
    "        # Learnable rotation angles\n",
    "        if rotation_type == 'all':\n",
    "            # Three angles per rotation (Rx, Ry, Rz)\n",
    "            self.angles = nn.Parameter(\n",
    "                torch.randn(self.num_rotations, 3) * 0.1\n",
    "            )\n",
    "        else:\n",
    "            # Single angle per rotation\n",
    "            self.angles = nn.Parameter(\n",
    "                torch.randn(self.num_rotations) * 0.1\n",
    "            )\n",
    "        \n",
    "        # Indices for rotation pairs\n",
    "        indices = torch.randperm(dim)[:self.num_rotations * 2]\n",
    "        self.register_buffer('idx1', indices[:self.num_rotations])\n",
    "        self.register_buffer('idx2', indices[self.num_rotations:])\n",
    "    \n",
    "    def _apply_rotation_2d(\n",
    "        self,\n",
    "        x1: Tensor,\n",
    "        x2: Tensor,\n",
    "        theta: Tensor\n",
    "    ) -> Tuple[Tensor, Tensor]:\n",
    "        \"\"\"\n",
    "        Apply 2D rotation to feature pairs.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x1, x2 : Tensor\n",
    "            Feature pairs to rotate (batch, num_rotations)\n",
    "        theta : Tensor\n",
    "            Rotation angles (num_rotations,)\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Tuple[Tensor, Tensor]\n",
    "            Rotated feature pairs\n",
    "        \"\"\"\n",
    "        cos_theta = torch.cos(theta)\n",
    "        sin_theta = torch.sin(theta)\n",
    "        \n",
    "        y1 = cos_theta * x1 - sin_theta * x2\n",
    "        y2 = sin_theta * x1 + cos_theta * x2\n",
    "        \n",
    "        return y1, y2\n",
    "    \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Apply rotation transformations.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x : Tensor\n",
    "            Input tensor of shape (batch, dim)\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Tensor\n",
    "            Rotated tensor of same shape\n",
    "        \"\"\"\n",
    "        # Clone to avoid in-place modification\n",
    "        y = x.clone()\n",
    "        \n",
    "        # Get feature pairs\n",
    "        x1 = x[:, self.idx1]  # (batch, num_rotations)\n",
    "        x2 = x[:, self.idx2]  # (batch, num_rotations)\n",
    "        \n",
    "        if self.rotation_type == 'all':\n",
    "            # Apply Rz, Ry, Rx in sequence\n",
    "            for i in range(3):\n",
    "                x1, x2 = self._apply_rotation_2d(x1, x2, self.angles[:, i])\n",
    "        else:\n",
    "            # Single rotation\n",
    "            x1, x2 = self._apply_rotation_2d(x1, x2, self.angles)\n",
    "        \n",
    "        # Update features\n",
    "        y = y.scatter(1, self.idx1.unsqueeze(0).expand(x.shape[0], -1), x1)\n",
    "        y = y.scatter(1, self.idx2.unsqueeze(0).expand(x.shape[0], -1), x2)\n",
    "        \n",
    "        return y\n",
    "    \n",
    "    def extra_repr(self) -> str:\n",
    "        return f\"dim={self.dim}, num_rotations={self.num_rotations}, type={self.rotation_type}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Rotation layer\n",
    "print(\"Testing RotationLayer...\")\n",
    "\n",
    "rotation_layer = RotationLayer(64, rotation_type='all').to(device)\n",
    "x = torch.randn(32, 64, device=device)\n",
    "y = rotation_layer(x)\n",
    "\n",
    "print(f\"Input shape: {x.shape}, Output shape: {y.shape}\")\n",
    "print(f\"Number of rotation angles: {rotation_layer.angles.shape}\")\n",
    "\n",
    "# Verify approximate norm preservation (rotations should preserve norms)\n",
    "x_norms = torch.norm(x, dim=1)\n",
    "y_norms = torch.norm(y, dim=1)\n",
    "norm_diff = torch.abs(x_norms - y_norms).mean().item()\n",
    "print(f\"Average norm difference: {norm_diff:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5 CNOT-Inspired Layer\n",
    "\n",
    "The CNOT (Controlled-NOT) gate creates entanglement between qubits.\n",
    "We adapt this to create controlled dependencies between features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNOTLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    CNOT-inspired neural network layer.\n",
    "    \n",
    "    Creates entanglement-like correlations between features through\n",
    "    controlled operations where one feature controls the transformation\n",
    "    of another.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dim : int\n",
    "        Feature dimension\n",
    "    num_controls : int\n",
    "        Number of control-target pairs\n",
    "    threshold : float\n",
    "        Activation threshold for control feature\n",
    "    learnable_threshold : bool\n",
    "        Whether threshold is learnable\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        num_controls: Optional[int] = None,\n",
    "        threshold: float = 0.0,\n",
    "        learnable_threshold: bool = True\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.num_controls = num_controls or (dim // 2)\n",
    "        \n",
    "        # Learnable thresholds\n",
    "        if learnable_threshold:\n",
    "            self.threshold = nn.Parameter(\n",
    "                torch.full((self.num_controls,), threshold)\n",
    "            )\n",
    "        else:\n",
    "            self.register_buffer(\n",
    "                'threshold',\n",
    "                torch.full((self.num_controls,), threshold)\n",
    "            )\n",
    "        \n",
    "        # Learnable transformation weights for target\n",
    "        self.transform_weight = nn.Parameter(\n",
    "            torch.randn(self.num_controls) * 0.1\n",
    "        )\n",
    "        \n",
    "        # Temperature for soft thresholding\n",
    "        self.temperature = nn.Parameter(torch.tensor(1.0))\n",
    "        \n",
    "        # Control and target indices\n",
    "        indices = torch.randperm(dim)[:self.num_controls * 2]\n",
    "        self.register_buffer('control_idx', indices[:self.num_controls])\n",
    "        self.register_buffer('target_idx', indices[self.num_controls:])\n",
    "    \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Apply CNOT-like controlled transformations.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x : Tensor\n",
    "            Input tensor of shape (batch, dim)\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Tensor\n",
    "            Transformed tensor\n",
    "        \"\"\"\n",
    "        y = x.clone()\n",
    "        \n",
    "        # Get control and target features\n",
    "        control = x[:, self.control_idx]  # (batch, num_controls)\n",
    "        target = x[:, self.target_idx]    # (batch, num_controls)\n",
    "        \n",
    "        # Soft control activation using sigmoid\n",
    "        control_activation = torch.sigmoid(\n",
    "            (control - self.threshold) * self.temperature\n",
    "        )\n",
    "        \n",
    "        # Apply controlled transformation (like XOR in quantum, we use negation-like transform)\n",
    "        # When control is active, transform target\n",
    "        transformed_target = target + control_activation * self.transform_weight * target\n",
    "        \n",
    "        # Update targets\n",
    "        y = y.scatter(\n",
    "            1,\n",
    "            self.target_idx.unsqueeze(0).expand(x.shape[0], -1),\n",
    "            transformed_target\n",
    "        )\n",
    "        \n",
    "        return y\n",
    "    \n",
    "    def get_entanglement_strength(self) -> Tensor:\n",
    "        \"\"\"\n",
    "        Compute a measure of entanglement strength.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Tensor\n",
    "            Average absolute transformation weight\n",
    "        \"\"\"\n",
    "        return torch.abs(self.transform_weight).mean()\n",
    "    \n",
    "    def extra_repr(self) -> str:\n",
    "        return f\"dim={self.dim}, num_controls={self.num_controls}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test CNOT layer\n",
    "print(\"Testing CNOTLayer...\")\n",
    "\n",
    "cnot_layer = CNOTLayer(64).to(device)\n",
    "x = torch.randn(32, 64, device=device)\n",
    "y = cnot_layer(x)\n",
    "\n",
    "print(f\"Input shape: {x.shape}, Output shape: {y.shape}\")\n",
    "print(f\"Entanglement strength: {cnot_layer.get_entanglement_strength().item():.6f}\")\n",
    "\n",
    "# Show that targets change based on controls\n",
    "# When control is high, target should change more\n",
    "x_high_control = torch.randn(32, 64, device=device)\n",
    "x_high_control[:, cnot_layer.control_idx] = 5.0  # High control values\n",
    "y_high = cnot_layer(x_high_control)\n",
    "\n",
    "x_low_control = x_high_control.clone()\n",
    "x_low_control[:, cnot_layer.control_idx] = -5.0  # Low control values\n",
    "y_low = cnot_layer(x_low_control)\n",
    "\n",
    "# Target difference should be larger than control difference\n",
    "target_diff = (y_high[:, cnot_layer.target_idx] - y_low[:, cnot_layer.target_idx]).abs().mean()\n",
    "print(f\"Target difference with high vs low control: {target_diff.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.6 Phase Gate Layer\n",
    "\n",
    "Phase gates apply phase shifts to quantum states. We adapt this\n",
    "to apply learnable modulations to features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhaseLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Phase gate-inspired neural network layer.\n",
    "    \n",
    "    Applies learnable phase-like modulations to features using\n",
    "    sinusoidal transformations.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dim : int\n",
    "        Feature dimension\n",
    "    phase_type : str\n",
    "        Type of phase gate: 'S' (pi/2), 'T' (pi/4), or 'learnable'\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        phase_type: str = 'learnable'\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.phase_type = phase_type\n",
    "        \n",
    "        if phase_type == 'S':\n",
    "            # S gate: pi/2 phase\n",
    "            self.register_buffer('phases', torch.full((dim,), math.pi / 2))\n",
    "        elif phase_type == 'T':\n",
    "            # T gate: pi/4 phase\n",
    "            self.register_buffer('phases', torch.full((dim,), math.pi / 4))\n",
    "        else:\n",
    "            # Learnable phases\n",
    "            self.phases = nn.Parameter(torch.randn(dim) * 0.1)\n",
    "        \n",
    "        # Learnable amplitude modulation\n",
    "        self.amplitude = nn.Parameter(torch.ones(dim))\n",
    "    \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Apply phase modulation.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x : Tensor\n",
    "            Input tensor of shape (batch, dim)\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Tensor\n",
    "            Phase-modulated tensor\n",
    "        \"\"\"\n",
    "        # Apply phase shift through sinusoidal modulation\n",
    "        # y = amplitude * (x * cos(phase) + |x| * sin(phase))\n",
    "        cos_phase = torch.cos(self.phases)\n",
    "        sin_phase = torch.sin(self.phases)\n",
    "        \n",
    "        y = self.amplitude * (x * cos_phase + torch.abs(x) * sin_phase)\n",
    "        \n",
    "        return y\n",
    "    \n",
    "    def extra_repr(self) -> str:\n",
    "        return f\"dim={self.dim}, type={self.phase_type}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Phase layer\n",
    "print(\"Testing PhaseLayer...\")\n",
    "\n",
    "for phase_type in ['S', 'T', 'learnable']:\n",
    "    phase_layer = PhaseLayer(64, phase_type=phase_type).to(device)\n",
    "    x = torch.randn(32, 64, device=device)\n",
    "    y = phase_layer(x)\n",
    "    print(f\"Phase type '{phase_type}': Input shape {x.shape}, Output shape {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.7 Composite Quantum Gate Block\n",
    "\n",
    "Combine multiple gate layers into a single quantum-inspired block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantumGateBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Composite quantum gate-inspired neural network block.\n",
    "    \n",
    "    Combines Hadamard, Rotation, CNOT, and Phase layers in a\n",
    "    configurable sequence similar to quantum circuits.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dim : int\n",
    "        Feature dimension\n",
    "    num_layers : int\n",
    "        Number of gate layers in sequence\n",
    "    use_hadamard : bool\n",
    "        Include Hadamard layers\n",
    "    use_rotation : bool\n",
    "        Include Rotation layers\n",
    "    use_cnot : bool\n",
    "        Include CNOT layers\n",
    "    use_phase : bool\n",
    "        Include Phase layers\n",
    "    residual : bool\n",
    "        Use residual connections\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        num_layers: int = 2,\n",
    "        use_hadamard: bool = True,\n",
    "        use_rotation: bool = True,\n",
    "        use_cnot: bool = True,\n",
    "        use_phase: bool = True,\n",
    "        residual: bool = True\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.residual = residual\n",
    "        \n",
    "        layers = []\n",
    "        for i in range(num_layers):\n",
    "            layer_gates = []\n",
    "            \n",
    "            if use_hadamard:\n",
    "                layer_gates.append(HadamardLayer(dim, normalize=True))\n",
    "            \n",
    "            if use_rotation:\n",
    "                layer_gates.append(RotationLayer(dim, rotation_type='all'))\n",
    "            \n",
    "            if use_cnot:\n",
    "                layer_gates.append(CNOTLayer(dim))\n",
    "            \n",
    "            if use_phase:\n",
    "                layer_gates.append(PhaseLayer(dim, phase_type='learnable'))\n",
    "            \n",
    "            layers.append(nn.Sequential(*layer_gates))\n",
    "        \n",
    "        self.layers = nn.ModuleList(layers)\n",
    "        \n",
    "        # Layer norm for residual\n",
    "        if residual:\n",
    "            self.layer_norms = nn.ModuleList([\n",
    "                nn.LayerNorm(dim) for _ in range(num_layers)\n",
    "            ])\n",
    "    \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Apply quantum gate block.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x : Tensor\n",
    "            Input tensor of shape (batch, dim)\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Tensor\n",
    "            Transformed tensor\n",
    "        \"\"\"\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            if self.residual:\n",
    "                x = x + self.layer_norms[i](layer(x))\n",
    "            else:\n",
    "                x = layer(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def extra_repr(self) -> str:\n",
    "        return f\"dim={self.dim}, num_layers={len(self.layers)}, residual={self.residual}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Quantum Gate Block\n",
    "print(\"Testing QuantumGateBlock...\")\n",
    "\n",
    "gate_block = QuantumGateBlock(\n",
    "    dim=64,\n",
    "    num_layers=3,\n",
    "    use_hadamard=True,\n",
    "    use_rotation=True,\n",
    "    use_cnot=True,\n",
    "    use_phase=True,\n",
    "    residual=True\n",
    ").to(device)\n",
    "\n",
    "x = torch.randn(32, 64, device=device)\n",
    "y = gate_block(x)\n",
    "\n",
    "print(f\"Input shape: {x.shape}, Output shape: {y.shape}\")\n",
    "print(f\"Number of parameters: {sum(p.numel() for p in gate_block.parameters())}\")\n",
    "print(f\"\\nBlock structure:\")\n",
    "print(gate_block)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.8 Gate-Enhanced World Model Components\n",
    "\n",
    "Now we integrate quantum gate layers into the world model architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GateEnhancedEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder with quantum gate-enhanced layers.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    obs_dim : int\n",
    "        Observation dimension\n",
    "    hidden_dim : int\n",
    "        Hidden layer dimension\n",
    "    embed_dim : int\n",
    "        Output embedding dimension\n",
    "    num_gate_layers : int\n",
    "        Number of quantum gate blocks\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        obs_dim: int,\n",
    "        hidden_dim: int = 256,\n",
    "        embed_dim: int = 64,\n",
    "        num_gate_layers: int = 2\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Input projection\n",
    "        self.input_proj = nn.Linear(obs_dim, hidden_dim)\n",
    "        \n",
    "        # Quantum gate block\n",
    "        self.gate_block = QuantumGateBlock(\n",
    "            dim=hidden_dim,\n",
    "            num_layers=num_gate_layers,\n",
    "            residual=True\n",
    "        )\n",
    "        \n",
    "        # Output projection\n",
    "        self.output_proj = nn.Sequential(\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.Linear(hidden_dim, embed_dim),\n",
    "            nn.ELU()\n",
    "        )\n",
    "    \n",
    "    def forward(self, obs: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Encode observations.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        obs : Tensor\n",
    "            Observations of shape (batch, obs_dim)\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Tensor\n",
    "            Embeddings of shape (batch, embed_dim)\n",
    "        \"\"\"\n",
    "        x = F.elu(self.input_proj(obs))\n",
    "        x = self.gate_block(x)\n",
    "        return self.output_proj(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GateEnhancedDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Decoder with quantum gate-enhanced layers.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    state_dim : int\n",
    "        State dimension (deterministic + stochastic)\n",
    "    hidden_dim : int\n",
    "        Hidden layer dimension\n",
    "    obs_dim : int\n",
    "        Output observation dimension\n",
    "    num_gate_layers : int\n",
    "        Number of quantum gate blocks\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        state_dim: int,\n",
    "        hidden_dim: int = 256,\n",
    "        obs_dim: int = 4,\n",
    "        num_gate_layers: int = 2\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Input projection\n",
    "        self.input_proj = nn.Linear(state_dim, hidden_dim)\n",
    "        \n",
    "        # Quantum gate block\n",
    "        self.gate_block = QuantumGateBlock(\n",
    "            dim=hidden_dim,\n",
    "            num_layers=num_gate_layers,\n",
    "            residual=True\n",
    "        )\n",
    "        \n",
    "        # Output layers (mean and log_std)\n",
    "        self.output_layer = nn.Sequential(\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ELU()\n",
    "        )\n",
    "        self.mean = nn.Linear(hidden_dim // 2, obs_dim)\n",
    "        self.log_std = nn.Linear(hidden_dim // 2, obs_dim)\n",
    "    \n",
    "    def forward(self, state: Tensor) -> Tuple[Tensor, Tensor]:\n",
    "        \"\"\"\n",
    "        Decode state to observation distribution.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        state : Tensor\n",
    "            State of shape (batch, state_dim)\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Tuple[Tensor, Tensor]\n",
    "            Mean and log_std of predicted observation distribution\n",
    "        \"\"\"\n",
    "        x = F.elu(self.input_proj(state))\n",
    "        x = self.gate_block(x)\n",
    "        x = self.output_layer(x)\n",
    "        \n",
    "        mean = self.mean(x)\n",
    "        log_std = self.log_std(x).clamp(-10, 2)\n",
    "        \n",
    "        return mean, log_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GateEnhancedRSSM(nn.Module):\n",
    "    \"\"\"\n",
    "    RSSM with quantum gate-enhanced transition model.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    embed_dim : int\n",
    "        Embedding dimension from encoder\n",
    "    action_dim : int\n",
    "        Action dimension\n",
    "    deter_dim : int\n",
    "        Deterministic state dimension\n",
    "    stoch_dim : int\n",
    "        Stochastic state dimension\n",
    "    hidden_dim : int\n",
    "        Hidden layer dimension\n",
    "    num_gate_layers : int\n",
    "        Number of quantum gate layers in transition\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_dim: int = 64,\n",
    "        action_dim: int = 1,\n",
    "        deter_dim: int = 128,\n",
    "        stoch_dim: int = 32,\n",
    "        hidden_dim: int = 256,\n",
    "        num_gate_layers: int = 2\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.deter_dim = deter_dim\n",
    "        self.stoch_dim = stoch_dim\n",
    "        \n",
    "        # GRU cell for deterministic state\n",
    "        self.gru = nn.GRUCell(stoch_dim + action_dim, deter_dim)\n",
    "        \n",
    "        # Gate-enhanced prior (imagination)\n",
    "        self.prior_gate = QuantumGateBlock(\n",
    "            dim=hidden_dim,\n",
    "            num_layers=num_gate_layers,\n",
    "            residual=True\n",
    "        )\n",
    "        self.prior_input = nn.Linear(deter_dim, hidden_dim)\n",
    "        self.prior_output = nn.Linear(hidden_dim, stoch_dim * 2)\n",
    "        \n",
    "        # Gate-enhanced posterior (with observation)\n",
    "        self.posterior_gate = QuantumGateBlock(\n",
    "            dim=hidden_dim,\n",
    "            num_layers=num_gate_layers,\n",
    "            residual=True\n",
    "        )\n",
    "        self.posterior_input = nn.Linear(deter_dim + embed_dim, hidden_dim)\n",
    "        self.posterior_output = nn.Linear(hidden_dim, stoch_dim * 2)\n",
    "    \n",
    "    def initial_state(self, batch_size: int, device: torch.device) -> Dict[str, Tensor]:\n",
    "        \"\"\"Get initial state.\"\"\"\n",
    "        return {\n",
    "            'deter': torch.zeros(batch_size, self.deter_dim, device=device),\n",
    "            'stoch': torch.zeros(batch_size, self.stoch_dim, device=device)\n",
    "        }\n",
    "    \n",
    "    def get_full_state(self, state: Dict[str, Tensor]) -> Tensor:\n",
    "        \"\"\"Concatenate deterministic and stochastic states.\"\"\"\n",
    "        return torch.cat([state['deter'], state['stoch']], dim=-1)\n",
    "    \n",
    "    def prior(self, deter: Tensor) -> Tuple[Tensor, Tensor]:\n",
    "        \"\"\"\n",
    "        Compute prior distribution (for imagination).\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        deter : Tensor\n",
    "            Deterministic state\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Tuple[Tensor, Tensor]\n",
    "            Mean and std of prior distribution\n",
    "        \"\"\"\n",
    "        x = F.elu(self.prior_input(deter))\n",
    "        x = self.prior_gate(x)\n",
    "        stats = self.prior_output(x)\n",
    "        mean, log_std = torch.chunk(stats, 2, dim=-1)\n",
    "        std = F.softplus(log_std) + 0.1\n",
    "        return mean, std\n",
    "    \n",
    "    def posterior(self, deter: Tensor, embed: Tensor) -> Tuple[Tensor, Tensor]:\n",
    "        \"\"\"\n",
    "        Compute posterior distribution (with observation).\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        deter : Tensor\n",
    "            Deterministic state\n",
    "        embed : Tensor\n",
    "            Observation embedding\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Tuple[Tensor, Tensor]\n",
    "            Mean and std of posterior distribution\n",
    "        \"\"\"\n",
    "        x = torch.cat([deter, embed], dim=-1)\n",
    "        x = F.elu(self.posterior_input(x))\n",
    "        x = self.posterior_gate(x)\n",
    "        stats = self.posterior_output(x)\n",
    "        mean, log_std = torch.chunk(stats, 2, dim=-1)\n",
    "        std = F.softplus(log_std) + 0.1\n",
    "        return mean, std\n",
    "    \n",
    "    def step(\n",
    "        self,\n",
    "        prev_state: Dict[str, Tensor],\n",
    "        action: Tensor,\n",
    "        embed: Optional[Tensor] = None\n",
    "    ) -> Tuple[Dict[str, Tensor], Dict[str, Tensor]]:\n",
    "        \"\"\"\n",
    "        Single step of RSSM.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        prev_state : Dict[str, Tensor]\n",
    "            Previous state with 'deter' and 'stoch'\n",
    "        action : Tensor\n",
    "            Action taken\n",
    "        embed : Optional[Tensor]\n",
    "            Observation embedding (None for imagination)\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Tuple[Dict[str, Tensor], Dict[str, Tensor]]\n",
    "            New state and distribution stats\n",
    "        \"\"\"\n",
    "        # Update deterministic state\n",
    "        gru_input = torch.cat([prev_state['stoch'], action], dim=-1)\n",
    "        deter = self.gru(gru_input, prev_state['deter'])\n",
    "        \n",
    "        # Get prior\n",
    "        prior_mean, prior_std = self.prior(deter)\n",
    "        \n",
    "        # Get posterior if embed available, otherwise use prior\n",
    "        if embed is not None:\n",
    "            post_mean, post_std = self.posterior(deter, embed)\n",
    "            # Sample from posterior\n",
    "            stoch = post_mean + post_std * torch.randn_like(post_std)\n",
    "        else:\n",
    "            post_mean, post_std = prior_mean, prior_std\n",
    "            # Sample from prior\n",
    "            stoch = prior_mean + prior_std * torch.randn_like(prior_std)\n",
    "        \n",
    "        new_state = {'deter': deter, 'stoch': stoch}\n",
    "        stats = {\n",
    "            'prior_mean': prior_mean,\n",
    "            'prior_std': prior_std,\n",
    "            'post_mean': post_mean,\n",
    "            'post_std': post_std\n",
    "        }\n",
    "        \n",
    "        return new_state, stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GateEnhancedWorldModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Complete world model with quantum gate-enhanced components.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    obs_dim : int\n",
    "        Observation dimension\n",
    "    action_dim : int\n",
    "        Action dimension\n",
    "    config : Optional[Dict]\n",
    "        Configuration dictionary\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        obs_dim: int,\n",
    "        action_dim: int,\n",
    "        config: Optional[Dict] = None\n",
    "    ):\n",
    "        super().__init__()\n",
    "        config = config or {}\n",
    "        \n",
    "        # Dimensions\n",
    "        hidden_dim = config.get('hidden_dim', 256)\n",
    "        embed_dim = config.get('embed_dim', 64)\n",
    "        deter_dim = config.get('deter_dim', 128)\n",
    "        stoch_dim = config.get('stoch_dim', 32)\n",
    "        num_gate_layers = config.get('num_gate_layers', 2)\n",
    "        \n",
    "        self.obs_dim = obs_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.state_dim = deter_dim + stoch_dim\n",
    "        \n",
    "        # Components\n",
    "        self.encoder = GateEnhancedEncoder(\n",
    "            obs_dim, hidden_dim, embed_dim, num_gate_layers\n",
    "        )\n",
    "        self.decoder = GateEnhancedDecoder(\n",
    "            self.state_dim, hidden_dim, obs_dim, num_gate_layers\n",
    "        )\n",
    "        self.rssm = GateEnhancedRSSM(\n",
    "            embed_dim, action_dim, deter_dim, stoch_dim, hidden_dim, num_gate_layers\n",
    "        )\n",
    "        \n",
    "        # Reward and continue predictors\n",
    "        self.reward_pred = nn.Sequential(\n",
    "            nn.Linear(self.state_dim, hidden_dim // 2),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(hidden_dim // 2, 1)\n",
    "        )\n",
    "        self.continue_pred = nn.Sequential(\n",
    "            nn.Linear(self.state_dim, hidden_dim // 2),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(hidden_dim // 2, 1)\n",
    "        )\n",
    "    \n",
    "    def initial_state(self, batch_size: int, device: torch.device) -> Dict[str, Tensor]:\n",
    "        \"\"\"Get initial RSSM state.\"\"\"\n",
    "        return self.rssm.initial_state(batch_size, device)\n",
    "    \n",
    "    def encode(self, obs: Tensor) -> Tensor:\n",
    "        \"\"\"Encode observation.\"\"\"\n",
    "        return self.encoder(obs)\n",
    "    \n",
    "    def decode(self, state: Dict[str, Tensor]) -> Tuple[Tensor, Tensor]:\n",
    "        \"\"\"Decode state to observation distribution.\"\"\"\n",
    "        full_state = self.rssm.get_full_state(state)\n",
    "        return self.decoder(full_state)\n",
    "    \n",
    "    def predict_reward(self, state: Dict[str, Tensor]) -> Tensor:\n",
    "        \"\"\"Predict reward from state.\"\"\"\n",
    "        full_state = self.rssm.get_full_state(state)\n",
    "        return self.reward_pred(full_state).squeeze(-1)\n",
    "    \n",
    "    def predict_continue(self, state: Dict[str, Tensor]) -> Tensor:\n",
    "        \"\"\"Predict continue probability from state.\"\"\"\n",
    "        full_state = self.rssm.get_full_state(state)\n",
    "        return torch.sigmoid(self.continue_pred(full_state)).squeeze(-1)\n",
    "    \n",
    "    def step(\n",
    "        self,\n",
    "        prev_state: Dict[str, Tensor],\n",
    "        action: Tensor,\n",
    "        obs: Optional[Tensor] = None\n",
    "    ) -> Tuple[Dict[str, Tensor], Dict[str, Tensor]]:\n",
    "        \"\"\"\n",
    "        Single step of world model.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        prev_state : Dict[str, Tensor]\n",
    "            Previous RSSM state\n",
    "        action : Tensor\n",
    "            Action taken\n",
    "        obs : Optional[Tensor]\n",
    "            Observation (None for imagination)\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Tuple[Dict[str, Tensor], Dict[str, Tensor]]\n",
    "            New state and distribution stats\n",
    "        \"\"\"\n",
    "        embed = self.encode(obs) if obs is not None else None\n",
    "        return self.rssm.step(prev_state, action, embed)\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        obs_seq: Tensor,\n",
    "        action_seq: Tensor\n",
    "    ) -> Dict[str, Tensor]:\n",
    "        \"\"\"\n",
    "        Process a sequence through the world model.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        obs_seq : Tensor\n",
    "            Observations (batch, seq_len, obs_dim)\n",
    "        action_seq : Tensor\n",
    "            Actions (batch, seq_len, action_dim)\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Dict[str, Tensor]\n",
    "            Dictionary with states, predictions, and distribution stats\n",
    "        \"\"\"\n",
    "        batch_size, seq_len = obs_seq.shape[:2]\n",
    "        device = obs_seq.device\n",
    "        \n",
    "        # Initialize\n",
    "        state = self.initial_state(batch_size, device)\n",
    "        \n",
    "        # Collect outputs\n",
    "        states = []\n",
    "        prior_means, prior_stds = [], []\n",
    "        post_means, post_stds = [], []\n",
    "        \n",
    "        for t in range(seq_len):\n",
    "            state, stats = self.step(state, action_seq[:, t], obs_seq[:, t])\n",
    "            \n",
    "            states.append(self.rssm.get_full_state(state))\n",
    "            prior_means.append(stats['prior_mean'])\n",
    "            prior_stds.append(stats['prior_std'])\n",
    "            post_means.append(stats['post_mean'])\n",
    "            post_stds.append(stats['post_std'])\n",
    "        \n",
    "        # Stack\n",
    "        states = torch.stack(states, dim=1)\n",
    "        \n",
    "        # Decode all states\n",
    "        flat_states = states.reshape(-1, states.shape[-1])\n",
    "        obs_mean, obs_log_std = self.decoder(flat_states)\n",
    "        obs_mean = obs_mean.reshape(batch_size, seq_len, -1)\n",
    "        obs_log_std = obs_log_std.reshape(batch_size, seq_len, -1)\n",
    "        \n",
    "        # Predict rewards\n",
    "        reward_pred = self.reward_pred(flat_states).reshape(batch_size, seq_len)\n",
    "        \n",
    "        return {\n",
    "            'states': states,\n",
    "            'obs_mean': obs_mean,\n",
    "            'obs_log_std': obs_log_std,\n",
    "            'reward_pred': reward_pred,\n",
    "            'prior_mean': torch.stack(prior_means, dim=1),\n",
    "            'prior_std': torch.stack(prior_stds, dim=1),\n",
    "            'post_mean': torch.stack(post_means, dim=1),\n",
    "            'post_std': torch.stack(post_stds, dim=1)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Gate-Enhanced World Model\n",
    "print(\"Testing GateEnhancedWorldModel...\")\n",
    "\n",
    "model = GateEnhancedWorldModel(\n",
    "    obs_dim=4,\n",
    "    action_dim=1,\n",
    "    config={'num_gate_layers': 2}\n",
    ").to(device)\n",
    "\n",
    "# Test forward pass\n",
    "batch_size, seq_len = 16, 20\n",
    "obs_seq = torch.randn(batch_size, seq_len, 4, device=device)\n",
    "action_seq = torch.randn(batch_size, seq_len, 1, device=device)\n",
    "\n",
    "outputs = model(obs_seq, action_seq)\n",
    "\n",
    "print(f\"States shape: {outputs['states'].shape}\")\n",
    "print(f\"Obs mean shape: {outputs['obs_mean'].shape}\")\n",
    "print(f\"Reward pred shape: {outputs['reward_pred'].shape}\")\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.9 Gate-Enhanced Training\n",
    "\n",
    "Training loop for the gate-enhanced world model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GateEnhancedTrainer:\n",
    "    \"\"\"\n",
    "    Trainer for gate-enhanced world model.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model : GateEnhancedWorldModel\n",
    "        The world model to train\n",
    "    learning_rate : float\n",
    "        Learning rate\n",
    "    kl_weight : float\n",
    "        Weight for KL divergence loss\n",
    "    free_nats : float\n",
    "        Free nats for KL loss\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model: GateEnhancedWorldModel,\n",
    "        learning_rate: float = 1e-4,\n",
    "        kl_weight: float = 1.0,\n",
    "        free_nats: float = 3.0\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.kl_weight = kl_weight\n",
    "        self.free_nats = free_nats\n",
    "        \n",
    "        self.optimizer = torch.optim.AdamW(\n",
    "            model.parameters(),\n",
    "            lr=learning_rate,\n",
    "            weight_decay=1e-5\n",
    "        )\n",
    "        \n",
    "        self.logger = MetricLogger(name='gate_enhanced')\n",
    "    \n",
    "    def compute_loss(\n",
    "        self,\n",
    "        obs_seq: Tensor,\n",
    "        action_seq: Tensor,\n",
    "        reward_seq: Tensor\n",
    "    ) -> Tuple[Tensor, Dict[str, float]]:\n",
    "        \"\"\"\n",
    "        Compute training loss.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        obs_seq : Tensor\n",
    "            Observations (batch, seq_len, obs_dim)\n",
    "        action_seq : Tensor\n",
    "            Actions (batch, seq_len, action_dim)\n",
    "        reward_seq : Tensor\n",
    "            Rewards (batch, seq_len)\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Tuple[Tensor, Dict[str, float]]\n",
    "            Total loss and individual loss components\n",
    "        \"\"\"\n",
    "        outputs = self.model(obs_seq, action_seq)\n",
    "        \n",
    "        # Reconstruction loss\n",
    "        obs_dist = torch.distributions.Normal(\n",
    "            outputs['obs_mean'],\n",
    "            torch.exp(outputs['obs_log_std'])\n",
    "        )\n",
    "        recon_loss = -obs_dist.log_prob(obs_seq).mean()\n",
    "        \n",
    "        # KL divergence loss with free nats\n",
    "        prior_dist = torch.distributions.Normal(\n",
    "            outputs['prior_mean'],\n",
    "            outputs['prior_std']\n",
    "        )\n",
    "        post_dist = torch.distributions.Normal(\n",
    "            outputs['post_mean'],\n",
    "            outputs['post_std']\n",
    "        )\n",
    "        kl_div = torch.distributions.kl_divergence(post_dist, prior_dist)\n",
    "        kl_loss = torch.maximum(\n",
    "            kl_div.mean(),\n",
    "            torch.tensor(self.free_nats, device=kl_div.device)\n",
    "        )\n",
    "        \n",
    "        # Reward loss\n",
    "        reward_loss = F.mse_loss(outputs['reward_pred'], reward_seq)\n",
    "        \n",
    "        # Total loss\n",
    "        total_loss = recon_loss + self.kl_weight * kl_loss + reward_loss\n",
    "        \n",
    "        metrics = {\n",
    "            'recon_loss': recon_loss.item(),\n",
    "            'kl_loss': kl_loss.item(),\n",
    "            'reward_loss': reward_loss.item(),\n",
    "            'total_loss': total_loss.item()\n",
    "        }\n",
    "        \n",
    "        return total_loss, metrics\n",
    "    \n",
    "    def train_step(\n",
    "        self,\n",
    "        obs_seq: Tensor,\n",
    "        action_seq: Tensor,\n",
    "        reward_seq: Tensor\n",
    "    ) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Single training step.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Dict[str, float]\n",
    "            Loss metrics\n",
    "        \"\"\"\n",
    "        self.model.train()\n",
    "        self.optimizer.zero_grad()\n",
    "        \n",
    "        loss, metrics = self.compute_loss(obs_seq, action_seq, reward_seq)\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.model.parameters(), 100.0)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # Log metrics\n",
    "        for key, value in metrics.items():\n",
    "            self.logger.log(**{key: value})\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def evaluate(\n",
    "        self,\n",
    "        obs_seq: Tensor,\n",
    "        action_seq: Tensor,\n",
    "        reward_seq: Tensor\n",
    "    ) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Evaluate model.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Dict[str, float]\n",
    "            Loss metrics\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            _, metrics = self.compute_loss(obs_seq, action_seq, reward_seq)\n",
    "        return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Gate-Enhanced Trainer\n",
    "print(\"Testing GateEnhancedTrainer...\")\n",
    "\n",
    "model = GateEnhancedWorldModel(obs_dim=4, action_dim=1).to(device)\n",
    "trainer = GateEnhancedTrainer(model)\n",
    "\n",
    "# Generate synthetic data\n",
    "obs_seq = torch.randn(16, 20, 4, device=device)\n",
    "action_seq = torch.randn(16, 20, 1, device=device)\n",
    "reward_seq = torch.randn(16, 20, device=device)\n",
    "\n",
    "# Run training step\n",
    "metrics = trainer.train_step(obs_seq, action_seq, reward_seq)\n",
    "print(f\"Training metrics:\")\n",
    "for key, value in metrics.items():\n",
    "    print(f\"  {key}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.10 Comparison: Gate-Enhanced vs Standard\n",
    "\n",
    "Compare the gate-enhanced world model against the standard baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from collections import deque\n",
    "\n",
    "def collect_episodes(\n",
    "    env_name: str,\n",
    "    num_episodes: int = 10,\n",
    "    max_steps: int = 200\n",
    ") -> List[Dict[str, np.ndarray]]:\n",
    "    \"\"\"\n",
    "    Collect episodes from environment.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    env_name : str\n",
    "        Environment name\n",
    "    num_episodes : int\n",
    "        Number of episodes to collect\n",
    "    max_steps : int\n",
    "        Maximum steps per episode\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    List[Dict]\n",
    "        List of episode dictionaries\n",
    "    \"\"\"\n",
    "    env = gym.make(env_name)\n",
    "    episodes = []\n",
    "    \n",
    "    for _ in range(num_episodes):\n",
    "        obs_list, action_list, reward_list = [], [], []\n",
    "        \n",
    "        obs, _ = env.reset()\n",
    "        obs_list.append(obs)\n",
    "        \n",
    "        for _ in range(max_steps):\n",
    "            action = env.action_space.sample()\n",
    "            next_obs, reward, terminated, truncated, _ = env.step(action)\n",
    "            \n",
    "            action_list.append([action] if isinstance(action, (int, float)) else action)\n",
    "            reward_list.append(reward)\n",
    "            obs_list.append(next_obs)\n",
    "            \n",
    "            if terminated or truncated:\n",
    "                break\n",
    "            obs = next_obs\n",
    "        \n",
    "        # Remove last obs (not paired with action)\n",
    "        obs_list = obs_list[:-1]\n",
    "        \n",
    "        if len(obs_list) > 10:  # Minimum episode length\n",
    "            episodes.append({\n",
    "                'obs': np.array(obs_list, dtype=np.float32),\n",
    "                'actions': np.array(action_list, dtype=np.float32),\n",
    "                'rewards': np.array(reward_list, dtype=np.float32)\n",
    "            })\n",
    "    \n",
    "    env.close()\n",
    "    return episodes\n",
    "\n",
    "\n",
    "def create_batches(\n",
    "    episodes: List[Dict],\n",
    "    batch_size: int = 16,\n",
    "    seq_len: int = 20,\n",
    "    device: torch.device = device\n",
    ") -> List[Tuple[Tensor, Tensor, Tensor]]:\n",
    "    \"\"\"\n",
    "    Create training batches from episodes.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    episodes : List[Dict]\n",
    "        List of episode dictionaries\n",
    "    batch_size : int\n",
    "        Batch size\n",
    "    seq_len : int\n",
    "        Sequence length\n",
    "    device : torch.device\n",
    "        Device to put tensors on\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    List[Tuple[Tensor, Tensor, Tensor]]\n",
    "        List of (obs, actions, rewards) batches\n",
    "    \"\"\"\n",
    "    # Collect all valid sequences\n",
    "    sequences = []\n",
    "    for ep in episodes:\n",
    "        ep_len = len(ep['obs'])\n",
    "        for start in range(0, ep_len - seq_len, seq_len // 2):\n",
    "            sequences.append({\n",
    "                'obs': ep['obs'][start:start+seq_len],\n",
    "                'actions': ep['actions'][start:start+seq_len],\n",
    "                'rewards': ep['rewards'][start:start+seq_len]\n",
    "            })\n",
    "    \n",
    "    # Shuffle and batch\n",
    "    np.random.shuffle(sequences)\n",
    "    batches = []\n",
    "    \n",
    "    for i in range(0, len(sequences) - batch_size, batch_size):\n",
    "        batch_seqs = sequences[i:i+batch_size]\n",
    "        \n",
    "        obs = torch.tensor(\n",
    "            np.stack([s['obs'] for s in batch_seqs]),\n",
    "            dtype=torch.float32, device=device\n",
    "        )\n",
    "        actions = torch.tensor(\n",
    "            np.stack([s['actions'] for s in batch_seqs]),\n",
    "            dtype=torch.float32, device=device\n",
    "        )\n",
    "        rewards = torch.tensor(\n",
    "            np.stack([s['rewards'] for s in batch_seqs]),\n",
    "            dtype=torch.float32, device=device\n",
    "        )\n",
    "        \n",
    "        batches.append((obs, actions, rewards))\n",
    "    \n",
    "    return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard World Model for comparison (from Phase 2)\n",
    "class StandardWorldModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Standard world model without quantum gate enhancements.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, obs_dim: int, action_dim: int, config: Optional[Dict] = None):\n",
    "        super().__init__()\n",
    "        config = config or {}\n",
    "        \n",
    "        hidden_dim = config.get('hidden_dim', 256)\n",
    "        embed_dim = config.get('embed_dim', 64)\n",
    "        deter_dim = config.get('deter_dim', 128)\n",
    "        stoch_dim = config.get('stoch_dim', 32)\n",
    "        \n",
    "        self.obs_dim = obs_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.state_dim = deter_dim + stoch_dim\n",
    "        self.deter_dim = deter_dim\n",
    "        self.stoch_dim = stoch_dim\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(obs_dim, hidden_dim),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(hidden_dim, embed_dim),\n",
    "            nn.ELU()\n",
    "        )\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder_net = nn.Sequential(\n",
    "            nn.Linear(self.state_dim, hidden_dim),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ELU()\n",
    "        )\n",
    "        self.decoder_mean = nn.Linear(hidden_dim // 2, obs_dim)\n",
    "        self.decoder_log_std = nn.Linear(hidden_dim // 2, obs_dim)\n",
    "        \n",
    "        # RSSM\n",
    "        self.gru = nn.GRUCell(stoch_dim + action_dim, deter_dim)\n",
    "        \n",
    "        self.prior_net = nn.Sequential(\n",
    "            nn.Linear(deter_dim, hidden_dim),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(hidden_dim, stoch_dim * 2)\n",
    "        )\n",
    "        \n",
    "        self.posterior_net = nn.Sequential(\n",
    "            nn.Linear(deter_dim + embed_dim, hidden_dim),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(hidden_dim, stoch_dim * 2)\n",
    "        )\n",
    "        \n",
    "        # Reward predictor\n",
    "        self.reward_pred = nn.Sequential(\n",
    "            nn.Linear(self.state_dim, hidden_dim // 2),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(hidden_dim // 2, 1)\n",
    "        )\n",
    "    \n",
    "    def initial_state(self, batch_size: int, device: torch.device) -> Dict[str, Tensor]:\n",
    "        return {\n",
    "            'deter': torch.zeros(batch_size, self.deter_dim, device=device),\n",
    "            'stoch': torch.zeros(batch_size, self.stoch_dim, device=device)\n",
    "        }\n",
    "    \n",
    "    def get_full_state(self, state: Dict[str, Tensor]) -> Tensor:\n",
    "        return torch.cat([state['deter'], state['stoch']], dim=-1)\n",
    "    \n",
    "    def forward(self, obs_seq: Tensor, action_seq: Tensor) -> Dict[str, Tensor]:\n",
    "        batch_size, seq_len = obs_seq.shape[:2]\n",
    "        device = obs_seq.device\n",
    "        \n",
    "        state = self.initial_state(batch_size, device)\n",
    "        \n",
    "        states = []\n",
    "        prior_means, prior_stds = [], []\n",
    "        post_means, post_stds = [], []\n",
    "        \n",
    "        for t in range(seq_len):\n",
    "            # Encode\n",
    "            embed = self.encoder(obs_seq[:, t])\n",
    "            \n",
    "            # Update deterministic\n",
    "            gru_input = torch.cat([state['stoch'], action_seq[:, t]], dim=-1)\n",
    "            deter = self.gru(gru_input, state['deter'])\n",
    "            \n",
    "            # Prior\n",
    "            prior_stats = self.prior_net(deter)\n",
    "            prior_mean, prior_log_std = torch.chunk(prior_stats, 2, dim=-1)\n",
    "            prior_std = F.softplus(prior_log_std) + 0.1\n",
    "            \n",
    "            # Posterior\n",
    "            post_input = torch.cat([deter, embed], dim=-1)\n",
    "            post_stats = self.posterior_net(post_input)\n",
    "            post_mean, post_log_std = torch.chunk(post_stats, 2, dim=-1)\n",
    "            post_std = F.softplus(post_log_std) + 0.1\n",
    "            \n",
    "            # Sample\n",
    "            stoch = post_mean + post_std * torch.randn_like(post_std)\n",
    "            \n",
    "            state = {'deter': deter, 'stoch': stoch}\n",
    "            \n",
    "            states.append(self.get_full_state(state))\n",
    "            prior_means.append(prior_mean)\n",
    "            prior_stds.append(prior_std)\n",
    "            post_means.append(post_mean)\n",
    "            post_stds.append(post_std)\n",
    "        \n",
    "        states = torch.stack(states, dim=1)\n",
    "        \n",
    "        # Decode\n",
    "        flat_states = states.reshape(-1, states.shape[-1])\n",
    "        dec_hidden = self.decoder_net(flat_states)\n",
    "        obs_mean = self.decoder_mean(dec_hidden).reshape(batch_size, seq_len, -1)\n",
    "        obs_log_std = self.decoder_log_std(dec_hidden).clamp(-10, 2).reshape(batch_size, seq_len, -1)\n",
    "        \n",
    "        reward_pred = self.reward_pred(flat_states).reshape(batch_size, seq_len)\n",
    "        \n",
    "        return {\n",
    "            'states': states,\n",
    "            'obs_mean': obs_mean,\n",
    "            'obs_log_std': obs_log_std,\n",
    "            'reward_pred': reward_pred,\n",
    "            'prior_mean': torch.stack(prior_means, dim=1),\n",
    "            'prior_std': torch.stack(prior_stds, dim=1),\n",
    "            'post_mean': torch.stack(post_means, dim=1),\n",
    "            'post_std': torch.stack(post_stds, dim=1)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StandardTrainer:\n",
    "    \"\"\"Standard trainer for comparison.\"\"\"\n",
    "    \n",
    "    def __init__(self, model, learning_rate=1e-4, kl_weight=1.0, free_nats=3.0):\n",
    "        self.model = model\n",
    "        self.kl_weight = kl_weight\n",
    "        self.free_nats = free_nats\n",
    "        self.optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "        self.logger = MetricLogger(name='standard')\n",
    "    \n",
    "    def train_step(self, obs_seq, action_seq, reward_seq):\n",
    "        self.model.train()\n",
    "        self.optimizer.zero_grad()\n",
    "        \n",
    "        outputs = self.model(obs_seq, action_seq)\n",
    "        \n",
    "        # Reconstruction loss\n",
    "        obs_dist = torch.distributions.Normal(outputs['obs_mean'], torch.exp(outputs['obs_log_std']))\n",
    "        recon_loss = -obs_dist.log_prob(obs_seq).mean()\n",
    "        \n",
    "        # KL loss\n",
    "        prior_dist = torch.distributions.Normal(outputs['prior_mean'], outputs['prior_std'])\n",
    "        post_dist = torch.distributions.Normal(outputs['post_mean'], outputs['post_std'])\n",
    "        kl_div = torch.distributions.kl_divergence(post_dist, prior_dist)\n",
    "        kl_loss = torch.maximum(kl_div.mean(), torch.tensor(self.free_nats, device=kl_div.device))\n",
    "        \n",
    "        # Reward loss\n",
    "        reward_loss = F.mse_loss(outputs['reward_pred'], reward_seq)\n",
    "        \n",
    "        total_loss = recon_loss + self.kl_weight * kl_loss + reward_loss\n",
    "        \n",
    "        total_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.model.parameters(), 100.0)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        metrics = {\n",
    "            'recon_loss': recon_loss.item(),\n",
    "            'kl_loss': kl_loss.item(),\n",
    "            'reward_loss': reward_loss.item(),\n",
    "            'total_loss': total_loss.item()\n",
    "        }\n",
    "        \n",
    "        for key, value in metrics.items():\n",
    "            self.logger.log(**{key: value})\n",
    "        \n",
    "        return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_comparison(\n",
    "    env_name: str = 'CartPole-v1',\n",
    "    num_episodes: int = 20,\n",
    "    num_epochs: int = 50,\n",
    "    batch_size: int = 16,\n",
    "    seq_len: int = 20,\n",
    "    seed: int = 42\n",
    ") -> Dict[str, List[float]]:\n",
    "    \"\"\"\n",
    "    Run comparison between gate-enhanced and standard world models.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    env_name : str\n",
    "        Environment name\n",
    "    num_episodes : int\n",
    "        Number of episodes to collect\n",
    "    num_epochs : int\n",
    "        Number of training epochs\n",
    "    batch_size : int\n",
    "        Batch size\n",
    "    seq_len : int\n",
    "        Sequence length\n",
    "    seed : int\n",
    "        Random seed\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Dict[str, List[float]]\n",
    "        Training histories for both models\n",
    "    \"\"\"\n",
    "    set_seed(seed)\n",
    "    \n",
    "    print(f\"Collecting {num_episodes} episodes from {env_name}...\")\n",
    "    episodes = collect_episodes(env_name, num_episodes)\n",
    "    \n",
    "    # Get dimensions\n",
    "    obs_dim = episodes[0]['obs'].shape[1]\n",
    "    action_dim = episodes[0]['actions'].shape[1]\n",
    "    \n",
    "    print(f\"Observation dim: {obs_dim}, Action dim: {action_dim}\")\n",
    "    print(f\"Creating training batches...\")\n",
    "    \n",
    "    batches = create_batches(episodes, batch_size, seq_len)\n",
    "    print(f\"Created {len(batches)} batches\")\n",
    "    \n",
    "    # Create models\n",
    "    print(\"\\nInitializing models...\")\n",
    "    \n",
    "    gate_model = GateEnhancedWorldModel(obs_dim, action_dim).to(device)\n",
    "    standard_model = StandardWorldModel(obs_dim, action_dim).to(device)\n",
    "    \n",
    "    gate_params = sum(p.numel() for p in gate_model.parameters())\n",
    "    standard_params = sum(p.numel() for p in standard_model.parameters())\n",
    "    print(f\"Gate-enhanced parameters: {gate_params:,}\")\n",
    "    print(f\"Standard parameters: {standard_params:,}\")\n",
    "    \n",
    "    # Create trainers\n",
    "    gate_trainer = GateEnhancedTrainer(gate_model)\n",
    "    standard_trainer = StandardTrainer(standard_model)\n",
    "    \n",
    "    # Training histories\n",
    "    histories = {\n",
    "        'gate_loss': [],\n",
    "        'standard_loss': [],\n",
    "        'gate_recon': [],\n",
    "        'standard_recon': []\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nTraining for {num_epochs} epochs...\")\n",
    "    timer = Timer().start()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        gate_losses, standard_losses = [], []\n",
    "        gate_recons, standard_recons = [], []\n",
    "        \n",
    "        for obs, actions, rewards in batches:\n",
    "            # Train gate-enhanced\n",
    "            gate_metrics = gate_trainer.train_step(obs, actions, rewards)\n",
    "            gate_losses.append(gate_metrics['total_loss'])\n",
    "            gate_recons.append(gate_metrics['recon_loss'])\n",
    "            \n",
    "            # Train standard\n",
    "            standard_metrics = standard_trainer.train_step(obs, actions, rewards)\n",
    "            standard_losses.append(standard_metrics['total_loss'])\n",
    "            standard_recons.append(standard_metrics['recon_loss'])\n",
    "        \n",
    "        # Record epoch averages\n",
    "        histories['gate_loss'].append(np.mean(gate_losses))\n",
    "        histories['standard_loss'].append(np.mean(standard_losses))\n",
    "        histories['gate_recon'].append(np.mean(gate_recons))\n",
    "        histories['standard_recon'].append(np.mean(standard_recons))\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}:\")\n",
    "            print(f\"  Gate-enhanced loss: {histories['gate_loss'][-1]:.4f}\")\n",
    "            print(f\"  Standard loss: {histories['standard_loss'][-1]:.4f}\")\n",
    "    \n",
    "    elapsed = timer.stop()\n",
    "    print(f\"\\nTraining completed in {elapsed:.2f}s\")\n",
    "    \n",
    "    return histories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run comparison\n",
    "histories = run_comparison(\n",
    "    env_name='CartPole-v1',\n",
    "    num_episodes=20,\n",
    "    num_epochs=50,\n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Total loss\n",
    "ax = axes[0]\n",
    "ax.plot(histories['gate_loss'], label='Gate-Enhanced', color=COLORS['gates'], linewidth=2)\n",
    "ax.plot(histories['standard_loss'], label='Standard', color=COLORS['baseline'], linewidth=2)\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Total Loss')\n",
    "ax.set_title('Total Loss Comparison')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Reconstruction loss\n",
    "ax = axes[1]\n",
    "ax.plot(histories['gate_recon'], label='Gate-Enhanced', color=COLORS['gates'], linewidth=2)\n",
    "ax.plot(histories['standard_recon'], label='Standard', color=COLORS['baseline'], linewidth=2)\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Reconstruction Loss')\n",
    "ax.set_title('Reconstruction Loss Comparison')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/figures/gate_enhanced_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Print statistics\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Final Comparison (last 10 epochs average):\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Gate-enhanced total loss: {np.mean(histories['gate_loss'][-10:]):.4f}\")\n",
    "print(f\"Standard total loss: {np.mean(histories['standard_loss'][-10:]):.4f}\")\n",
    "print(f\"Gate-enhanced recon loss: {np.mean(histories['gate_recon'][-10:]):.4f}\")\n",
    "print(f\"Standard recon loss: {np.mean(histories['standard_recon'][-10:]):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.11 Gate Layer Analysis\n",
    "\n",
    "Analyze the learned gate parameters to understand what the model has learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_gate_layers(model: GateEnhancedWorldModel):\n",
    "    \"\"\"\n",
    "    Analyze the learned parameters in quantum gate layers.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model : GateEnhancedWorldModel\n",
    "        The trained model\n",
    "    \"\"\"\n",
    "    print(\"Gate Layer Analysis\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Analyze encoder gate block\n",
    "    print(\"\\nEncoder Gate Block:\")\n",
    "    for i, layer in enumerate(model.encoder.gate_block.layers):\n",
    "        print(f\"\\n  Layer {i+1}:\")\n",
    "        for sublayer in layer:\n",
    "            if isinstance(sublayer, RotationLayer):\n",
    "                angles = sublayer.angles.detach().cpu().numpy()\n",
    "                print(f\"    Rotation angles - mean: {angles.mean():.4f}, std: {angles.std():.4f}\")\n",
    "            elif isinstance(sublayer, CNOTLayer):\n",
    "                entanglement = sublayer.get_entanglement_strength().item()\n",
    "                print(f\"    CNOT entanglement strength: {entanglement:.4f}\")\n",
    "            elif isinstance(sublayer, PhaseLayer):\n",
    "                phases = sublayer.phases.detach().cpu().numpy()\n",
    "                print(f\"    Phase angles - mean: {phases.mean():.4f}, std: {phases.std():.4f}\")\n",
    "            elif isinstance(sublayer, HadamardLayer):\n",
    "                scale = sublayer.scale.detach().cpu().numpy()\n",
    "                print(f\"    Hadamard scale - mean: {scale.mean():.4f}, std: {scale.std():.4f}\")\n",
    "    \n",
    "    # Analyze RSSM gate blocks\n",
    "    print(\"\\nRSSM Prior Gate Block:\")\n",
    "    for i, layer in enumerate(model.rssm.prior_gate.layers):\n",
    "        print(f\"  Layer {i+1}:\")\n",
    "        for sublayer in layer:\n",
    "            if isinstance(sublayer, CNOTLayer):\n",
    "                entanglement = sublayer.get_entanglement_strength().item()\n",
    "                print(f\"    CNOT entanglement: {entanglement:.4f}\")\n",
    "\n",
    "\n",
    "# Run analysis on a trained model\n",
    "print(\"Creating and training model for analysis...\")\n",
    "analysis_model = GateEnhancedWorldModel(obs_dim=4, action_dim=1).to(device)\n",
    "\n",
    "# Quick training\n",
    "trainer = GateEnhancedTrainer(analysis_model)\n",
    "for _ in range(10):\n",
    "    obs = torch.randn(16, 20, 4, device=device)\n",
    "    actions = torch.randn(16, 20, 1, device=device)\n",
    "    rewards = torch.randn(16, 20, device=device)\n",
    "    trainer.train_step(obs, actions, rewards)\n",
    "\n",
    "analyze_gate_layers(analysis_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.12 Summary\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "1. **Hadamard Layers**: Create orthogonal feature mixing, preserving information while enabling complex transformations\n",
    "\n",
    "2. **Rotation Layers**: Learn task-specific rotations in feature space, providing flexible parameterized transformations\n",
    "\n",
    "3. **CNOT Layers**: Create controlled dependencies between features, enabling entanglement-like correlations\n",
    "\n",
    "4. **Phase Layers**: Apply learnable phase modulations for additional expressivity\n",
    "\n",
    "### Implementation Notes\n",
    "\n",
    "- All layers preserve dimension and can be stacked\n",
    "- Residual connections improve training stability\n",
    "- The gate-enhanced model has more parameters but offers richer representations\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Phase 6: Error Correction Ensemble\n",
    "- Phase 7: Comprehensive Comparison\n",
    "- Phase 8: Ablation Studies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Phase 5: Gate-Enhanced Neural Layers - COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nImplemented:\")\n",
    "print(\"  - HadamardLayer: Orthogonal feature mixing\")\n",
    "print(\"  - RotationLayer: Parameterized rotations (Rx, Ry, Rz)\")\n",
    "print(\"  - CNOTLayer: Controlled operations for correlations\")\n",
    "print(\"  - PhaseLayer: Phase modulations\")\n",
    "print(\"  - QuantumGateBlock: Composite quantum circuit-like block\")\n",
    "print(\"  - GateEnhancedWorldModel: Full world model integration\")\n",
    "print(\"  - Comparison with standard world model\")\n",
    "print(\"\\nReady for Phase 6: Error Correction Ensemble\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
