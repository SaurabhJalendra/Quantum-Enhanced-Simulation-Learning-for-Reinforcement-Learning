{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 6B: Fully Integrated Quantum-Enhanced World Model\n",
    "\n",
    "**Quantum-Enhanced Simulation Learning for Reinforcement Learning**\n",
    "\n",
    "Author: Saurabh Jalendra  \n",
    "Institution: BITS Pilani (WILP Division)  \n",
    "Date: December 2025\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook implements the **Fully Integrated** approach that combines ALL quantum-inspired components:\n",
    "\n",
    "| Component | Source | Target Bottleneck |\n",
    "|-----------|--------|-------------------|\n",
    "| QAOA Optimizer | Notebook 03 | Local Minima |\n",
    "| Superposition Replay | Notebook 04 | Sample Inefficiency |\n",
    "| Gate-Enhanced Layers | Notebook 05 | Slow Convergence |\n",
    "| Error Correction Ensemble | Notebook 06 | Compounding Errors |\n",
    "\n",
    "**Research Question:** Does combining all quantum-inspired methods provide better results than any single method alone?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6B.1 Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "from dataclasses import dataclass\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import json\n",
    "import time\n",
    "from scipy import stats\n",
    "\n",
    "# Device configuration\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {DEVICE}')\n",
    "\n",
    "# Standard configuration (consistent with all notebooks)\n",
    "CONFIG = {\n",
    "    'obs_dim': 4,\n",
    "    'action_dim': 2,\n",
    "    'stoch_dim': 64,\n",
    "    'deter_dim': 512,\n",
    "    'hidden_dim': 512,\n",
    "    'batch_size': 32,\n",
    "    'seq_len': 20,\n",
    "    'num_epochs': 50,\n",
    "    'learning_rate': 3e-4,\n",
    "    'num_episodes': 100,\n",
    "    'num_ensemble': 5,\n",
    "    'seeds': [42, 123, 456, 789, 1024],\n",
    "}\n",
    "\n",
    "print(f'Configuration: {CONFIG}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6B.2 Component 1: Gate-Enhanced Layers\n",
    "\n",
    "Quantum gate-inspired neural network layers for richer representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HadamardLayer(nn.Module):\n",
    "    \"\"\"Hadamard-inspired feature mixing layer.\"\"\"\n",
    "    \n",
    "    def __init__(self, dim: int):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        # Learnable Hadamard-like transformation\n",
    "        self.weight = nn.Parameter(torch.randn(dim, dim) / np.sqrt(dim))\n",
    "        self.bias = nn.Parameter(torch.zeros(dim))\n",
    "        # Make it close to orthogonal\n",
    "        nn.init.orthogonal_(self.weight)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Apply Hadamard-like mixing\n",
    "        return F.linear(x, self.weight, self.bias)\n",
    "\n",
    "\n",
    "class PhaseLayer(nn.Module):\n",
    "    \"\"\"Phase gate-inspired modulation layer.\"\"\"\n",
    "    \n",
    "    def __init__(self, dim: int):\n",
    "        super().__init__()\n",
    "        self.phase = nn.Parameter(torch.zeros(dim))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Apply learnable phase modulation\n",
    "        return x * torch.cos(self.phase) + torch.roll(x, 1, dims=-1) * torch.sin(self.phase)\n",
    "\n",
    "\n",
    "class EntanglementLayer(nn.Module):\n",
    "    \"\"\"CNOT-inspired entanglement layer.\"\"\"\n",
    "    \n",
    "    def __init__(self, dim: int, num_pairs: int = 4):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.num_pairs = min(num_pairs, dim // 2)\n",
    "        self.control_weights = nn.Parameter(torch.randn(self.num_pairs))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        output = x.clone()\n",
    "        for i in range(self.num_pairs):\n",
    "            control_idx = i * 2\n",
    "            target_idx = i * 2 + 1\n",
    "            if target_idx < self.dim:\n",
    "                # CNOT-like: target XOR control (soft version)\n",
    "                gate = torch.sigmoid(self.control_weights[i])\n",
    "                output[..., target_idx] = output[..., target_idx] * (1 - gate) + \\\n",
    "                                          (output[..., target_idx] * output[..., control_idx]) * gate\n",
    "        return output\n",
    "\n",
    "\n",
    "class QuantumGateBlock(nn.Module):\n",
    "    \"\"\"Combined quantum gate block with all gate types.\"\"\"\n",
    "    \n",
    "    def __init__(self, dim: int, use_hadamard: bool = True, use_phase: bool = True, use_entangle: bool = True):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        if use_hadamard:\n",
    "            layers.append(HadamardLayer(dim))\n",
    "            layers.append(nn.ELU())\n",
    "        if use_phase:\n",
    "            layers.append(PhaseLayer(dim))\n",
    "        if use_entangle:\n",
    "            layers.append(EntanglementLayer(dim))\n",
    "        layers.append(nn.LayerNorm(dim))\n",
    "        self.block = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.block(x) + x  # Residual connection\n",
    "\n",
    "print('Gate-Enhanced Layers defined.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6B.3 Component 2: Gate-Enhanced World Model\n",
    "\n",
    "World model with quantum gate layers integrated into encoder, decoder, and RSSM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GateEnhancedWorldModel(nn.Module):\n",
    "    \"\"\"World model with quantum gate-enhanced layers.\"\"\"\n",
    "    \n",
    "    def __init__(self, obs_dim: int, action_dim: int, config: Dict):\n",
    "        super().__init__()\n",
    "        self.obs_dim = obs_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.stoch_dim = config.get('stoch_dim', 64)\n",
    "        self.deter_dim = config.get('deter_dim', 512)\n",
    "        self.hidden_dim = config.get('hidden_dim', 512)\n",
    "        self.state_dim = self.stoch_dim + self.deter_dim\n",
    "        \n",
    "        # Gate-enhanced encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(obs_dim, self.hidden_dim),\n",
    "            nn.ELU(),\n",
    "            QuantumGateBlock(self.hidden_dim),\n",
    "            nn.Linear(self.hidden_dim, self.hidden_dim // 2),\n",
    "            nn.ELU(),\n",
    "        )\n",
    "        \n",
    "        # RSSM components with gate enhancement\n",
    "        self.gru = nn.GRUCell(self.stoch_dim + action_dim, self.deter_dim)\n",
    "        \n",
    "        self.prior = nn.Sequential(\n",
    "            nn.Linear(self.deter_dim, self.hidden_dim // 2),\n",
    "            nn.ELU(),\n",
    "            QuantumGateBlock(self.hidden_dim // 2),\n",
    "            nn.Linear(self.hidden_dim // 2, self.stoch_dim * 2)\n",
    "        )\n",
    "        \n",
    "        self.posterior = nn.Sequential(\n",
    "            nn.Linear(self.deter_dim + self.hidden_dim // 2, self.hidden_dim // 2),\n",
    "            nn.ELU(),\n",
    "            QuantumGateBlock(self.hidden_dim // 2),\n",
    "            nn.Linear(self.hidden_dim // 2, self.stoch_dim * 2)\n",
    "        )\n",
    "        \n",
    "        # Gate-enhanced decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(self.state_dim, self.hidden_dim),\n",
    "            nn.ELU(),\n",
    "            QuantumGateBlock(self.hidden_dim),\n",
    "            nn.Linear(self.hidden_dim, obs_dim * 2)\n",
    "        )\n",
    "        \n",
    "        # Reward predictor\n",
    "        self.reward_pred = nn.Sequential(\n",
    "            nn.Linear(self.state_dim, self.hidden_dim // 2),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(self.hidden_dim // 2, 1)\n",
    "        )\n",
    "    \n",
    "    def initial_state(self, batch_size: int):\n",
    "        return {\n",
    "            'deter': torch.zeros(batch_size, self.deter_dim, device=DEVICE),\n",
    "            'stoch': torch.zeros(batch_size, self.stoch_dim, device=DEVICE)\n",
    "        }\n",
    "    \n",
    "    def get_dist(self, stats):\n",
    "        mean, log_std = stats.chunk(2, dim=-1)\n",
    "        std = F.softplus(log_std) + 0.1\n",
    "        return torch.distributions.Normal(mean, std)\n",
    "    \n",
    "    def forward(self, obs_seq, action_seq):\n",
    "        batch_size, seq_len = obs_seq.shape[:2]\n",
    "        \n",
    "        # Initialize\n",
    "        state = self.initial_state(batch_size)\n",
    "        \n",
    "        # Collect outputs\n",
    "        prior_means, prior_stds = [], []\n",
    "        post_means, post_stds = [], []\n",
    "        obs_means, obs_stds = [], []\n",
    "        rewards = []\n",
    "        states = []\n",
    "        \n",
    "        for t in range(seq_len):\n",
    "            # Encode observation\n",
    "            embed = self.encoder(obs_seq[:, t])\n",
    "            \n",
    "            # Prior (before seeing observation)\n",
    "            prior_stats = self.prior(state['deter'])\n",
    "            prior_dist = self.get_dist(prior_stats)\n",
    "            \n",
    "            # Posterior (after seeing observation)\n",
    "            post_input = torch.cat([state['deter'], embed], dim=-1)\n",
    "            post_stats = self.posterior(post_input)\n",
    "            post_dist = self.get_dist(post_stats)\n",
    "            \n",
    "            # Sample stochastic state\n",
    "            stoch = post_dist.rsample()\n",
    "            \n",
    "            # Update deterministic state\n",
    "            gru_input = torch.cat([stoch, action_seq[:, t]], dim=-1)\n",
    "            deter = self.gru(gru_input, state['deter'])\n",
    "            \n",
    "            # Full state\n",
    "            full_state = torch.cat([deter, stoch], dim=-1)\n",
    "            \n",
    "            # Decode\n",
    "            obs_stats = self.decoder(full_state)\n",
    "            obs_dist = self.get_dist(obs_stats)\n",
    "            \n",
    "            # Reward\n",
    "            reward = self.reward_pred(full_state)\n",
    "            \n",
    "            # Store\n",
    "            prior_means.append(prior_dist.mean)\n",
    "            prior_stds.append(prior_dist.stddev)\n",
    "            post_means.append(post_dist.mean)\n",
    "            post_stds.append(post_dist.stddev)\n",
    "            obs_means.append(obs_dist.mean)\n",
    "            obs_stds.append(obs_dist.stddev)\n",
    "            rewards.append(reward)\n",
    "            states.append(full_state)\n",
    "            \n",
    "            # Update state\n",
    "            state = {'deter': deter, 'stoch': stoch}\n",
    "        \n",
    "        return {\n",
    "            'prior_mean': torch.stack(prior_means, dim=1),\n",
    "            'prior_std': torch.stack(prior_stds, dim=1),\n",
    "            'post_mean': torch.stack(post_means, dim=1),\n",
    "            'post_std': torch.stack(post_stds, dim=1),\n",
    "            'obs_mean': torch.stack(obs_means, dim=1),\n",
    "            'obs_std': torch.stack(obs_stds, dim=1),\n",
    "            'reward': torch.stack(rewards, dim=1).squeeze(-1),\n",
    "            'states': torch.stack(states, dim=1),\n",
    "        }\n",
    "\n",
    "# Test\n",
    "model = GateEnhancedWorldModel(CONFIG['obs_dim'], CONFIG['action_dim'], CONFIG).to(DEVICE)\n",
    "print(f'GateEnhancedWorldModel parameters: {sum(p.numel() for p in model.parameters()):,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6B.4 Component 3: Error Correction Ensemble\n",
    "\n",
    "Ensemble of gate-enhanced models with majority voting for robust predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ErrorCorrectionEnsemble(nn.Module):\n",
    "    \"\"\"Ensemble of gate-enhanced world models with error correction.\"\"\"\n",
    "    \n",
    "    def __init__(self, obs_dim: int, action_dim: int, config: Dict, num_models: int = 5):\n",
    "        super().__init__()\n",
    "        self.num_models = num_models\n",
    "        self.models = nn.ModuleList([\n",
    "            GateEnhancedWorldModel(obs_dim, action_dim, config)\n",
    "            for _ in range(num_models)\n",
    "        ])\n",
    "        self.correction_method = config.get('correction_method', 'weighted')\n",
    "    \n",
    "    def forward(self, obs_seq, action_seq, return_all: bool = False):\n",
    "        # Get predictions from all models\n",
    "        all_outputs = [model(obs_seq, action_seq) for model in self.models]\n",
    "        \n",
    "        # Stack predictions\n",
    "        obs_preds = torch.stack([out['obs_mean'] for out in all_outputs], dim=0)\n",
    "        reward_preds = torch.stack([out['reward'] for out in all_outputs], dim=0)\n",
    "        \n",
    "        # Apply error correction\n",
    "        if self.correction_method == 'majority':\n",
    "            corrected_obs = obs_preds.median(dim=0).values\n",
    "            corrected_reward = reward_preds.median(dim=0).values\n",
    "        elif self.correction_method == 'weighted':\n",
    "            # Weight by inverse disagreement\n",
    "            disagreement = obs_preds.var(dim=0, keepdim=True)\n",
    "            weights = 1.0 / (disagreement.mean(dim=-1, keepdim=True) + 1e-6)\n",
    "            weights = weights / weights.sum(dim=0, keepdim=True)\n",
    "            corrected_obs = (obs_preds * weights).sum(dim=0)\n",
    "            corrected_reward = (reward_preds * weights.squeeze(-1)).sum(dim=0)\n",
    "        else:  # simple average\n",
    "            corrected_obs = obs_preds.mean(dim=0)\n",
    "            corrected_reward = reward_preds.mean(dim=0)\n",
    "        \n",
    "        # Calculate uncertainty\n",
    "        uncertainty = obs_preds.std(dim=0).mean(dim=-1)\n",
    "        \n",
    "        result = {\n",
    "            'obs_mean': corrected_obs,\n",
    "            'reward': corrected_reward,\n",
    "            'uncertainty': uncertainty,\n",
    "            'prior_mean': all_outputs[0]['prior_mean'],\n",
    "            'prior_std': all_outputs[0]['prior_std'],\n",
    "            'post_mean': all_outputs[0]['post_mean'],\n",
    "            'post_std': all_outputs[0]['post_std'],\n",
    "            'obs_std': all_outputs[0]['obs_std'],\n",
    "        }\n",
    "        \n",
    "        if return_all:\n",
    "            result['all_outputs'] = all_outputs\n",
    "        \n",
    "        return result\n",
    "\n",
    "# Test\n",
    "ensemble = ErrorCorrectionEnsemble(CONFIG['obs_dim'], CONFIG['action_dim'], CONFIG, num_models=5).to(DEVICE)\n",
    "print(f'ErrorCorrectionEnsemble total parameters: {sum(p.numel() for p in ensemble.parameters()):,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6B.5 Component 4: Superposition Replay Buffer\n",
    "\n",
    "Experience replay with quantum superposition-inspired sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Episode:\n",
    "    observations: np.ndarray\n",
    "    actions: np.ndarray\n",
    "    rewards: np.ndarray\n",
    "    total_reward: float\n",
    "\n",
    "\n",
    "class SuperpositionReplayBuffer:\n",
    "    \"\"\"Replay buffer with quantum superposition-inspired sampling.\"\"\"\n",
    "    \n",
    "    def __init__(self, capacity: int = 1000, parallel_samples: int = 4):\n",
    "        self.capacity = capacity\n",
    "        self.parallel_samples = parallel_samples\n",
    "        self.episodes: List[Episode] = []\n",
    "        self.amplitudes: np.ndarray = np.array([])\n",
    "    \n",
    "    def add_episode(self, episode: Episode):\n",
    "        if len(self.episodes) >= self.capacity:\n",
    "            self.episodes.pop(0)\n",
    "        self.episodes.append(episode)\n",
    "        self._update_amplitudes()\n",
    "    \n",
    "    def _update_amplitudes(self):\n",
    "        \"\"\"Update quantum-like amplitudes based on episode quality.\"\"\"\n",
    "        if not self.episodes:\n",
    "            self.amplitudes = np.array([])\n",
    "            return\n",
    "        \n",
    "        rewards = np.array([ep.total_reward for ep in self.episodes])\n",
    "        # Normalize to create amplitude distribution\n",
    "        if rewards.std() > 0:\n",
    "            normalized = (rewards - rewards.mean()) / rewards.std()\n",
    "        else:\n",
    "            normalized = np.zeros_like(rewards)\n",
    "        \n",
    "        # Convert to probabilities (amplitude squared)\n",
    "        amplitudes = np.exp(normalized)\n",
    "        self.amplitudes = amplitudes / amplitudes.sum()\n",
    "    \n",
    "    def sample_superposition(self, batch_size: int, seq_len: int) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Sample using superposition-inspired parallel sampling.\"\"\"\n",
    "        if len(self.episodes) < self.parallel_samples:\n",
    "            return self.sample_standard(batch_size, seq_len)\n",
    "        \n",
    "        # Sample multiple episodes in parallel (superposition)\n",
    "        obs_batch, act_batch, rew_batch = [], [], []\n",
    "        \n",
    "        for _ in range(batch_size):\n",
    "            # Sample parallel_samples episodes based on amplitudes\n",
    "            indices = np.random.choice(\n",
    "                len(self.episodes), \n",
    "                size=self.parallel_samples,\n",
    "                p=self.amplitudes,\n",
    "                replace=True\n",
    "            )\n",
    "            \n",
    "            # \"Collapse\" to one by weighted combination\n",
    "            selected_amplitudes = self.amplitudes[indices]\n",
    "            collapse_probs = selected_amplitudes / selected_amplitudes.sum()\n",
    "            chosen_idx = np.random.choice(indices, p=collapse_probs)\n",
    "            \n",
    "            episode = self.episodes[chosen_idx]\n",
    "            \n",
    "            # Random start point\n",
    "            max_start = max(0, len(episode.observations) - seq_len)\n",
    "            start = np.random.randint(0, max_start + 1) if max_start > 0 else 0\n",
    "            end = min(start + seq_len, len(episode.observations))\n",
    "            \n",
    "            obs = episode.observations[start:end]\n",
    "            act = episode.actions[start:end]\n",
    "            rew = episode.rewards[start:end]\n",
    "            \n",
    "            # Pad if necessary\n",
    "            if len(obs) < seq_len:\n",
    "                pad_len = seq_len - len(obs)\n",
    "                obs = np.pad(obs, ((0, pad_len), (0, 0)), mode='edge')\n",
    "                act = np.pad(act, ((0, pad_len), (0, 0)), mode='edge')\n",
    "                rew = np.pad(rew, (0, pad_len), mode='edge')\n",
    "            \n",
    "            obs_batch.append(obs)\n",
    "            act_batch.append(act)\n",
    "            rew_batch.append(rew)\n",
    "        \n",
    "        return (\n",
    "            torch.FloatTensor(np.array(obs_batch)).to(DEVICE),\n",
    "            torch.FloatTensor(np.array(act_batch)).to(DEVICE),\n",
    "            torch.FloatTensor(np.array(rew_batch)).to(DEVICE)\n",
    "        )\n",
    "    \n",
    "    def sample_standard(self, batch_size: int, seq_len: int) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Standard uniform sampling.\"\"\"\n",
    "        obs_batch, act_batch, rew_batch = [], [], []\n",
    "        \n",
    "        for _ in range(batch_size):\n",
    "            episode = np.random.choice(self.episodes)\n",
    "            max_start = max(0, len(episode.observations) - seq_len)\n",
    "            start = np.random.randint(0, max_start + 1) if max_start > 0 else 0\n",
    "            end = min(start + seq_len, len(episode.observations))\n",
    "            \n",
    "            obs = episode.observations[start:end]\n",
    "            act = episode.actions[start:end]\n",
    "            rew = episode.rewards[start:end]\n",
    "            \n",
    "            if len(obs) < seq_len:\n",
    "                pad_len = seq_len - len(obs)\n",
    "                obs = np.pad(obs, ((0, pad_len), (0, 0)), mode='edge')\n",
    "                act = np.pad(act, ((0, pad_len), (0, 0)), mode='edge')\n",
    "                rew = np.pad(rew, (0, pad_len), mode='edge')\n",
    "            \n",
    "            obs_batch.append(obs)\n",
    "            act_batch.append(act)\n",
    "            rew_batch.append(rew)\n",
    "        \n",
    "        return (\n",
    "            torch.FloatTensor(np.array(obs_batch)).to(DEVICE),\n",
    "            torch.FloatTensor(np.array(act_batch)).to(DEVICE),\n",
    "            torch.FloatTensor(np.array(rew_batch)).to(DEVICE)\n",
    "        )\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.episodes)\n",
    "\n",
    "print('SuperpositionReplayBuffer defined.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6B.6 Component 5: QAOA-Inspired Optimizer\n",
    "\n",
    "Optimizer that alternates between cost and mixing operators to escape local minima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QAOAOptimizer:\n",
    "    \"\"\"QAOA-inspired optimizer with alternating cost/mixing phases.\"\"\"\n",
    "    \n",
    "    def __init__(self, params, lr: float = 3e-4, p_layers: int = 4,\n",
    "                 gamma_init: float = 0.1, beta_init: float = 0.001):\n",
    "        self.params = list(params)\n",
    "        self.lr = lr\n",
    "        self.p_layers = p_layers\n",
    "        \n",
    "        # QAOA parameters\n",
    "        self.gammas = [gamma_init] * p_layers\n",
    "        self.betas = [beta_init] * p_layers\n",
    "        \n",
    "        # Base optimizer\n",
    "        self.base_optimizer = torch.optim.Adam(self.params, lr=lr)\n",
    "        \n",
    "        # State tracking\n",
    "        self.step_count = 0\n",
    "        self.current_layer = 0\n",
    "        self.phase = 'cost'  # 'cost' or 'mixing'\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        self.base_optimizer.zero_grad()\n",
    "    \n",
    "    def step(self, loss: torch.Tensor = None):\n",
    "        \"\"\"Perform QAOA-inspired optimization step.\"\"\"\n",
    "        self.step_count += 1\n",
    "        \n",
    "        # Determine current phase\n",
    "        layer_idx = self.current_layer % self.p_layers\n",
    "        \n",
    "        if self.phase == 'cost':\n",
    "            # Cost phase: standard gradient descent with gamma scaling\n",
    "            gamma = self.gammas[layer_idx]\n",
    "            for param in self.params:\n",
    "                if param.grad is not None:\n",
    "                    param.grad.data.mul_(gamma)\n",
    "            self.base_optimizer.step()\n",
    "            self.phase = 'mixing'\n",
    "        else:\n",
    "            # Mixing phase: add exploration noise\n",
    "            beta = self.betas[layer_idx]\n",
    "            with torch.no_grad():\n",
    "                for param in self.params:\n",
    "                    if param.requires_grad:\n",
    "                        noise = torch.randn_like(param) * beta * self.lr\n",
    "                        param.add_(noise)\n",
    "            self.phase = 'cost'\n",
    "            self.current_layer += 1\n",
    "    \n",
    "    def state_dict(self):\n",
    "        return {\n",
    "            'base_optimizer': self.base_optimizer.state_dict(),\n",
    "            'step_count': self.step_count,\n",
    "            'gammas': self.gammas,\n",
    "            'betas': self.betas,\n",
    "        }\n",
    "    \n",
    "    def load_state_dict(self, state_dict):\n",
    "        self.base_optimizer.load_state_dict(state_dict['base_optimizer'])\n",
    "        self.step_count = state_dict['step_count']\n",
    "        self.gammas = state_dict['gammas']\n",
    "        self.betas = state_dict['betas']\n",
    "\n",
    "print('QAOAOptimizer defined.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6B.7 Fully Integrated Trainer\n",
    "\n",
    "Training loop that combines all quantum-inspired components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullyIntegratedTrainer:\n",
    "    \"\"\"\n",
    "    Fully integrated trainer combining:\n",
    "    - QAOA optimizer (escapes local minima)\n",
    "    - Superposition replay (sample efficiency)\n",
    "    - Gate-enhanced layers (better representations)\n",
    "    - Error correction ensemble (robust predictions)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, ensemble: ErrorCorrectionEnsemble, buffer: SuperpositionReplayBuffer,\n",
    "                 config: Dict):\n",
    "        self.ensemble = ensemble\n",
    "        self.buffer = buffer\n",
    "        self.config = config\n",
    "        \n",
    "        # Create QAOA optimizer for each model in ensemble\n",
    "        self.optimizers = [\n",
    "            QAOAOptimizer(\n",
    "                model.parameters(),\n",
    "                lr=config.get('learning_rate', 3e-4),\n",
    "                p_layers=config.get('qaoa_layers', 4),\n",
    "                gamma_init=config.get('gamma_init', 0.1),\n",
    "                beta_init=config.get('beta_init', 0.001)\n",
    "            )\n",
    "            for model in ensemble.models\n",
    "        ]\n",
    "        \n",
    "        self.training_history = defaultdict(list)\n",
    "    \n",
    "    def compute_loss(self, outputs: Dict, obs_seq: torch.Tensor, reward_seq: torch.Tensor) -> Tuple[torch.Tensor, Dict]:\n",
    "        \"\"\"Compute world model loss.\"\"\"\n",
    "        # Reconstruction loss\n",
    "        recon_loss = F.mse_loss(outputs['obs_mean'], obs_seq)\n",
    "        \n",
    "        # KL divergence\n",
    "        prior_dist = torch.distributions.Normal(outputs['prior_mean'], outputs['prior_std'])\n",
    "        post_dist = torch.distributions.Normal(outputs['post_mean'], outputs['post_std'])\n",
    "        kl_loss = torch.distributions.kl_divergence(post_dist, prior_dist).mean()\n",
    "        kl_loss = torch.clamp(kl_loss, min=1.0)  # Free bits\n",
    "        \n",
    "        # Reward loss\n",
    "        reward_loss = F.mse_loss(outputs['reward'], reward_seq)\n",
    "        \n",
    "        # Total loss\n",
    "        total_loss = recon_loss + 0.1 * kl_loss + reward_loss\n",
    "        \n",
    "        metrics = {\n",
    "            'recon_loss': recon_loss.item(),\n",
    "            'kl_loss': kl_loss.item(),\n",
    "            'reward_loss': reward_loss.item(),\n",
    "            'total_loss': total_loss.item(),\n",
    "        }\n",
    "        \n",
    "        return total_loss, metrics\n",
    "    \n",
    "    def train_step(self, use_superposition: bool = True) -> Dict:\n",
    "        \"\"\"Single training step with all quantum components.\"\"\"\n",
    "        self.ensemble.train()\n",
    "        \n",
    "        # Sample using superposition replay\n",
    "        if use_superposition:\n",
    "            obs_seq, action_seq, reward_seq = self.buffer.sample_superposition(\n",
    "                self.config['batch_size'], self.config['seq_len']\n",
    "            )\n",
    "        else:\n",
    "            obs_seq, action_seq, reward_seq = self.buffer.sample_standard(\n",
    "                self.config['batch_size'], self.config['seq_len']\n",
    "            )\n",
    "        \n",
    "        total_loss = 0.0\n",
    "        all_metrics = defaultdict(list)\n",
    "        \n",
    "        # Train each model in ensemble with QAOA optimizer\n",
    "        for model, optimizer in zip(self.ensemble.models, self.optimizers):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(obs_seq, action_seq)\n",
    "            loss, metrics = self.compute_loss(outputs, obs_seq, reward_seq)\n",
    "            \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 100.0)\n",
    "            optimizer.step(loss)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            for k, v in metrics.items():\n",
    "                all_metrics[k].append(v)\n",
    "        \n",
    "        # Average metrics\n",
    "        avg_metrics = {k: np.mean(v) for k, v in all_metrics.items()}\n",
    "        avg_metrics['ensemble_loss'] = total_loss / len(self.ensemble.models)\n",
    "        \n",
    "        return avg_metrics\n",
    "    \n",
    "    def train(self, num_epochs: int, use_superposition: bool = True) -> Dict:\n",
    "        \"\"\"Full training loop.\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            epoch_metrics = defaultdict(list)\n",
    "            \n",
    "            # Multiple steps per epoch\n",
    "            num_steps = max(1, len(self.buffer) // self.config['batch_size'])\n",
    "            for _ in range(num_steps):\n",
    "                metrics = self.train_step(use_superposition)\n",
    "                for k, v in metrics.items():\n",
    "                    epoch_metrics[k].append(v)\n",
    "            \n",
    "            # Average epoch metrics\n",
    "            for k, v in epoch_metrics.items():\n",
    "                self.training_history[k].append(np.mean(v))\n",
    "            \n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                print(f\"Epoch {epoch+1}/{num_epochs}: Loss = {self.training_history['total_loss'][-1]:.4f}\")\n",
    "        \n",
    "        training_time = time.time() - start_time\n",
    "        self.training_history['training_time'] = training_time\n",
    "        \n",
    "        return dict(self.training_history)\n",
    "    \n",
    "    def evaluate(self, test_obs: torch.Tensor, test_actions: torch.Tensor) -> Dict:\n",
    "        \"\"\"Evaluate the ensemble.\"\"\"\n",
    "        self.ensemble.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = self.ensemble(test_obs, test_actions, return_all=True)\n",
    "            \n",
    "            # Ensemble prediction error\n",
    "            ensemble_mse = F.mse_loss(outputs['obs_mean'], test_obs).item()\n",
    "            \n",
    "            # Individual model errors\n",
    "            individual_mses = []\n",
    "            for out in outputs['all_outputs']:\n",
    "                mse = F.mse_loss(out['obs_mean'], test_obs).item()\n",
    "                individual_mses.append(mse)\n",
    "            \n",
    "            # Uncertainty\n",
    "            mean_uncertainty = outputs['uncertainty'].mean().item()\n",
    "        \n",
    "        return {\n",
    "            'ensemble_mse': ensemble_mse,\n",
    "            'avg_individual_mse': np.mean(individual_mses),\n",
    "            'best_individual_mse': min(individual_mses),\n",
    "            'worst_individual_mse': max(individual_mses),\n",
    "            'mean_uncertainty': mean_uncertainty,\n",
    "        }\n",
    "\n",
    "print('FullyIntegratedTrainer defined.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6B.8 Long-Horizon Prediction Test\n",
    "\n",
    "Test multi-step prediction accuracy (critical for world models used in planning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_long_horizon(model, obs_seq: torch.Tensor, action_seq: torch.Tensor, \n",
    "                          horizons: List[int] = [1, 5, 10, 20]) -> Dict:\n",
    "    \"\"\"\n",
    "    Evaluate prediction accuracy at different horizons.\n",
    "    \n",
    "    This is critical because world models are used for PLANNING,\n",
    "    which requires accurate predictions many steps into the future.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    results = {}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Get model predictions\n",
    "        if hasattr(model, 'models'):  # Ensemble\n",
    "            outputs = model(obs_seq, action_seq)\n",
    "            pred_obs = outputs['obs_mean']\n",
    "        else:  # Single model\n",
    "            outputs = model(obs_seq, action_seq)\n",
    "            pred_obs = outputs['obs_mean']\n",
    "        \n",
    "        # Calculate error at each horizon\n",
    "        for h in horizons:\n",
    "            if h <= obs_seq.shape[1]:\n",
    "                # MSE at horizon h\n",
    "                mse_at_h = F.mse_loss(pred_obs[:, h-1:h], obs_seq[:, h-1:h]).item()\n",
    "                results[f'mse_horizon_{h}'] = mse_at_h\n",
    "        \n",
    "        # Average error over all horizons\n",
    "        results['avg_horizon_mse'] = np.mean([v for k, v in results.items() if 'mse_horizon' in k])\n",
    "        \n",
    "        # Error growth rate (how fast does error compound?)\n",
    "        if len(horizons) >= 2:\n",
    "            first_h = horizons[0]\n",
    "            last_h = min(horizons[-1], obs_seq.shape[1])\n",
    "            if f'mse_horizon_{first_h}' in results and f'mse_horizon_{last_h}' in results:\n",
    "                error_growth = results[f'mse_horizon_{last_h}'] / (results[f'mse_horizon_{first_h}'] + 1e-8)\n",
    "                results['error_growth_rate'] = error_growth\n",
    "    \n",
    "    return results\n",
    "\n",
    "print('Long-horizon evaluation function defined.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6B.9 Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_episodes(env_name: str, num_episodes: int, seed: int = 42) -> List[Episode]:\n",
    "    \"\"\"Collect episodes from environment.\"\"\"\n",
    "    env = gym.make(env_name)\n",
    "    episodes = []\n",
    "    \n",
    "    for ep in range(num_episodes):\n",
    "        obs, _ = env.reset(seed=seed + ep)\n",
    "        observations, actions, rewards = [obs], [], []\n",
    "        \n",
    "        done = False\n",
    "        while not done:\n",
    "            action = env.action_space.sample()\n",
    "            next_obs, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            # One-hot encode action for discrete spaces\n",
    "            if isinstance(env.action_space, gym.spaces.Discrete):\n",
    "                action_onehot = np.zeros(env.action_space.n)\n",
    "                action_onehot[action] = 1\n",
    "                actions.append(action_onehot)\n",
    "            else:\n",
    "                actions.append(action)\n",
    "            \n",
    "            rewards.append(reward)\n",
    "            observations.append(next_obs)\n",
    "        \n",
    "        episodes.append(Episode(\n",
    "            observations=np.array(observations[:-1]),\n",
    "            actions=np.array(actions),\n",
    "            rewards=np.array(rewards),\n",
    "            total_reward=sum(rewards)\n",
    "        ))\n",
    "    \n",
    "    env.close()\n",
    "    avg_reward = np.mean([ep.total_reward for ep in episodes])\n",
    "    avg_length = np.mean([len(ep.observations) for ep in episodes])\n",
    "    print(f'Collected {num_episodes} episodes, avg reward: {avg_reward:.1f}, avg length: {avg_length:.1f}')\n",
    "    return episodes\n",
    "\n",
    "# Collect data\n",
    "print('Collecting training data...')\n",
    "episodes = collect_episodes('CartPole-v1', CONFIG['num_episodes'], seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6B.10 Classical Baseline for Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassicalWorldModel(nn.Module):\n",
    "    \"\"\"Standard world model without quantum enhancements (for comparison).\"\"\"\n",
    "    \n",
    "    def __init__(self, obs_dim: int, action_dim: int, config: Dict):\n",
    "        super().__init__()\n",
    "        self.obs_dim = obs_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.stoch_dim = config.get('stoch_dim', 64)\n",
    "        self.deter_dim = config.get('deter_dim', 512)\n",
    "        self.hidden_dim = config.get('hidden_dim', 512)\n",
    "        self.state_dim = self.stoch_dim + self.deter_dim\n",
    "        \n",
    "        # Standard encoder (no gates)\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(obs_dim, self.hidden_dim),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(self.hidden_dim, self.hidden_dim // 2),\n",
    "            nn.ELU(),\n",
    "        )\n",
    "        \n",
    "        # Standard RSSM\n",
    "        self.gru = nn.GRUCell(self.stoch_dim + action_dim, self.deter_dim)\n",
    "        \n",
    "        self.prior = nn.Sequential(\n",
    "            nn.Linear(self.deter_dim, self.hidden_dim // 2),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(self.hidden_dim // 2, self.stoch_dim * 2)\n",
    "        )\n",
    "        \n",
    "        self.posterior = nn.Sequential(\n",
    "            nn.Linear(self.deter_dim + self.hidden_dim // 2, self.hidden_dim // 2),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(self.hidden_dim // 2, self.stoch_dim * 2)\n",
    "        )\n",
    "        \n",
    "        # Standard decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(self.state_dim, self.hidden_dim),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(self.hidden_dim, obs_dim * 2)\n",
    "        )\n",
    "        \n",
    "        self.reward_pred = nn.Sequential(\n",
    "            nn.Linear(self.state_dim, self.hidden_dim // 2),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(self.hidden_dim // 2, 1)\n",
    "        )\n",
    "    \n",
    "    def initial_state(self, batch_size: int):\n",
    "        return {\n",
    "            'deter': torch.zeros(batch_size, self.deter_dim, device=DEVICE),\n",
    "            'stoch': torch.zeros(batch_size, self.stoch_dim, device=DEVICE)\n",
    "        }\n",
    "    \n",
    "    def get_dist(self, stats):\n",
    "        mean, log_std = stats.chunk(2, dim=-1)\n",
    "        std = F.softplus(log_std) + 0.1\n",
    "        return torch.distributions.Normal(mean, std)\n",
    "    \n",
    "    def forward(self, obs_seq, action_seq):\n",
    "        batch_size, seq_len = obs_seq.shape[:2]\n",
    "        state = self.initial_state(batch_size)\n",
    "        \n",
    "        prior_means, prior_stds = [], []\n",
    "        post_means, post_stds = [], []\n",
    "        obs_means, obs_stds = [], []\n",
    "        rewards = []\n",
    "        \n",
    "        for t in range(seq_len):\n",
    "            embed = self.encoder(obs_seq[:, t])\n",
    "            \n",
    "            prior_stats = self.prior(state['deter'])\n",
    "            prior_dist = self.get_dist(prior_stats)\n",
    "            \n",
    "            post_input = torch.cat([state['deter'], embed], dim=-1)\n",
    "            post_stats = self.posterior(post_input)\n",
    "            post_dist = self.get_dist(post_stats)\n",
    "            \n",
    "            stoch = post_dist.rsample()\n",
    "            gru_input = torch.cat([stoch, action_seq[:, t]], dim=-1)\n",
    "            deter = self.gru(gru_input, state['deter'])\n",
    "            full_state = torch.cat([deter, stoch], dim=-1)\n",
    "            \n",
    "            obs_stats = self.decoder(full_state)\n",
    "            obs_dist = self.get_dist(obs_stats)\n",
    "            reward = self.reward_pred(full_state)\n",
    "            \n",
    "            prior_means.append(prior_dist.mean)\n",
    "            prior_stds.append(prior_dist.stddev)\n",
    "            post_means.append(post_dist.mean)\n",
    "            post_stds.append(post_dist.stddev)\n",
    "            obs_means.append(obs_dist.mean)\n",
    "            obs_stds.append(obs_dist.stddev)\n",
    "            rewards.append(reward)\n",
    "            \n",
    "            state = {'deter': deter, 'stoch': stoch}\n",
    "        \n",
    "        return {\n",
    "            'prior_mean': torch.stack(prior_means, dim=1),\n",
    "            'prior_std': torch.stack(prior_stds, dim=1),\n",
    "            'post_mean': torch.stack(post_means, dim=1),\n",
    "            'post_std': torch.stack(post_stds, dim=1),\n",
    "            'obs_mean': torch.stack(obs_means, dim=1),\n",
    "            'obs_std': torch.stack(obs_stds, dim=1),\n",
    "            'reward': torch.stack(rewards, dim=1).squeeze(-1),\n",
    "        }\n",
    "\n",
    "print('ClassicalWorldModel defined.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6B.11 Multi-Seed Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(seed: int, config: Dict) -> Dict:\n",
    "    \"\"\"Run single experiment with given seed.\"\"\"\n",
    "    print(f'\\n{\"=\"*60}')\n",
    "    print(f'Running experiment with seed {seed}')\n",
    "    print(f'{\"=\"*60}')\n",
    "    \n",
    "    # Set seeds\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Collect data\n",
    "    episodes = collect_episodes('CartPole-v1', config['num_episodes'], seed=seed)\n",
    "    \n",
    "    # Prepare buffer\n",
    "    buffer = SuperpositionReplayBuffer(capacity=1000, parallel_samples=4)\n",
    "    for ep in episodes:\n",
    "        buffer.add_episode(ep)\n",
    "    \n",
    "    # Prepare test data\n",
    "    test_episodes = episodes[-10:]\n",
    "    test_obs = torch.FloatTensor(np.stack([ep.observations[:config['seq_len']] for ep in test_episodes])).to(DEVICE)\n",
    "    test_actions = torch.FloatTensor(np.stack([ep.actions[:config['seq_len']] for ep in test_episodes])).to(DEVICE)\n",
    "    test_rewards = torch.FloatTensor(np.stack([ep.rewards[:config['seq_len']] for ep in test_episodes])).to(DEVICE)\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # ============================================================\n",
    "    # Train Classical Baseline\n",
    "    # ============================================================\n",
    "    print('\\nTraining Classical Baseline...')\n",
    "    classical_model = ClassicalWorldModel(config['obs_dim'], config['action_dim'], config).to(DEVICE)\n",
    "    classical_optimizer = torch.optim.Adam(classical_model.parameters(), lr=config['learning_rate'])\n",
    "    \n",
    "    classical_start = time.time()\n",
    "    classical_losses = []\n",
    "    \n",
    "    for epoch in range(config['num_epochs']):\n",
    "        classical_model.train()\n",
    "        obs, act, rew = buffer.sample_standard(config['batch_size'], config['seq_len'])\n",
    "        \n",
    "        classical_optimizer.zero_grad()\n",
    "        outputs = classical_model(obs, act)\n",
    "        \n",
    "        recon_loss = F.mse_loss(outputs['obs_mean'], obs)\n",
    "        prior_dist = torch.distributions.Normal(outputs['prior_mean'], outputs['prior_std'])\n",
    "        post_dist = torch.distributions.Normal(outputs['post_mean'], outputs['post_std'])\n",
    "        kl_loss = torch.clamp(torch.distributions.kl_divergence(post_dist, prior_dist).mean(), min=1.0)\n",
    "        reward_loss = F.mse_loss(outputs['reward'], rew)\n",
    "        loss = recon_loss + 0.1 * kl_loss + reward_loss\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(classical_model.parameters(), 100.0)\n",
    "        classical_optimizer.step()\n",
    "        \n",
    "        classical_losses.append(loss.item())\n",
    "        \n",
    "        if (epoch + 1) % 25 == 0:\n",
    "            print(f'  Epoch {epoch+1}: Loss = {loss.item():.4f}')\n",
    "    \n",
    "    classical_time = time.time() - classical_start\n",
    "    \n",
    "    # Evaluate classical\n",
    "    classical_model.eval()\n",
    "    with torch.no_grad():\n",
    "        classical_outputs = classical_model(test_obs, test_actions)\n",
    "        classical_mse = F.mse_loss(classical_outputs['obs_mean'], test_obs).item()\n",
    "    \n",
    "    classical_horizon = evaluate_long_horizon(classical_model, test_obs, test_actions)\n",
    "    \n",
    "    results['classical'] = {\n",
    "        'final_loss': classical_losses[-1],\n",
    "        'test_mse': classical_mse,\n",
    "        'training_time': classical_time,\n",
    "        'long_horizon': classical_horizon,\n",
    "        'params': sum(p.numel() for p in classical_model.parameters()),\n",
    "    }\n",
    "    \n",
    "    # ============================================================\n",
    "    # Train Fully Integrated\n",
    "    # ============================================================\n",
    "    print('\\nTraining Fully Integrated (QAOA + Superposition + Gates + Ensemble)...')\n",
    "    \n",
    "    integrated_ensemble = ErrorCorrectionEnsemble(\n",
    "        config['obs_dim'], config['action_dim'], config, num_models=config['num_ensemble']\n",
    "    ).to(DEVICE)\n",
    "    \n",
    "    integrated_trainer = FullyIntegratedTrainer(integrated_ensemble, buffer, config)\n",
    "    \n",
    "    integrated_start = time.time()\n",
    "    integrated_history = integrated_trainer.train(config['num_epochs'], use_superposition=True)\n",
    "    integrated_time = time.time() - integrated_start\n",
    "    \n",
    "    # Evaluate integrated\n",
    "    integrated_eval = integrated_trainer.evaluate(test_obs, test_actions)\n",
    "    integrated_horizon = evaluate_long_horizon(integrated_ensemble, test_obs, test_actions)\n",
    "    \n",
    "    results['integrated'] = {\n",
    "        'final_loss': integrated_history['total_loss'][-1],\n",
    "        'test_mse': integrated_eval['ensemble_mse'],\n",
    "        'training_time': integrated_time,\n",
    "        'long_horizon': integrated_horizon,\n",
    "        'uncertainty': integrated_eval['mean_uncertainty'],\n",
    "        'params': sum(p.numel() for p in integrated_ensemble.parameters()),\n",
    "    }\n",
    "    \n",
    "    print(f'\\nSeed {seed} Results:')\n",
    "    print(f'  Classical - Loss: {results[\"classical\"][\"final_loss\"]:.4f}, MSE: {results[\"classical\"][\"test_mse\"]:.6f}')\n",
    "    print(f'  Integrated - Loss: {results[\"integrated\"][\"final_loss\"]:.4f}, MSE: {results[\"integrated\"][\"test_mse\"]:.6f}')\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run experiments\n",
    "all_results = []\n",
    "for seed in CONFIG['seeds']:\n",
    "    result = run_experiment(seed, CONFIG)\n",
    "    all_results.append(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6B.12 Statistical Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('='*70)\n",
    "print('STATISTICAL ANALYSIS')\n",
    "print('='*70)\n",
    "\n",
    "# Extract metrics\n",
    "classical_losses = [r['classical']['final_loss'] for r in all_results]\n",
    "integrated_losses = [r['integrated']['final_loss'] for r in all_results]\n",
    "classical_mses = [r['classical']['test_mse'] for r in all_results]\n",
    "integrated_mses = [r['integrated']['test_mse'] for r in all_results]\n",
    "classical_times = [r['classical']['training_time'] for r in all_results]\n",
    "integrated_times = [r['integrated']['training_time'] for r in all_results]\n",
    "\n",
    "# Long-horizon metrics\n",
    "classical_horizon_growth = [r['classical']['long_horizon'].get('error_growth_rate', 1.0) for r in all_results]\n",
    "integrated_horizon_growth = [r['integrated']['long_horizon'].get('error_growth_rate', 1.0) for r in all_results]\n",
    "\n",
    "print('\\n' + '-'*70)\n",
    "print('1. FINAL LOSS COMPARISON')\n",
    "print('-'*70)\n",
    "print(f'Classical:  {np.mean(classical_losses):.4f} +/- {np.std(classical_losses):.4f}')\n",
    "print(f'Integrated: {np.mean(integrated_losses):.4f} +/- {np.std(integrated_losses):.4f}')\n",
    "\n",
    "# Mann-Whitney U test\n",
    "stat, p_loss = stats.mannwhitneyu(classical_losses, integrated_losses, alternative='two-sided')\n",
    "print(f'Mann-Whitney U: p={p_loss:.6f}')\n",
    "\n",
    "# Cohen's d\n",
    "pooled_std = np.sqrt((np.std(classical_losses)**2 + np.std(integrated_losses)**2) / 2)\n",
    "cohens_d = (np.mean(classical_losses) - np.mean(integrated_losses)) / pooled_std\n",
    "print(f\"Cohen's d: {cohens_d:.4f}\")\n",
    "\n",
    "print('\\n' + '-'*70)\n",
    "print('2. TEST MSE COMPARISON')\n",
    "print('-'*70)\n",
    "print(f'Classical:  {np.mean(classical_mses):.6f} +/- {np.std(classical_mses):.6f}')\n",
    "print(f'Integrated: {np.mean(integrated_mses):.6f} +/- {np.std(integrated_mses):.6f}')\n",
    "\n",
    "stat, p_mse = stats.mannwhitneyu(classical_mses, integrated_mses, alternative='two-sided')\n",
    "print(f'Mann-Whitney U: p={p_mse:.6f}')\n",
    "\n",
    "improvement = (np.mean(classical_mses) - np.mean(integrated_mses)) / np.mean(classical_mses) * 100\n",
    "print(f'Improvement: {improvement:.1f}%')\n",
    "\n",
    "print('\\n' + '-'*70)\n",
    "print('3. LONG-HORIZON PREDICTION (Error Growth Rate)')\n",
    "print('-'*70)\n",
    "print(f'Classical error growth:  {np.mean(classical_horizon_growth):.4f} +/- {np.std(classical_horizon_growth):.4f}')\n",
    "print(f'Integrated error growth: {np.mean(integrated_horizon_growth):.4f} +/- {np.std(integrated_horizon_growth):.4f}')\n",
    "print('(Lower is better - errors compound slower)')\n",
    "\n",
    "print('\\n' + '-'*70)\n",
    "print('4. TRAINING TIME')\n",
    "print('-'*70)\n",
    "print(f'Classical:  {np.mean(classical_times):.2f}s +/- {np.std(classical_times):.2f}s')\n",
    "print(f'Integrated: {np.mean(integrated_times):.2f}s +/- {np.std(integrated_times):.2f}s')\n",
    "\n",
    "print('\\n' + '-'*70)\n",
    "print('5. COMPUTATIONAL COST')\n",
    "print('-'*70)\n",
    "print(f'Classical params:  {all_results[0][\"classical\"][\"params\"]:,}')\n",
    "print(f'Integrated params: {all_results[0][\"integrated\"][\"params\"]:,}')\n",
    "print(f'Ratio: {all_results[0][\"integrated\"][\"params\"] / all_results[0][\"classical\"][\"params\"]:.1f}x')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6B.13 Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "results_dir = Path('../experiments/results/fully_integrated')\n",
    "results_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Prepare results dict\n",
    "save_results = {\n",
    "    'metadata': {\n",
    "        'experiment': 'Fully Integrated Quantum-Enhanced World Model',\n",
    "        'seeds': CONFIG['seeds'],\n",
    "        'num_epochs': CONFIG['num_epochs'],\n",
    "        'environment': 'CartPole-v1',\n",
    "    },\n",
    "    'classical': {\n",
    "        'final_losses': classical_losses,\n",
    "        'test_mses': classical_mses,\n",
    "        'training_times': classical_times,\n",
    "        'mean_loss': float(np.mean(classical_losses)),\n",
    "        'std_loss': float(np.std(classical_losses)),\n",
    "        'mean_mse': float(np.mean(classical_mses)),\n",
    "        'std_mse': float(np.std(classical_mses)),\n",
    "    },\n",
    "    'integrated': {\n",
    "        'final_losses': integrated_losses,\n",
    "        'test_mses': integrated_mses,\n",
    "        'training_times': integrated_times,\n",
    "        'mean_loss': float(np.mean(integrated_losses)),\n",
    "        'std_loss': float(np.std(integrated_losses)),\n",
    "        'mean_mse': float(np.mean(integrated_mses)),\n",
    "        'std_mse': float(np.std(integrated_mses)),\n",
    "    },\n",
    "    'statistical_tests': {\n",
    "        'loss_p_value': float(p_loss),\n",
    "        'mse_p_value': float(p_mse),\n",
    "        'cohens_d': float(cohens_d),\n",
    "        'mse_improvement_percent': float(improvement),\n",
    "    },\n",
    "    'long_horizon': {\n",
    "        'classical_error_growth': classical_horizon_growth,\n",
    "        'integrated_error_growth': integrated_horizon_growth,\n",
    "    },\n",
    "}\n",
    "\n",
    "with open(results_dir / 'complete_metrics.json', 'w') as f:\n",
    "    json.dump(save_results, f, indent=2)\n",
    "\n",
    "print(f'Results saved to {results_dir / \"complete_metrics.json\"}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6B.14 Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 1. Final Loss Comparison\n",
    "ax = axes[0, 0]\n",
    "x = np.arange(len(CONFIG['seeds']))\n",
    "width = 0.35\n",
    "ax.bar(x - width/2, classical_losses, width, label='Classical', color='steelblue', alpha=0.8)\n",
    "ax.bar(x + width/2, integrated_losses, width, label='Integrated', color='darkorange', alpha=0.8)\n",
    "ax.set_xlabel('Seed')\n",
    "ax.set_ylabel('Final Loss')\n",
    "ax.set_title('Final Loss by Seed')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(CONFIG['seeds'])\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Test MSE Comparison\n",
    "ax = axes[0, 1]\n",
    "ax.bar(x - width/2, classical_mses, width, label='Classical', color='steelblue', alpha=0.8)\n",
    "ax.bar(x + width/2, integrated_mses, width, label='Integrated', color='darkorange', alpha=0.8)\n",
    "ax.set_xlabel('Seed')\n",
    "ax.set_ylabel('Test MSE')\n",
    "ax.set_title('Test MSE by Seed')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(CONFIG['seeds'])\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Summary Statistics\n",
    "ax = axes[1, 0]\n",
    "metrics = ['Loss', 'MSE', 'Time (s)']\n",
    "classical_means = [np.mean(classical_losses), np.mean(classical_mses)*100, np.mean(classical_times)]\n",
    "integrated_means = [np.mean(integrated_losses), np.mean(integrated_mses)*100, np.mean(integrated_times)]\n",
    "x = np.arange(len(metrics))\n",
    "ax.bar(x - width/2, classical_means, width, label='Classical', color='steelblue', alpha=0.8)\n",
    "ax.bar(x + width/2, integrated_means, width, label='Integrated', color='darkorange', alpha=0.8)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(metrics)\n",
    "ax.set_title('Summary Comparison')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Long-Horizon Error Growth\n",
    "ax = axes[1, 1]\n",
    "ax.bar(x[:len(classical_horizon_growth)] - width/2, classical_horizon_growth, width, \n",
    "       label='Classical', color='steelblue', alpha=0.8)\n",
    "ax.bar(x[:len(integrated_horizon_growth)] + width/2, integrated_horizon_growth, width, \n",
    "       label='Integrated', color='darkorange', alpha=0.8)\n",
    "ax.set_xlabel('Seed Index')\n",
    "ax.set_ylabel('Error Growth Rate')\n",
    "ax.set_title('Long-Horizon Error Growth (Lower is Better)')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(results_dir / 'comparison_plots.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f'Figure saved to {results_dir / \"comparison_plots.png\"}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6B.15 Summary\n",
    "\n",
    "### What This Notebook Combines\n",
    "\n",
    "| Component | From | Solves |\n",
    "|-----------|------|--------|\n",
    "| QAOA Optimizer | Notebook 03 | Local minima |\n",
    "| Superposition Replay | Notebook 04 | Sample inefficiency |\n",
    "| Gate-Enhanced Layers | Notebook 05 | Slow convergence |\n",
    "| Error Correction Ensemble | Notebook 06 | Compounding errors |\n",
    "\n",
    "### Key Metrics Measured\n",
    "\n",
    "1. **Training Efficiency**: Wall-clock time\n",
    "2. **Sample Efficiency**: Final loss with same data\n",
    "3. **Final Performance**: Test MSE\n",
    "4. **Long-Horizon Prediction**: Error growth rate\n",
    "5. **Computational Cost**: Parameter count\n",
    "6. **Statistical Significance**: Mann-Whitney U, Cohen's d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('='*70)\n",
    "print('Phase 6B: Fully Integrated - COMPLETE')\n",
    "print('='*70)\n",
    "print()\n",
    "print('Components integrated:')\n",
    "print('  - QAOA Optimizer (escapes local minima)')\n",
    "print('  - Superposition Replay (sample efficiency)')\n",
    "print('  - Gate-Enhanced Layers (better representations)')\n",
    "print('  - Error Correction Ensemble (robust predictions)')\n",
    "print()\n",
    "print('Metrics measured:')\n",
    "print('  - Training efficiency (time)')\n",
    "print('  - Sample efficiency (loss)')\n",
    "print('  - Final performance (MSE)')\n",
    "print('  - Long-horizon prediction (error growth)')\n",
    "print('  - Computational cost (params)')\n",
    "print('  - Statistical significance (p-values)')\n",
    "print()\n",
    "print('Ready for comparison with individual approaches!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
