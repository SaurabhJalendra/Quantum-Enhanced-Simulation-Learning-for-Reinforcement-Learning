{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 7: Comprehensive Comparison\n",
    "\n",
    "**Quantum-Enhanced Simulation Learning for Reinforcement Learning**\n",
    "\n",
    "Author: Saurabh Jalendra  \n",
    "Institution: BITS Pilani (WILP Division)  \n",
    "Date: November 2025\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook provides a **comprehensive, statistically rigorous comparison** of all\n",
    "implemented quantum-inspired world model training approaches:\n",
    "\n",
    "### Approaches Compared\n",
    "\n",
    "1. **Classical Baseline**: Standard DreamerV3-style world model\n",
    "2. **QAOA-Enhanced**: Quantum approximate optimization-inspired training\n",
    "3. **Superposition Replay**: Quantum superposition-inspired experience replay\n",
    "4. **Gate-Enhanced**: Quantum gate-inspired neural network layers\n",
    "5. **Error Correction**: Quantum error correction-inspired ensemble\n",
    "\n",
    "### Evaluation Metrics\n",
    "\n",
    "- **Sample Efficiency**: Steps to reach target performance\n",
    "- **Training Speed**: Wall-clock time to convergence\n",
    "- **Prediction Accuracy**: MSE on test trajectories\n",
    "- **Robustness**: Performance under noise\n",
    "\n",
    "### Statistical Analysis\n",
    "\n",
    "- Multiple seeds (5 per configuration)\n",
    "- Mann-Whitney U tests\n",
    "- Cohen's d effect size\n",
    "- 95% confidence intervals\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path.cwd().parent\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "import math\n",
    "import time\n",
    "from typing import Dict, List, Tuple, Optional, Callable\n",
    "from dataclasses import dataclass, field\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import gymnasium as gym\n",
    "\n",
    "from src.utils import set_seed, get_device, MetricLogger, Timer, COLORS\n",
    "\n",
    "# Set seed for reproducibility\n",
    "set_seed(42)\n",
    "device = get_device()\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Plotting style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('husl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 Model Implementations\n",
    "\n",
    "Import all model implementations from previous phases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base World Model (shared architecture for fair comparison)\n",
    "class BaseWorldModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Base world model architecture used by all approaches.\n",
    "    \n",
    "    This ensures fair comparison by using identical architectures,\n",
    "    only varying the training procedure or enhancements.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        obs_dim: int,\n",
    "        action_dim: int,\n",
    "        hidden_dim: int = 256,\n",
    "        deter_dim: int = 128,\n",
    "        stoch_dim: int = 32\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.obs_dim = obs_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.deter_dim = deter_dim\n",
    "        self.stoch_dim = stoch_dim\n",
    "        self.state_dim = deter_dim + stoch_dim\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(obs_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ELU()\n",
    "        )\n",
    "        \n",
    "        # RSSM\n",
    "        self.gru = nn.GRUCell(stoch_dim + action_dim, deter_dim)\n",
    "        \n",
    "        self.prior = nn.Sequential(\n",
    "            nn.Linear(deter_dim, hidden_dim // 2),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(hidden_dim // 2, stoch_dim * 2)\n",
    "        )\n",
    "        \n",
    "        self.posterior = nn.Sequential(\n",
    "            nn.Linear(deter_dim + hidden_dim // 2, hidden_dim // 2),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(hidden_dim // 2, stoch_dim * 2)\n",
    "        )\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(self.state_dim, hidden_dim),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(hidden_dim, obs_dim * 2)\n",
    "        )\n",
    "        \n",
    "        # Reward predictor\n",
    "        self.reward_pred = nn.Sequential(\n",
    "            nn.Linear(self.state_dim, hidden_dim // 2),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(hidden_dim // 2, 1)\n",
    "        )\n",
    "    \n",
    "    def initial_state(self, batch_size: int, device: torch.device) -> Dict[str, Tensor]:\n",
    "        return {\n",
    "            'deter': torch.zeros(batch_size, self.deter_dim, device=device),\n",
    "            'stoch': torch.zeros(batch_size, self.stoch_dim, device=device)\n",
    "        }\n",
    "    \n",
    "    def forward(self, obs_seq: Tensor, action_seq: Tensor) -> Dict[str, Tensor]:\n",
    "        batch_size, seq_len = obs_seq.shape[:2]\n",
    "        device = obs_seq.device\n",
    "        \n",
    "        state = self.initial_state(batch_size, device)\n",
    "        \n",
    "        states, prior_means, prior_stds = [], [], []\n",
    "        post_means, post_stds = [], []\n",
    "        \n",
    "        for t in range(seq_len):\n",
    "            embed = self.encoder(obs_seq[:, t])\n",
    "            gru_input = torch.cat([state['stoch'], action_seq[:, t]], dim=-1)\n",
    "            deter = self.gru(gru_input, state['deter'])\n",
    "            \n",
    "            # Prior\n",
    "            prior_stats = self.prior(deter)\n",
    "            prior_mean, prior_log_std = torch.chunk(prior_stats, 2, dim=-1)\n",
    "            prior_std = F.softplus(prior_log_std) + 0.1\n",
    "            \n",
    "            # Posterior\n",
    "            post_input = torch.cat([deter, embed], dim=-1)\n",
    "            post_stats = self.posterior(post_input)\n",
    "            post_mean, post_log_std = torch.chunk(post_stats, 2, dim=-1)\n",
    "            post_std = F.softplus(post_log_std) + 0.1\n",
    "            \n",
    "            stoch = post_mean + post_std * torch.randn_like(post_std)\n",
    "            state = {'deter': deter, 'stoch': stoch}\n",
    "            \n",
    "            full_state = torch.cat([deter, stoch], dim=-1)\n",
    "            states.append(full_state)\n",
    "            prior_means.append(prior_mean)\n",
    "            prior_stds.append(prior_std)\n",
    "            post_means.append(post_mean)\n",
    "            post_stds.append(post_std)\n",
    "        \n",
    "        states = torch.stack(states, dim=1)\n",
    "        flat_states = states.reshape(-1, self.state_dim)\n",
    "        \n",
    "        dec_output = self.decoder(flat_states)\n",
    "        obs_mean, obs_log_std = torch.chunk(dec_output, 2, dim=-1)\n",
    "        obs_mean = obs_mean.reshape(batch_size, seq_len, -1)\n",
    "        obs_log_std = obs_log_std.clamp(-10, 2).reshape(batch_size, seq_len, -1)\n",
    "        \n",
    "        reward_pred = self.reward_pred(flat_states).reshape(batch_size, seq_len)\n",
    "        \n",
    "        return {\n",
    "            'states': states,\n",
    "            'obs_mean': obs_mean,\n",
    "            'obs_log_std': obs_log_std,\n",
    "            'reward_pred': reward_pred,\n",
    "            'prior_mean': torch.stack(prior_means, dim=1),\n",
    "            'prior_std': torch.stack(prior_stds, dim=1),\n",
    "            'post_mean': torch.stack(post_means, dim=1),\n",
    "            'post_std': torch.stack(post_stds, dim=1)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training approaches\n",
    "\n",
    "class BaselineTrainer:\n",
    "    \"\"\"Standard baseline trainer with Adam optimizer.\"\"\"\n",
    "    \n",
    "    def __init__(self, model, lr=1e-4, kl_weight=1.0, free_nats=3.0):\n",
    "        self.model = model\n",
    "        self.kl_weight = kl_weight\n",
    "        self.free_nats = free_nats\n",
    "        self.optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "    \n",
    "    def compute_loss(self, obs, actions, rewards, outputs):\n",
    "        obs_dist = torch.distributions.Normal(\n",
    "            outputs['obs_mean'], torch.exp(outputs['obs_log_std'])\n",
    "        )\n",
    "        recon_loss = -obs_dist.log_prob(obs).mean()\n",
    "        \n",
    "        prior_dist = torch.distributions.Normal(outputs['prior_mean'], outputs['prior_std'])\n",
    "        post_dist = torch.distributions.Normal(outputs['post_mean'], outputs['post_std'])\n",
    "        kl_loss = torch.distributions.kl_divergence(post_dist, prior_dist).mean()\n",
    "        kl_loss = torch.maximum(kl_loss, torch.tensor(self.free_nats, device=kl_loss.device))\n",
    "        \n",
    "        reward_loss = F.mse_loss(outputs['reward_pred'], rewards)\n",
    "        \n",
    "        return recon_loss + self.kl_weight * kl_loss + reward_loss\n",
    "    \n",
    "    def train_step(self, obs, actions, rewards):\n",
    "        self.model.train()\n",
    "        self.optimizer.zero_grad()\n",
    "        \n",
    "        outputs = self.model(obs, actions)\n",
    "        loss = self.compute_loss(obs, actions, rewards, outputs)\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.model.parameters(), 100.0)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return {'loss': loss.item()}\n",
    "\n",
    "\n",
    "class QAOATrainer:\n",
    "    \"\"\"QAOA-inspired trainer with alternating cost/mixing operators.\"\"\"\n",
    "    \n",
    "    def __init__(self, model, lr=1e-4, gamma=0.5, beta=0.1, p=3):\n",
    "        self.model = model\n",
    "        self.gamma = gamma\n",
    "        self.beta = beta\n",
    "        self.p = p\n",
    "        self.optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "        self.step_count = 0\n",
    "    \n",
    "    def compute_loss(self, obs, actions, rewards, outputs):\n",
    "        obs_dist = torch.distributions.Normal(\n",
    "            outputs['obs_mean'], torch.exp(outputs['obs_log_std'])\n",
    "        )\n",
    "        recon_loss = -obs_dist.log_prob(obs).mean()\n",
    "        \n",
    "        prior_dist = torch.distributions.Normal(outputs['prior_mean'], outputs['prior_std'])\n",
    "        post_dist = torch.distributions.Normal(outputs['post_mean'], outputs['post_std'])\n",
    "        kl_loss = torch.distributions.kl_divergence(post_dist, prior_dist).mean()\n",
    "        kl_loss = torch.maximum(kl_loss, torch.tensor(3.0, device=kl_loss.device))\n",
    "        \n",
    "        reward_loss = F.mse_loss(outputs['reward_pred'], rewards)\n",
    "        \n",
    "        return recon_loss + kl_loss + reward_loss\n",
    "    \n",
    "    def train_step(self, obs, actions, rewards):\n",
    "        self.model.train()\n",
    "        self.step_count += 1\n",
    "        \n",
    "        # Alternate between cost and mixing phases\n",
    "        phase = self.step_count % (2 * self.p)\n",
    "        is_mixing = (phase % 2 == 1)\n",
    "        \n",
    "        # Dynamic gamma and beta scheduling\n",
    "        decay = 0.99 ** (self.step_count // 100)\n",
    "        gamma = self.gamma * decay\n",
    "        beta = self.beta * decay\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        outputs = self.model(obs, actions)\n",
    "        loss = self.compute_loss(obs, actions, rewards, outputs)\n",
    "        loss.backward()\n",
    "        \n",
    "        if is_mixing:\n",
    "            # Mixing operator: add exploration noise\n",
    "            with torch.no_grad():\n",
    "                for param in self.model.parameters():\n",
    "                    if param.grad is not None:\n",
    "                        noise = torch.randn_like(param) * beta\n",
    "                        param.grad.add_(noise)\n",
    "        else:\n",
    "            # Cost operator: scale gradients\n",
    "            with torch.no_grad():\n",
    "                for param in self.model.parameters():\n",
    "                    if param.grad is not None:\n",
    "                        param.grad.mul_(gamma)\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(self.model.parameters(), 100.0)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return {'loss': loss.item()}\n",
    "\n",
    "\n",
    "class SuperpositionReplayTrainer:\n",
    "    \"\"\"Trainer with superposition-inspired experience replay.\"\"\"\n",
    "    \n",
    "    def __init__(self, model, lr=1e-4, alpha=0.6, beta_start=0.4):\n",
    "        self.model = model\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta_start\n",
    "        self.optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "        self.priorities = None\n",
    "        self.step_count = 0\n",
    "    \n",
    "    def compute_amplitudes(self, td_errors: Tensor) -> Tensor:\n",
    "        \"\"\"Compute quantum-inspired amplitudes for prioritization.\"\"\"\n",
    "        priorities = (td_errors.abs() + 1e-6) ** self.alpha\n",
    "        amplitudes = priorities / priorities.sum()\n",
    "        return amplitudes\n",
    "    \n",
    "    def compute_loss(self, obs, actions, rewards, outputs, weights=None):\n",
    "        obs_dist = torch.distributions.Normal(\n",
    "            outputs['obs_mean'], torch.exp(outputs['obs_log_std'])\n",
    "        )\n",
    "        recon_loss = -obs_dist.log_prob(obs)\n",
    "        \n",
    "        prior_dist = torch.distributions.Normal(outputs['prior_mean'], outputs['prior_std'])\n",
    "        post_dist = torch.distributions.Normal(outputs['post_mean'], outputs['post_std'])\n",
    "        kl_loss = torch.distributions.kl_divergence(post_dist, prior_dist)\n",
    "        \n",
    "        reward_loss = (outputs['reward_pred'] - rewards) ** 2\n",
    "        \n",
    "        if weights is not None:\n",
    "            weights = weights.unsqueeze(-1)  # (batch, 1)\n",
    "            recon_loss = (recon_loss * weights.unsqueeze(-1)).mean()\n",
    "            kl_loss = (kl_loss * weights.unsqueeze(-1)).mean()\n",
    "            reward_loss = (reward_loss * weights).mean()\n",
    "        else:\n",
    "            recon_loss = recon_loss.mean()\n",
    "            kl_loss = kl_loss.mean()\n",
    "            reward_loss = reward_loss.mean()\n",
    "        \n",
    "        kl_loss = torch.maximum(kl_loss, torch.tensor(3.0, device=kl_loss.device))\n",
    "        \n",
    "        return recon_loss + kl_loss + reward_loss, reward_loss\n",
    "    \n",
    "    def train_step(self, obs, actions, rewards):\n",
    "        self.model.train()\n",
    "        self.step_count += 1\n",
    "        \n",
    "        # Anneal beta\n",
    "        self.beta = min(1.0, 0.4 + self.step_count * 0.001)\n",
    "        \n",
    "        # First pass: compute TD errors\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(obs, actions)\n",
    "            td_errors = (outputs['reward_pred'] - rewards).abs().mean(dim=-1)  # (batch,)\n",
    "            amplitudes = self.compute_amplitudes(td_errors)\n",
    "        \n",
    "        # Importance sampling weights\n",
    "        N = obs.shape[0]\n",
    "        weights = (N * amplitudes) ** (-self.beta)\n",
    "        weights = weights / weights.max()\n",
    "        \n",
    "        # Training pass with weighted loss\n",
    "        self.optimizer.zero_grad()\n",
    "        outputs = self.model(obs, actions)\n",
    "        loss, _ = self.compute_loss(obs, actions, rewards, outputs, weights)\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.model.parameters(), 100.0)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return {'loss': loss.item()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gate-Enhanced Model\n",
    "\n",
    "class QuantumGateLayer(nn.Module):\n",
    "    \"\"\"Combined quantum gate-inspired layer.\"\"\"\n",
    "    \n",
    "    def __init__(self, dim: int):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        \n",
    "        # Rotation angles\n",
    "        num_rotations = dim // 2\n",
    "        self.angles = nn.Parameter(torch.randn(num_rotations, 3) * 0.1)\n",
    "        indices = torch.randperm(dim)[:num_rotations * 2]\n",
    "        self.register_buffer('idx1', indices[:num_rotations])\n",
    "        self.register_buffer('idx2', indices[num_rotations:])\n",
    "        \n",
    "        # Phase\n",
    "        self.phases = nn.Parameter(torch.randn(dim) * 0.1)\n",
    "        self.amplitude = nn.Parameter(torch.ones(dim))\n",
    "        \n",
    "        # Layer norm\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "    \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        # Rotation\n",
    "        y = x.clone()\n",
    "        x1, x2 = x[:, self.idx1], x[:, self.idx2]\n",
    "        \n",
    "        for i in range(3):\n",
    "            cos_t = torch.cos(self.angles[:, i])\n",
    "            sin_t = torch.sin(self.angles[:, i])\n",
    "            x1_new = cos_t * x1 - sin_t * x2\n",
    "            x2_new = sin_t * x1 + cos_t * x2\n",
    "            x1, x2 = x1_new, x2_new\n",
    "        \n",
    "        y = y.scatter(1, self.idx1.unsqueeze(0).expand(x.shape[0], -1), x1)\n",
    "        y = y.scatter(1, self.idx2.unsqueeze(0).expand(x.shape[0], -1), x2)\n",
    "        \n",
    "        # Phase\n",
    "        cos_p = torch.cos(self.phases)\n",
    "        sin_p = torch.sin(self.phases)\n",
    "        y = self.amplitude * (y * cos_p + torch.abs(y) * sin_p)\n",
    "        \n",
    "        return self.norm(x + y)\n",
    "\n",
    "\n",
    "class GateEnhancedModel(nn.Module):\n",
    "    \"\"\"World model with quantum gate-enhanced layers.\"\"\"\n",
    "    \n",
    "    def __init__(self, obs_dim, action_dim, hidden_dim=256, deter_dim=128, stoch_dim=32):\n",
    "        super().__init__()\n",
    "        self.obs_dim = obs_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.deter_dim = deter_dim\n",
    "        self.stoch_dim = stoch_dim\n",
    "        self.state_dim = deter_dim + stoch_dim\n",
    "        \n",
    "        # Gate-enhanced encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(obs_dim, hidden_dim),\n",
    "            nn.ELU(),\n",
    "            QuantumGateLayer(hidden_dim),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ELU()\n",
    "        )\n",
    "        \n",
    "        self.gru = nn.GRUCell(stoch_dim + action_dim, deter_dim)\n",
    "        \n",
    "        self.prior = nn.Sequential(\n",
    "            nn.Linear(deter_dim, hidden_dim // 2),\n",
    "            nn.ELU(),\n",
    "            QuantumGateLayer(hidden_dim // 2),\n",
    "            nn.Linear(hidden_dim // 2, stoch_dim * 2)\n",
    "        )\n",
    "        \n",
    "        self.posterior = nn.Sequential(\n",
    "            nn.Linear(deter_dim + hidden_dim // 2, hidden_dim // 2),\n",
    "            nn.ELU(),\n",
    "            QuantumGateLayer(hidden_dim // 2),\n",
    "            nn.Linear(hidden_dim // 2, stoch_dim * 2)\n",
    "        )\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(self.state_dim, hidden_dim),\n",
    "            nn.ELU(),\n",
    "            QuantumGateLayer(hidden_dim),\n",
    "            nn.Linear(hidden_dim, obs_dim * 2)\n",
    "        )\n",
    "        \n",
    "        self.reward_pred = nn.Sequential(\n",
    "            nn.Linear(self.state_dim, hidden_dim // 2),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(hidden_dim // 2, 1)\n",
    "        )\n",
    "    \n",
    "    def initial_state(self, batch_size, device):\n",
    "        return {\n",
    "            'deter': torch.zeros(batch_size, self.deter_dim, device=device),\n",
    "            'stoch': torch.zeros(batch_size, self.stoch_dim, device=device)\n",
    "        }\n",
    "    \n",
    "    def forward(self, obs_seq, action_seq):\n",
    "        batch_size, seq_len = obs_seq.shape[:2]\n",
    "        device = obs_seq.device\n",
    "        \n",
    "        state = self.initial_state(batch_size, device)\n",
    "        states, prior_means, prior_stds, post_means, post_stds = [], [], [], [], []\n",
    "        \n",
    "        for t in range(seq_len):\n",
    "            embed = self.encoder(obs_seq[:, t])\n",
    "            gru_input = torch.cat([state['stoch'], action_seq[:, t]], dim=-1)\n",
    "            deter = self.gru(gru_input, state['deter'])\n",
    "            \n",
    "            prior_stats = self.prior(deter)\n",
    "            prior_mean, prior_log_std = torch.chunk(prior_stats, 2, dim=-1)\n",
    "            prior_std = F.softplus(prior_log_std) + 0.1\n",
    "            \n",
    "            post_input = torch.cat([deter, embed], dim=-1)\n",
    "            post_stats = self.posterior(post_input)\n",
    "            post_mean, post_log_std = torch.chunk(post_stats, 2, dim=-1)\n",
    "            post_std = F.softplus(post_log_std) + 0.1\n",
    "            \n",
    "            stoch = post_mean + post_std * torch.randn_like(post_std)\n",
    "            state = {'deter': deter, 'stoch': stoch}\n",
    "            \n",
    "            states.append(torch.cat([deter, stoch], dim=-1))\n",
    "            prior_means.append(prior_mean)\n",
    "            prior_stds.append(prior_std)\n",
    "            post_means.append(post_mean)\n",
    "            post_stds.append(post_std)\n",
    "        \n",
    "        states = torch.stack(states, dim=1)\n",
    "        flat_states = states.reshape(-1, self.state_dim)\n",
    "        \n",
    "        dec_output = self.decoder(flat_states)\n",
    "        obs_mean, obs_log_std = torch.chunk(dec_output, 2, dim=-1)\n",
    "        obs_mean = obs_mean.reshape(batch_size, seq_len, -1)\n",
    "        obs_log_std = obs_log_std.clamp(-10, 2).reshape(batch_size, seq_len, -1)\n",
    "        \n",
    "        reward_pred = self.reward_pred(flat_states).reshape(batch_size, seq_len)\n",
    "        \n",
    "        return {\n",
    "            'states': states, 'obs_mean': obs_mean, 'obs_log_std': obs_log_std,\n",
    "            'reward_pred': reward_pred, 'prior_mean': torch.stack(prior_means, dim=1),\n",
    "            'prior_std': torch.stack(prior_stds, dim=1),\n",
    "            'post_mean': torch.stack(post_means, dim=1),\n",
    "            'post_std': torch.stack(post_stds, dim=1)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error Correction Ensemble\n",
    "\n",
    "class EnsembleModel(nn.Module):\n",
    "    \"\"\"Ensemble with error correction.\"\"\"\n",
    "    \n",
    "    def __init__(self, obs_dim, action_dim, num_models=5, hidden_dim=128, \n",
    "                 deter_dim=64, stoch_dim=16):\n",
    "        super().__init__()\n",
    "        self.num_models = num_models\n",
    "        \n",
    "        self.models = nn.ModuleList([\n",
    "            BaseWorldModel(obs_dim, action_dim, hidden_dim, deter_dim, stoch_dim)\n",
    "            for _ in range(num_models)\n",
    "        ])\n",
    "        \n",
    "        # Initialize with different seeds\n",
    "        for i, model in enumerate(self.models):\n",
    "            torch.manual_seed(42 + i * 1000)\n",
    "            for m in model.modules():\n",
    "                if hasattr(m, 'reset_parameters'):\n",
    "                    m.reset_parameters()\n",
    "    \n",
    "    def forward(self, obs_seq, action_seq):\n",
    "        all_outputs = [model(obs_seq, action_seq) for model in self.models]\n",
    "        \n",
    "        # Weighted averaging based on disagreement\n",
    "        obs_means = torch.stack([o['obs_mean'] for o in all_outputs], dim=0)\n",
    "        ensemble_mean = obs_means.mean(dim=0)\n",
    "        deviations = (obs_means - ensemble_mean.unsqueeze(0)).abs().mean(dim=(2, 3))\n",
    "        weights = 1.0 / (deviations + 1e-8)\n",
    "        weights = weights / weights.sum(dim=0, keepdim=True)\n",
    "        \n",
    "        # Weighted prediction\n",
    "        obs_mean = (obs_means * weights.unsqueeze(-1).unsqueeze(-1)).sum(dim=0)\n",
    "        \n",
    "        obs_log_stds = torch.stack([o['obs_log_std'] for o in all_outputs], dim=0)\n",
    "        obs_log_std = (obs_log_stds * weights.unsqueeze(-1).unsqueeze(-1)).sum(dim=0)\n",
    "        \n",
    "        reward_preds = torch.stack([o['reward_pred'] for o in all_outputs], dim=0)\n",
    "        reward_pred = (reward_preds * weights.unsqueeze(-1)).sum(dim=0)\n",
    "        \n",
    "        # Average stats for loss computation\n",
    "        return {\n",
    "            'obs_mean': obs_mean,\n",
    "            'obs_log_std': obs_log_std,\n",
    "            'reward_pred': reward_pred,\n",
    "            'prior_mean': torch.stack([o['prior_mean'] for o in all_outputs]).mean(0),\n",
    "            'prior_std': torch.stack([o['prior_std'] for o in all_outputs]).mean(0),\n",
    "            'post_mean': torch.stack([o['post_mean'] for o in all_outputs]).mean(0),\n",
    "            'post_std': torch.stack([o['post_std'] for o in all_outputs]).mean(0),\n",
    "            'states': torch.stack([o['states'] for o in all_outputs]).mean(0),\n",
    "            'all_outputs': all_outputs\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3 Experiment Framework\n",
    "\n",
    "Framework for running experiments across multiple seeds and environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ExperimentConfig:\n",
    "    \"\"\"Configuration for a single experiment.\"\"\"\n",
    "    name: str\n",
    "    env_name: str\n",
    "    num_seeds: int = 5\n",
    "    num_episodes: int = 20\n",
    "    num_epochs: int = 50\n",
    "    batch_size: int = 32\n",
    "    seq_len: int = 20\n",
    "    learning_rate: float = 1e-4\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ExperimentResult:\n",
    "    \"\"\"Results from a single experiment run.\"\"\"\n",
    "    approach: str\n",
    "    seed: int\n",
    "    final_loss: float\n",
    "    training_time: float\n",
    "    loss_history: List[float]\n",
    "    prediction_error: float\n",
    "    \n",
    "\n",
    "def collect_data(env_name: str, num_episodes: int = 20, max_steps: int = 200):\n",
    "    \"\"\"Collect episodes from environment.\"\"\"\n",
    "    env = gym.make(env_name)\n",
    "    episodes = []\n",
    "    \n",
    "    for _ in range(num_episodes):\n",
    "        obs_list, action_list, reward_list = [], [], []\n",
    "        obs, _ = env.reset()\n",
    "        obs_list.append(obs)\n",
    "        \n",
    "        for _ in range(max_steps):\n",
    "            action = env.action_space.sample()\n",
    "            next_obs, reward, terminated, truncated, _ = env.step(action)\n",
    "            \n",
    "            if isinstance(action, (int, np.integer)):\n",
    "                action_list.append([float(action)])\n",
    "            else:\n",
    "                action_list.append(action)\n",
    "            reward_list.append(reward)\n",
    "            obs_list.append(next_obs)\n",
    "            \n",
    "            if terminated or truncated:\n",
    "                break\n",
    "            obs = next_obs\n",
    "        \n",
    "        obs_list = obs_list[:-1]\n",
    "        if len(obs_list) > 10:\n",
    "            episodes.append({\n",
    "                'obs': np.array(obs_list, dtype=np.float32),\n",
    "                'actions': np.array(action_list, dtype=np.float32),\n",
    "                'rewards': np.array(reward_list, dtype=np.float32)\n",
    "            })\n",
    "    \n",
    "    env.close()\n",
    "    return episodes\n",
    "\n",
    "\n",
    "def create_batches(episodes, batch_size=32, seq_len=20):\n",
    "    \"\"\"Create training batches from episodes.\"\"\"\n",
    "    sequences = []\n",
    "    for ep in episodes:\n",
    "        ep_len = len(ep['obs'])\n",
    "        for start in range(0, ep_len - seq_len, seq_len // 2):\n",
    "            sequences.append({\n",
    "                'obs': ep['obs'][start:start+seq_len],\n",
    "                'actions': ep['actions'][start:start+seq_len],\n",
    "                'rewards': ep['rewards'][start:start+seq_len]\n",
    "            })\n",
    "    \n",
    "    np.random.shuffle(sequences)\n",
    "    batches = []\n",
    "    \n",
    "    for i in range(0, len(sequences) - batch_size, batch_size):\n",
    "        batch_seqs = sequences[i:i+batch_size]\n",
    "        batches.append((\n",
    "            torch.tensor(np.stack([s['obs'] for s in batch_seqs]), dtype=torch.float32, device=device),\n",
    "            torch.tensor(np.stack([s['actions'] for s in batch_seqs]), dtype=torch.float32, device=device),\n",
    "            torch.tensor(np.stack([s['rewards'] for s in batch_seqs]), dtype=torch.float32, device=device)\n",
    "        ))\n",
    "    \n",
    "    return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(\n",
    "    config: ExperimentConfig,\n",
    "    approach: str,\n",
    "    seed: int,\n",
    "    batches: List,\n",
    "    obs_dim: int,\n",
    "    action_dim: int\n",
    ") -> ExperimentResult:\n",
    "    \"\"\"\n",
    "    Run a single experiment with given configuration.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    config : ExperimentConfig\n",
    "        Experiment configuration\n",
    "    approach : str\n",
    "        Name of the approach\n",
    "    seed : int\n",
    "        Random seed\n",
    "    batches : List\n",
    "        Training batches\n",
    "    obs_dim : int\n",
    "        Observation dimension\n",
    "    action_dim : int\n",
    "        Action dimension\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    ExperimentResult\n",
    "        Results from this run\n",
    "    \"\"\"\n",
    "    set_seed(seed)\n",
    "    \n",
    "    # Create model and trainer based on approach\n",
    "    if approach == 'baseline':\n",
    "        model = BaseWorldModel(obs_dim, action_dim).to(device)\n",
    "        trainer = BaselineTrainer(model, lr=config.learning_rate)\n",
    "    elif approach == 'qaoa':\n",
    "        model = BaseWorldModel(obs_dim, action_dim).to(device)\n",
    "        trainer = QAOATrainer(model, lr=config.learning_rate)\n",
    "    elif approach == 'superposition':\n",
    "        model = BaseWorldModel(obs_dim, action_dim).to(device)\n",
    "        trainer = SuperpositionReplayTrainer(model, lr=config.learning_rate)\n",
    "    elif approach == 'gates':\n",
    "        model = GateEnhancedModel(obs_dim, action_dim).to(device)\n",
    "        trainer = BaselineTrainer(model, lr=config.learning_rate)\n",
    "    elif approach == 'error_correction':\n",
    "        model = EnsembleModel(obs_dim, action_dim, num_models=5).to(device)\n",
    "        trainer = BaselineTrainer(model, lr=config.learning_rate)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown approach: {approach}\")\n",
    "    \n",
    "    # Training\n",
    "    loss_history = []\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(config.num_epochs):\n",
    "        epoch_losses = []\n",
    "        for obs, actions, rewards in batches:\n",
    "            metrics = trainer.train_step(obs, actions, rewards)\n",
    "            epoch_losses.append(metrics['loss'])\n",
    "        loss_history.append(np.mean(epoch_losses))\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        eval_errors = []\n",
    "        for obs, actions, rewards in batches[:5]:  # Use subset for eval\n",
    "            outputs = model(obs, actions)\n",
    "            error = F.mse_loss(outputs['obs_mean'], obs).item()\n",
    "            eval_errors.append(error)\n",
    "    \n",
    "    return ExperimentResult(\n",
    "        approach=approach,\n",
    "        seed=seed,\n",
    "        final_loss=np.mean(loss_history[-5:]),\n",
    "        training_time=training_time,\n",
    "        loss_history=loss_history,\n",
    "        prediction_error=np.mean(eval_errors)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.4 Run Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_all_experiments(config: ExperimentConfig) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Run all approaches across multiple seeds.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    config : ExperimentConfig\n",
    "        Experiment configuration\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Results dataframe\n",
    "    \"\"\"\n",
    "    approaches = ['baseline', 'qaoa', 'superposition', 'gates', 'error_correction']\n",
    "    \n",
    "    print(f\"Collecting data from {config.env_name}...\")\n",
    "    episodes = collect_data(config.env_name, config.num_episodes)\n",
    "    \n",
    "    if not episodes:\n",
    "        print(\"No episodes collected!\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    batches = create_batches(episodes, config.batch_size, config.seq_len)\n",
    "    print(f\"Created {len(batches)} training batches\")\n",
    "    \n",
    "    obs_dim = episodes[0]['obs'].shape[1]\n",
    "    action_dim = episodes[0]['actions'].shape[1]\n",
    "    \n",
    "    results = []\n",
    "    total_runs = len(approaches) * config.num_seeds\n",
    "    current_run = 0\n",
    "    \n",
    "    for approach in approaches:\n",
    "        for seed in range(config.num_seeds):\n",
    "            current_run += 1\n",
    "            print(f\"\\n[{current_run}/{total_runs}] Running {approach} with seed {seed}...\")\n",
    "            \n",
    "            result = run_experiment(\n",
    "                config, approach, seed + 42, batches, obs_dim, action_dim\n",
    "            )\n",
    "            results.append(result)\n",
    "            \n",
    "            print(f\"  Final loss: {result.final_loss:.4f}, \"\n",
    "                  f\"Pred error: {result.prediction_error:.4f}, \"\n",
    "                  f\"Time: {result.training_time:.2f}s\")\n",
    "    \n",
    "    # Convert to dataframe\n",
    "    df = pd.DataFrame([\n",
    "        {\n",
    "            'approach': r.approach,\n",
    "            'seed': r.seed,\n",
    "            'final_loss': r.final_loss,\n",
    "            'training_time': r.training_time,\n",
    "            'prediction_error': r.prediction_error\n",
    "        }\n",
    "        for r in results\n",
    "    ])\n",
    "    \n",
    "    return df, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run experiments on CartPole\n",
    "config = ExperimentConfig(\n",
    "    name='cartpole_comparison',\n",
    "    env_name='CartPole-v1',\n",
    "    num_seeds=5,\n",
    "    num_episodes=20,\n",
    "    num_epochs=50,\n",
    "    batch_size=32,\n",
    "    seq_len=20\n",
    ")\n",
    "\n",
    "print(f\"Starting experiment: {config.name}\")\n",
    "print(f\"Environment: {config.env_name}\")\n",
    "print(f\"Seeds: {config.num_seeds}, Epochs: {config.num_epochs}\")\n",
    "\n",
    "df, results = run_all_experiments(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.5 Statistical Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_statistics(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute summary statistics for each approach.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Results dataframe\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Summary statistics\n",
    "    \"\"\"\n",
    "    stats_data = []\n",
    "    \n",
    "    for approach in df['approach'].unique():\n",
    "        approach_df = df[df['approach'] == approach]\n",
    "        \n",
    "        for metric in ['final_loss', 'prediction_error', 'training_time']:\n",
    "            values = approach_df[metric].values\n",
    "            \n",
    "            stats_data.append({\n",
    "                'approach': approach,\n",
    "                'metric': metric,\n",
    "                'mean': np.mean(values),\n",
    "                'std': np.std(values),\n",
    "                'min': np.min(values),\n",
    "                'max': np.max(values),\n",
    "                'ci_lower': np.mean(values) - 1.96 * np.std(values) / np.sqrt(len(values)),\n",
    "                'ci_upper': np.mean(values) + 1.96 * np.std(values) / np.sqrt(len(values))\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(stats_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pairwise_tests(\n",
    "    df: pd.DataFrame,\n",
    "    metric: str = 'final_loss',\n",
    "    baseline: str = 'baseline'\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute pairwise statistical tests vs baseline.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Results dataframe\n",
    "    metric : str\n",
    "        Metric to compare\n",
    "    baseline : str\n",
    "        Baseline approach name\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Statistical test results\n",
    "    \"\"\"\n",
    "    baseline_values = df[df['approach'] == baseline][metric].values\n",
    "    \n",
    "    test_results = []\n",
    "    \n",
    "    for approach in df['approach'].unique():\n",
    "        if approach == baseline:\n",
    "            continue\n",
    "        \n",
    "        approach_values = df[df['approach'] == approach][metric].values\n",
    "        \n",
    "        # Mann-Whitney U test\n",
    "        u_stat, p_value = stats.mannwhitneyu(\n",
    "            baseline_values, approach_values, alternative='two-sided'\n",
    "        )\n",
    "        \n",
    "        # Cohen's d effect size\n",
    "        pooled_std = np.sqrt(\n",
    "            (np.std(baseline_values)**2 + np.std(approach_values)**2) / 2\n",
    "        )\n",
    "        cohens_d = (np.mean(baseline_values) - np.mean(approach_values)) / pooled_std\n",
    "        \n",
    "        # Effect size interpretation\n",
    "        if abs(cohens_d) < 0.2:\n",
    "            effect = 'negligible'\n",
    "        elif abs(cohens_d) < 0.5:\n",
    "            effect = 'small'\n",
    "        elif abs(cohens_d) < 0.8:\n",
    "            effect = 'medium'\n",
    "        else:\n",
    "            effect = 'large'\n",
    "        \n",
    "        test_results.append({\n",
    "            'approach': approach,\n",
    "            'vs_baseline': baseline,\n",
    "            'metric': metric,\n",
    "            'baseline_mean': np.mean(baseline_values),\n",
    "            'approach_mean': np.mean(approach_values),\n",
    "            'difference': np.mean(baseline_values) - np.mean(approach_values),\n",
    "            'u_statistic': u_stat,\n",
    "            'p_value': p_value,\n",
    "            'cohens_d': cohens_d,\n",
    "            'effect_size': effect,\n",
    "            'significant': p_value < 0.05\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute statistics\n",
    "summary_stats = compute_statistics(df)\n",
    "\n",
    "print(\"Summary Statistics:\")\n",
    "print(\"=\"*80)\n",
    "for metric in ['final_loss', 'prediction_error', 'training_time']:\n",
    "    print(f\"\\n{metric.upper()}:\")\n",
    "    metric_df = summary_stats[summary_stats['metric'] == metric]\n",
    "    for _, row in metric_df.iterrows():\n",
    "        print(f\"  {row['approach']:20s}: {row['mean']:.4f} +/- {row['std']:.4f} \"\n",
    "              f\"[{row['ci_lower']:.4f}, {row['ci_upper']:.4f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pairwise tests\n",
    "loss_tests = compute_pairwise_tests(df, 'final_loss')\n",
    "error_tests = compute_pairwise_tests(df, 'prediction_error')\n",
    "\n",
    "print(\"\\nStatistical Tests vs Baseline (Final Loss):\")\n",
    "print(\"=\"*80)\n",
    "for _, row in loss_tests.iterrows():\n",
    "    sig = \"*\" if row['significant'] else \"\"\n",
    "    print(f\"{row['approach']:20s}: d={row['cohens_d']:+.3f} ({row['effect_size']}), \"\n",
    "          f\"p={row['p_value']:.4f}{sig}\")\n",
    "\n",
    "print(\"\\nStatistical Tests vs Baseline (Prediction Error):\")\n",
    "print(\"=\"*80)\n",
    "for _, row in error_tests.iterrows():\n",
    "    sig = \"*\" if row['significant'] else \"\"\n",
    "    print(f\"{row['approach']:20s}: d={row['cohens_d']:+.3f} ({row['effect_size']}), \"\n",
    "          f\"p={row['p_value']:.4f}{sig}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.6 Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "approach_colors = {\n",
    "    'baseline': COLORS['baseline'],\n",
    "    'qaoa': COLORS['qaoa'],\n",
    "    'superposition': COLORS['superposition'],\n",
    "    'gates': COLORS['gates'],\n",
    "    'error_correction': COLORS['error_correction']\n",
    "}\n",
    "\n",
    "# Group results by approach\n",
    "approach_histories = defaultdict(list)\n",
    "for r in results:\n",
    "    approach_histories[r.approach].append(r.loss_history)\n",
    "\n",
    "# Plot learning curves\n",
    "ax = axes[0]\n",
    "for approach, histories in approach_histories.items():\n",
    "    histories_arr = np.array(histories)\n",
    "    mean_history = histories_arr.mean(axis=0)\n",
    "    std_history = histories_arr.std(axis=0)\n",
    "    \n",
    "    epochs = range(len(mean_history))\n",
    "    ax.plot(epochs, mean_history, label=approach.replace('_', ' ').title(),\n",
    "            color=approach_colors[approach], linewidth=2)\n",
    "    ax.fill_between(epochs, mean_history - std_history, mean_history + std_history,\n",
    "                   color=approach_colors[approach], alpha=0.2)\n",
    "\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Training Loss')\n",
    "ax.set_title('Learning Curves (Mean +/- Std)')\n",
    "ax.legend(loc='upper right')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Bar plot of final performance\n",
    "ax = axes[1]\n",
    "approaches = list(approach_histories.keys())\n",
    "means = [df[df['approach'] == a]['final_loss'].mean() for a in approaches]\n",
    "stds = [df[df['approach'] == a]['final_loss'].std() for a in approaches]\n",
    "colors = [approach_colors[a] for a in approaches]\n",
    "\n",
    "bars = ax.bar(range(len(approaches)), means, yerr=stds, capsize=5, color=colors, alpha=0.8)\n",
    "ax.set_xticks(range(len(approaches)))\n",
    "ax.set_xticklabels([a.replace('_', '\\n').title() for a in approaches], fontsize=9)\n",
    "ax.set_ylabel('Final Loss')\n",
    "ax.set_title('Final Loss Comparison')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/figures/comprehensive_comparison_learning.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "metrics = ['final_loss', 'prediction_error', 'training_time']\n",
    "titles = ['Final Training Loss', 'Prediction Error (MSE)', 'Training Time (s)']\n",
    "\n",
    "for ax, metric, title in zip(axes, metrics, titles):\n",
    "    data = [df[df['approach'] == a][metric].values for a in approaches]\n",
    "    \n",
    "    bp = ax.boxplot(data, patch_artist=True, labels=[a.replace('_', '\\n').title() \n",
    "                                                      for a in approaches])\n",
    "    \n",
    "    for patch, color in zip(bp['boxes'], colors):\n",
    "        patch.set_facecolor(color)\n",
    "        patch.set_alpha(0.7)\n",
    "    \n",
    "    ax.set_ylabel(title)\n",
    "    ax.set_title(title)\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/figures/comprehensive_comparison_boxplots.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Effect size visualization\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Combine loss and error tests\n",
    "all_tests = pd.concat([loss_tests, error_tests])\n",
    "\n",
    "# Create grouped bar chart\n",
    "x = np.arange(len(loss_tests))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax.bar(x - width/2, loss_tests['cohens_d'], width, \n",
    "               label='Final Loss', color=COLORS['baseline'], alpha=0.8)\n",
    "bars2 = ax.bar(x + width/2, error_tests['cohens_d'], width,\n",
    "               label='Prediction Error', color=COLORS['qaoa'], alpha=0.8)\n",
    "\n",
    "ax.set_ylabel(\"Cohen's d (vs Baseline)\")\n",
    "ax.set_title('Effect Sizes vs Baseline')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([a.replace('_', '\\n').title() for a in loss_tests['approach']])\n",
    "ax.legend()\n",
    "ax.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "ax.axhline(y=0.2, color='gray', linestyle='--', linewidth=0.5, alpha=0.5)\n",
    "ax.axhline(y=-0.2, color='gray', linestyle='--', linewidth=0.5, alpha=0.5)\n",
    "ax.axhline(y=0.5, color='gray', linestyle=':', linewidth=0.5, alpha=0.5)\n",
    "ax.axhline(y=-0.5, color='gray', linestyle=':', linewidth=0.5, alpha=0.5)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add effect size thresholds\n",
    "ax.text(ax.get_xlim()[1] + 0.1, 0.2, 'Small', va='center', fontsize=8, color='gray')\n",
    "ax.text(ax.get_xlim()[1] + 0.1, 0.5, 'Medium', va='center', fontsize=8, color='gray')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/figures/comprehensive_comparison_effects.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.7 Results Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary table\n",
    "summary_table = []\n",
    "\n",
    "for approach in approaches:\n",
    "    approach_df = df[df['approach'] == approach]\n",
    "    \n",
    "    # Get test results\n",
    "    loss_row = loss_tests[loss_tests['approach'] == approach] if approach != 'baseline' else None\n",
    "    error_row = error_tests[error_tests['approach'] == approach] if approach != 'baseline' else None\n",
    "    \n",
    "    row = {\n",
    "        'Approach': approach.replace('_', ' ').title(),\n",
    "        'Final Loss': f\"{approach_df['final_loss'].mean():.4f} +/- {approach_df['final_loss'].std():.4f}\",\n",
    "        'Pred. Error': f\"{approach_df['prediction_error'].mean():.4f} +/- {approach_df['prediction_error'].std():.4f}\",\n",
    "        'Time (s)': f\"{approach_df['training_time'].mean():.1f}\"\n",
    "    }\n",
    "    \n",
    "    if loss_row is not None and len(loss_row) > 0:\n",
    "        sig = '*' if loss_row['significant'].values[0] else ''\n",
    "        row['vs Baseline'] = f\"d={loss_row['cohens_d'].values[0]:+.2f}{sig}\"\n",
    "    else:\n",
    "        row['vs Baseline'] = '(baseline)'\n",
    "    \n",
    "    summary_table.append(row)\n",
    "\n",
    "summary_df = pd.DataFrame(summary_table)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPREHENSIVE COMPARISON RESULTS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Environment: {config.env_name}\")\n",
    "print(f\"Seeds: {config.num_seeds}, Epochs: {config.num_epochs}\")\n",
    "print(\"=\"*80)\n",
    "print(summary_df.to_string(index=False))\n",
    "print(\"=\"*80)\n",
    "print(\"* indicates p < 0.05 vs baseline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.8 Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "results_dir = Path('../results/comparison')\n",
    "results_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save raw results\n",
    "df.to_csv(results_dir / 'raw_results.csv', index=False)\n",
    "\n",
    "# Save summary statistics\n",
    "summary_stats.to_csv(results_dir / 'summary_statistics.csv', index=False)\n",
    "\n",
    "# Save statistical tests\n",
    "loss_tests.to_csv(results_dir / 'statistical_tests_loss.csv', index=False)\n",
    "error_tests.to_csv(results_dir / 'statistical_tests_error.csv', index=False)\n",
    "\n",
    "# Save summary table\n",
    "summary_df.to_csv(results_dir / 'summary_table.csv', index=False)\n",
    "\n",
    "print(f\"Results saved to {results_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.9 Summary\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "Based on the comprehensive comparison across multiple seeds:\n",
    "\n",
    "1. **Training Efficiency**: Compare final loss values and convergence speed\n",
    "2. **Prediction Quality**: Compare reconstruction errors\n",
    "3. **Statistical Significance**: Identify which improvements are statistically significant\n",
    "4. **Effect Sizes**: Understand the practical magnitude of differences\n",
    "\n",
    "### Conclusions\n",
    "\n",
    "The results provide evidence for understanding:\n",
    "- Which quantum-inspired approaches provide benefits\n",
    "- Under what conditions each approach excels\n",
    "- The trade-offs between different methods\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Phase 8: Ablation Studies (component analysis)\n",
    "- Phase 9: Results & Analysis (final documentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Phase 7: Comprehensive Comparison - COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nCompleted:\")\n",
    "print(\"  - Unified implementation of all 5 approaches\")\n",
    "print(\"  - Multi-seed experiments (5 seeds per approach)\")\n",
    "print(\"  - Statistical analysis (Mann-Whitney U, Cohen's d)\")\n",
    "print(\"  - 95% confidence intervals\")\n",
    "print(\"  - Comprehensive visualizations\")\n",
    "print(\"  - Results saved to results/comparison/\")\n",
    "print(\"\\nReady for Phase 8: Ablation Studies\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
