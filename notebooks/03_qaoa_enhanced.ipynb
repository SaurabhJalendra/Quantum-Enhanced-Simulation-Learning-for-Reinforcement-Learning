{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 3: QAOA-Enhanced Training\n",
    "\n",
    "**Notebook:** `03_qaoa_enhanced.ipynb`  \n",
    "**Phase:** 3 of 9  \n",
    "**Purpose:** Implement Quantum Approximate Optimization Algorithm (QAOA) inspired optimizer for world model training  \n",
    "**Author:** Saurabh Jalendra  \n",
    "**Institution:** BITS Pilani (WILP Division)  \n",
    "**Date:** November 2025\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Setup & Imports](#1-setup--imports)\n",
    "2. [QAOA Background](#2-qaoa-background)\n",
    "3. [Cost Operator](#3-cost-operator)\n",
    "4. [Mixing Operator](#4-mixing-operator)\n",
    "5. [QAOA-Inspired Optimizer](#5-qaoa-inspired-optimizer)\n",
    "6. [Enhanced Training Loop](#6-enhanced-training-loop)\n",
    "7. [Experiments](#7-experiments)\n",
    "8. [Comparison with Baseline](#8-comparison-with-baseline)\n",
    "9. [Visualizations](#9-visualizations)\n",
    "10. [Summary](#10-summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Cell: Imports and Configuration\n",
    "Purpose: Import packages and set up environment\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Tuple, Optional, Union, Any, NamedTuple, Callable\n",
    "from dataclasses import dataclass, field\n",
    "from collections import deque\n",
    "import copy\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "\n",
    "import gymnasium as gym\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import seaborn as sns\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Add project to path\n",
    "PROJECT_ROOT = Path.cwd().parent\n",
    "sys.path.insert(0, str(PROJECT_ROOT / \"src\"))\n",
    "\n",
    "from utils import set_seed, get_device, MetricLogger, timer, COLORS\n",
    "\n",
    "SEED = 42\n",
    "set_seed(SEED)\n",
    "DEVICE = get_device()\n",
    "\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"PyTorch: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Cell: Import World Model from Phase 2\n",
    "Purpose: Reuse the world model architecture\n",
    "\"\"\"\n",
    "\n",
    "# We'll re-implement the core components here to keep the notebook self-contained\n",
    "# In practice, these would be imported from src/models/\n",
    "\n",
    "class RSSMState(NamedTuple):\n",
    "    \"\"\"RSSM state with deterministic and stochastic components.\"\"\"\n",
    "    deter: torch.Tensor\n",
    "    stoch: torch.Tensor\n",
    "    \n",
    "    @property\n",
    "    def combined(self) -> torch.Tensor:\n",
    "        return torch.cat([self.deter, self.stoch], dim=-1)\n",
    "\n",
    "\n",
    "class MLPEncoder(nn.Module):\n",
    "    \"\"\"MLP encoder for state-based observations.\"\"\"\n",
    "    def __init__(self, obs_dim: int, hidden_dims: List[int], latent_dim: int):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        in_dim = obs_dim\n",
    "        for hd in hidden_dims:\n",
    "            layers.extend([nn.Linear(in_dim, hd), nn.ELU()])\n",
    "            in_dim = hd\n",
    "        layers.append(nn.Linear(in_dim, latent_dim))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, obs: torch.Tensor) -> torch.Tensor:\n",
    "        return self.network(obs)\n",
    "\n",
    "\n",
    "class MLPDecoder(nn.Module):\n",
    "    \"\"\"MLP decoder for observation reconstruction.\"\"\"\n",
    "    def __init__(self, latent_dim: int, hidden_dims: List[int], obs_dim: int):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        in_dim = latent_dim\n",
    "        for hd in hidden_dims:\n",
    "            layers.extend([nn.Linear(in_dim, hd), nn.ELU()])\n",
    "            in_dim = hd\n",
    "        layers.append(nn.Linear(in_dim, obs_dim))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, latent: torch.Tensor) -> torch.Tensor:\n",
    "        return self.network(latent)\n",
    "\n",
    "\n",
    "class RSSM(nn.Module):\n",
    "    \"\"\"Recurrent State-Space Model.\"\"\"\n",
    "    def __init__(self, stoch_dim: int, deter_dim: int, hidden_dim: int, \n",
    "                 action_dim: int, embed_dim: int, min_std: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.stoch_dim = stoch_dim\n",
    "        self.deter_dim = deter_dim\n",
    "        self.min_std = min_std\n",
    "        \n",
    "        self.input_proj = nn.Sequential(nn.Linear(stoch_dim + action_dim, hidden_dim), nn.ELU())\n",
    "        self.gru = nn.GRUCell(hidden_dim, deter_dim)\n",
    "        self.prior_net = nn.Sequential(nn.Linear(deter_dim, hidden_dim), nn.ELU(), nn.Linear(hidden_dim, 2*stoch_dim))\n",
    "        self.posterior_net = nn.Sequential(nn.Linear(deter_dim + embed_dim, hidden_dim), nn.ELU(), nn.Linear(hidden_dim, 2*stoch_dim))\n",
    "    \n",
    "    def initial_state(self, batch_size: int, device: torch.device) -> RSSMState:\n",
    "        return RSSMState(\n",
    "            deter=torch.zeros(batch_size, self.deter_dim, device=device),\n",
    "            stoch=torch.zeros(batch_size, self.stoch_dim, device=device)\n",
    "        )\n",
    "    \n",
    "    def _get_dist(self, stats: torch.Tensor) -> Normal:\n",
    "        mean, std = torch.chunk(stats, 2, dim=-1)\n",
    "        std = F.softplus(std) + self.min_std\n",
    "        return Normal(mean, std)\n",
    "    \n",
    "    def imagine_step(self, prev_state: RSSMState, action: torch.Tensor) -> Tuple[RSSMState, Normal]:\n",
    "        x = self.input_proj(torch.cat([prev_state.stoch, action], dim=-1))\n",
    "        deter = self.gru(x, prev_state.deter)\n",
    "        prior_dist = self._get_dist(self.prior_net(deter))\n",
    "        stoch = prior_dist.rsample()\n",
    "        return RSSMState(deter=deter, stoch=stoch), prior_dist\n",
    "    \n",
    "    def observe_step(self, prev_state: RSSMState, action: torch.Tensor, \n",
    "                    embed: torch.Tensor) -> Tuple[RSSMState, Normal, Normal]:\n",
    "        prior_state, prior_dist = self.imagine_step(prev_state, action)\n",
    "        posterior_dist = self._get_dist(self.posterior_net(torch.cat([prior_state.deter, embed], dim=-1)))\n",
    "        stoch = posterior_dist.rsample()\n",
    "        return RSSMState(deter=prior_state.deter, stoch=stoch), prior_dist, posterior_dist\n",
    "\n",
    "\n",
    "class RewardPredictor(nn.Module):\n",
    "    def __init__(self, state_dim: int, hidden_dims: List[int]):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        in_dim = state_dim\n",
    "        for hd in hidden_dims:\n",
    "            layers.extend([nn.Linear(in_dim, hd), nn.ELU()])\n",
    "            in_dim = hd\n",
    "        layers.append(nn.Linear(in_dim, 1))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, state: torch.Tensor) -> torch.Tensor:\n",
    "        return self.network(state).squeeze(-1)\n",
    "\n",
    "\n",
    "class ContinuePredictor(nn.Module):\n",
    "    def __init__(self, state_dim: int, hidden_dims: List[int]):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        in_dim = state_dim\n",
    "        for hd in hidden_dims:\n",
    "            layers.extend([nn.Linear(in_dim, hd), nn.ELU()])\n",
    "            in_dim = hd\n",
    "        layers.append(nn.Linear(in_dim, 1))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, state: torch.Tensor) -> torch.Tensor:\n",
    "        return self.network(state).squeeze(-1)\n",
    "\n",
    "\n",
    "class WorldModel(nn.Module):\n",
    "    \"\"\"Complete DreamerV3-style World Model.\"\"\"\n",
    "    def __init__(self, obs_dim: int, action_dim: int, stoch_dim: int = 32,\n",
    "                 deter_dim: int = 256, hidden_dim: int = 256,\n",
    "                 encoder_hidden: List[int] = [256, 256],\n",
    "                 decoder_hidden: List[int] = [256, 256],\n",
    "                 predictor_hidden: List[int] = [256, 256]):\n",
    "        super().__init__()\n",
    "        self.obs_dim = obs_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.state_dim = stoch_dim + deter_dim\n",
    "        \n",
    "        self.encoder = MLPEncoder(obs_dim, encoder_hidden, hidden_dim)\n",
    "        self.rssm = RSSM(stoch_dim, deter_dim, hidden_dim, action_dim, hidden_dim)\n",
    "        self.reward_predictor = RewardPredictor(self.state_dim, predictor_hidden)\n",
    "        self.continue_predictor = ContinuePredictor(self.state_dim, predictor_hidden)\n",
    "        self.decoder = MLPDecoder(self.state_dim, decoder_hidden, obs_dim)\n",
    "    \n",
    "    def initial_state(self, batch_size: int) -> RSSMState:\n",
    "        device = next(self.parameters()).device\n",
    "        return self.rssm.initial_state(batch_size, device)\n",
    "    \n",
    "    def observe(self, obs: torch.Tensor, action: torch.Tensor, \n",
    "               prev_state: RSSMState) -> Tuple[RSSMState, Normal, Normal]:\n",
    "        embed = self.encoder(obs)\n",
    "        return self.rssm.observe_step(prev_state, action, embed)\n",
    "    \n",
    "    def predict(self, state: RSSMState) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        combined = state.combined\n",
    "        return self.decoder(combined), self.reward_predictor(combined), self.continue_predictor(combined)\n",
    "    \n",
    "    def forward(self, obs_seq: torch.Tensor, action_seq: torch.Tensor) -> Dict[str, Any]:\n",
    "        batch_size, seq_len, _ = obs_seq.shape\n",
    "        state = self.initial_state(batch_size)\n",
    "        \n",
    "        priors, posteriors = [], []\n",
    "        recon_obs, pred_rewards, pred_continues = [], [], []\n",
    "        \n",
    "        for t in range(seq_len):\n",
    "            state, prior, posterior = self.observe(obs_seq[:, t], action_seq[:, t], state)\n",
    "            recon, reward, cont = self.predict(state)\n",
    "            priors.append(prior)\n",
    "            posteriors.append(posterior)\n",
    "            recon_obs.append(recon)\n",
    "            pred_rewards.append(reward)\n",
    "            pred_continues.append(cont)\n",
    "        \n",
    "        return {\n",
    "            'recon_obs': torch.stack(recon_obs, dim=1),\n",
    "            'pred_rewards': torch.stack(pred_rewards, dim=1),\n",
    "            'pred_continues': torch.stack(pred_continues, dim=1),\n",
    "            'priors': priors, 'posteriors': posteriors\n",
    "        }\n",
    "\n",
    "\n",
    "print(\"World model components loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. QAOA Background\n",
    "\n",
    "The Quantum Approximate Optimization Algorithm (QAOA) alternates between:\n",
    "1. **Cost Operator** $U(C, \\gamma)$: Encodes the optimization objective\n",
    "2. **Mixing Operator** $U(B, \\beta)$: Enables exploration of the solution space\n",
    "\n",
    "For our classical, quantum-*inspired* implementation:\n",
    "- **Cost Layer**: Standard gradient descent step based on the loss\n",
    "- **Mixing Layer**: Adds structured noise/perturbation to parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Cell: QAOA Diagram\n",
    "Purpose: Visualize the QAOA-inspired training process\n",
    "\"\"\"\n",
    "\n",
    "def create_qaoa_diagram(figsize: Tuple[int, int] = (14, 8)) -> plt.Figure:\n",
    "    \"\"\"\n",
    "    Create diagram illustrating QAOA-inspired optimization.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    ax.set_xlim(0, 14)\n",
    "    ax.set_ylim(0, 8)\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # Draw the QAOA layers\n",
    "    colors = {\n",
    "        'params': '#3498db',\n",
    "        'cost': '#e74c3c',\n",
    "        'mixing': '#9b59b6',\n",
    "        'output': '#2ecc71'\n",
    "    }\n",
    "    \n",
    "    # Initial parameters\n",
    "    rect = mpatches.FancyBboxPatch((0.5, 3.5), 2, 1, boxstyle=\"round,pad=0.05\",\n",
    "                                   facecolor=colors['params'], alpha=0.8)\n",
    "    ax.add_patch(rect)\n",
    "    ax.text(1.5, 4, 'θ₀\\n(Initial)', ha='center', va='center', fontsize=10, color='white', fontweight='bold')\n",
    "    \n",
    "    # P layers\n",
    "    for p in range(3):\n",
    "        x_offset = 3 + p * 3.5\n",
    "        \n",
    "        # Cost layer\n",
    "        rect = mpatches.FancyBboxPatch((x_offset, 5), 2, 1.5, boxstyle=\"round,pad=0.05\",\n",
    "                                       facecolor=colors['cost'], alpha=0.8)\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(x_offset + 1, 5.75, f'Cost\\nU(C,γ{p+1})', ha='center', va='center', \n",
    "               fontsize=9, color='white', fontweight='bold')\n",
    "        \n",
    "        # Mixing layer\n",
    "        rect = mpatches.FancyBboxPatch((x_offset, 2), 2, 1.5, boxstyle=\"round,pad=0.05\",\n",
    "                                       facecolor=colors['mixing'], alpha=0.8)\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(x_offset + 1, 2.75, f'Mixing\\nU(B,β{p+1})', ha='center', va='center', \n",
    "               fontsize=9, color='white', fontweight='bold')\n",
    "        \n",
    "        # Arrows\n",
    "        if p == 0:\n",
    "            ax.annotate('', xy=(x_offset, 4), xytext=(2.6, 4),\n",
    "                       arrowprops=dict(arrowstyle='->', color='black', lw=1.5))\n",
    "        else:\n",
    "            ax.annotate('', xy=(x_offset, 4), xytext=(x_offset - 1.4, 4),\n",
    "                       arrowprops=dict(arrowstyle='->', color='black', lw=1.5))\n",
    "        \n",
    "        # Connect cost and mixing\n",
    "        ax.annotate('', xy=(x_offset + 1, 5), xytext=(x_offset + 1, 3.6),\n",
    "                   arrowprops=dict(arrowstyle='->', color='black', lw=1.5))\n",
    "        ax.annotate('', xy=(x_offset + 2.1, 4), xytext=(x_offset + 1, 2),\n",
    "                   arrowprops=dict(arrowstyle='->', color='black', lw=1.5))\n",
    "    \n",
    "    # Output\n",
    "    rect = mpatches.FancyBboxPatch((12.5, 3.5), 1.2, 1, boxstyle=\"round,pad=0.05\",\n",
    "                                   facecolor=colors['output'], alpha=0.8)\n",
    "    ax.add_patch(rect)\n",
    "    ax.text(13.1, 4, 'θ*', ha='center', va='center', fontsize=12, color='white', fontweight='bold')\n",
    "    ax.annotate('', xy=(12.4, 4), xytext=(11.6, 4),\n",
    "               arrowprops=dict(arrowstyle='->', color='black', lw=1.5))\n",
    "    \n",
    "    # Labels\n",
    "    ax.text(7, 7.5, 'QAOA-Inspired Training Loop', ha='center', fontsize=14, fontweight='bold')\n",
    "    ax.text(7, 0.5, 'p layers of alternating Cost (gradient) and Mixing (exploration) operators',\n",
    "           ha='center', fontsize=10, style='italic')\n",
    "    \n",
    "    # Legend\n",
    "    legend_elements = [\n",
    "        mpatches.Patch(facecolor=colors['params'], label='Parameters'),\n",
    "        mpatches.Patch(facecolor=colors['cost'], label='Cost Operator (Gradient)'),\n",
    "        mpatches.Patch(facecolor=colors['mixing'], label='Mixing Operator (Exploration)'),\n",
    "        mpatches.Patch(facecolor=colors['output'], label='Optimized Parameters')\n",
    "    ]\n",
    "    ax.legend(handles=legend_elements, loc='upper right', fontsize=9)\n",
    "    \n",
    "    return fig\n",
    "\n",
    "\n",
    "fig = create_qaoa_diagram()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Cost Operator\n",
    "\n",
    "The cost operator applies gradient-based updates proportional to the loss landscape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Cell: Cost Operator\n",
    "Purpose: Implement the QAOA-inspired cost operator\n",
    "\"\"\"\n",
    "\n",
    "class CostOperator:\n",
    "    \"\"\"\n",
    "    QAOA-inspired cost operator.\n",
    "    \n",
    "    Applies gradient-based parameter updates scaled by gamma.\n",
    "    Analogous to the quantum cost unitary U(C, gamma).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    gamma : float\n",
    "        Cost operator strength (learning rate scale)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, gamma: float = 0.1):\n",
    "        self.gamma = gamma\n",
    "    \n",
    "    def apply(\n",
    "        self,\n",
    "        model: nn.Module,\n",
    "        loss: torch.Tensor,\n",
    "        base_lr: float = 3e-4\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Apply cost operator (gradient descent step).\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        model : nn.Module\n",
    "            Model to update\n",
    "        loss : torch.Tensor\n",
    "            Loss value\n",
    "        base_lr : float\n",
    "            Base learning rate\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Dict[str, torch.Tensor]\n",
    "            Gradient information\n",
    "        \"\"\"\n",
    "        # Compute gradients\n",
    "        gradients = torch.autograd.grad(\n",
    "            loss, model.parameters(),\n",
    "            create_graph=False,\n",
    "            retain_graph=True\n",
    "        )\n",
    "        \n",
    "        # Apply scaled gradient update\n",
    "        effective_lr = base_lr * self.gamma\n",
    "        grad_norm = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for param, grad in zip(model.parameters(), gradients):\n",
    "                if grad is not None:\n",
    "                    param.sub_(effective_lr * grad)\n",
    "                    grad_norm += grad.norm().item() ** 2\n",
    "        \n",
    "        grad_norm = np.sqrt(grad_norm)\n",
    "        \n",
    "        return {'grad_norm': grad_norm, 'effective_lr': effective_lr}\n",
    "\n",
    "\n",
    "# Test cost operator\n",
    "test_model = nn.Linear(10, 5).to(DEVICE)\n",
    "test_input = torch.randn(8, 10, device=DEVICE)\n",
    "test_target = torch.randn(8, 5, device=DEVICE)\n",
    "test_loss = F.mse_loss(test_model(test_input), test_target)\n",
    "\n",
    "cost_op = CostOperator(gamma=1.0)\n",
    "info = cost_op.apply(test_model, test_loss)\n",
    "print(f\"Cost operator applied. Grad norm: {info['grad_norm']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Mixing Operator\n",
    "\n",
    "The mixing operator adds structured exploration to the parameter space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Cell: Mixing Operator\n",
    "Purpose: Implement the QAOA-inspired mixing operator\n",
    "\"\"\"\n",
    "\n",
    "class MixingOperator:\n",
    "    \"\"\"\n",
    "    QAOA-inspired mixing operator.\n",
    "    \n",
    "    Adds structured perturbation to parameters to enable exploration.\n",
    "    Analogous to the quantum mixing unitary U(B, beta).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    beta : float\n",
    "        Mixing strength\n",
    "    noise_type : str\n",
    "        Type of noise ('gaussian', 'uniform', 'sinusoidal')\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, beta: float = 0.1, noise_type: str = 'gaussian'):\n",
    "        self.beta = beta\n",
    "        self.noise_type = noise_type\n",
    "        self._step = 0\n",
    "    \n",
    "    def _generate_noise(self, shape: torch.Size, device: torch.device) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Generate mixing noise.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        shape : torch.Size\n",
    "            Shape of noise tensor\n",
    "        device : torch.device\n",
    "            Device for tensor\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        torch.Tensor\n",
    "            Noise tensor\n",
    "        \"\"\"\n",
    "        if self.noise_type == 'gaussian':\n",
    "            return torch.randn(shape, device=device)\n",
    "        elif self.noise_type == 'uniform':\n",
    "            return 2 * torch.rand(shape, device=device) - 1\n",
    "        elif self.noise_type == 'sinusoidal':\n",
    "            # Structured sinusoidal noise inspired by quantum phase\n",
    "            base = torch.randn(shape, device=device)\n",
    "            phase = self._step * 0.1\n",
    "            return base * np.sin(phase)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown noise type: {self.noise_type}\")\n",
    "    \n",
    "    def apply(self, model: nn.Module) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Apply mixing operator (add perturbation).\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        model : nn.Module\n",
    "            Model to perturb\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Dict[str, float]\n",
    "            Perturbation information\n",
    "        \"\"\"\n",
    "        self._step += 1\n",
    "        total_perturbation = 0.0\n",
    "        num_params = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for param in model.parameters():\n",
    "                noise = self._generate_noise(param.shape, param.device)\n",
    "                perturbation = self.beta * noise\n",
    "                param.add_(perturbation)\n",
    "                total_perturbation += perturbation.abs().sum().item()\n",
    "                num_params += param.numel()\n",
    "        \n",
    "        avg_perturbation = total_perturbation / num_params\n",
    "        \n",
    "        return {'avg_perturbation': avg_perturbation, 'step': self._step}\n",
    "\n",
    "\n",
    "# Test mixing operator\n",
    "mix_op = MixingOperator(beta=0.01, noise_type='gaussian')\n",
    "\n",
    "# Store initial params\n",
    "initial_params = [p.clone() for p in test_model.parameters()]\n",
    "\n",
    "# Apply mixing\n",
    "info = mix_op.apply(test_model)\n",
    "print(f\"Mixing operator applied. Avg perturbation: {info['avg_perturbation']:.6f}\")\n",
    "\n",
    "# Verify params changed\n",
    "param_diff = sum((p1 - p2).abs().sum().item() \n",
    "                 for p1, p2 in zip(initial_params, test_model.parameters()))\n",
    "print(f\"Total parameter change: {param_diff:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. QAOA-Inspired Optimizer\n",
    "\n",
    "Combine cost and mixing operators into a complete QAOA-inspired optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Cell: QAOA Optimizer\n",
    "Purpose: Implement complete QAOA-inspired optimizer\n",
    "\"\"\"\n",
    "\n",
    "class QAOAOptimizer:\n",
    "    \"\"\"\n",
    "    QAOA-Inspired Optimizer for neural network training.\n",
    "    \n",
    "    Alternates between cost (gradient) and mixing (exploration) operators\n",
    "    in a manner inspired by the Quantum Approximate Optimization Algorithm.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    params : iterable\n",
    "        Model parameters\n",
    "    p_layers : int\n",
    "        Number of QAOA layers (alternating cost-mixing pairs)\n",
    "    lr : float\n",
    "        Base learning rate\n",
    "    gamma_init : float\n",
    "        Initial gamma (cost operator strength)\n",
    "    beta_init : float\n",
    "        Initial beta (mixing operator strength)\n",
    "    gamma_schedule : str\n",
    "        Schedule for gamma ('constant', 'linear_decay', 'cosine')\n",
    "    beta_schedule : str\n",
    "        Schedule for beta ('constant', 'linear_decay', 'cosine')\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        params,\n",
    "        p_layers: int = 3,\n",
    "        lr: float = 3e-4,\n",
    "        gamma_init: float = 1.0,\n",
    "        beta_init: float = 0.1,\n",
    "        gamma_schedule: str = 'constant',\n",
    "        beta_schedule: str = 'linear_decay',\n",
    "        total_steps: int = 10000\n",
    "    ):\n",
    "        self.params = list(params)\n",
    "        self.p_layers = p_layers\n",
    "        self.lr = lr\n",
    "        self.gamma_init = gamma_init\n",
    "        self.beta_init = beta_init\n",
    "        self.gamma_schedule = gamma_schedule\n",
    "        self.beta_schedule = beta_schedule\n",
    "        self.total_steps = total_steps\n",
    "        \n",
    "        self._step = 0\n",
    "        \n",
    "        # Learnable QAOA angles (optional advanced feature)\n",
    "        self.gammas = [gamma_init] * p_layers\n",
    "        self.betas = [beta_init] * p_layers\n",
    "    \n",
    "    def _get_scheduled_value(self, init_val: float, schedule: str, layer: int) -> float:\n",
    "        \"\"\"\n",
    "        Get scheduled parameter value.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        init_val : float\n",
    "            Initial value\n",
    "        schedule : str\n",
    "            Schedule type\n",
    "        layer : int\n",
    "            Current layer index\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            Scheduled value\n",
    "        \"\"\"\n",
    "        progress = self._step / max(self.total_steps, 1)\n",
    "        layer_factor = (layer + 1) / self.p_layers  # Later layers have different scaling\n",
    "        \n",
    "        if schedule == 'constant':\n",
    "            return init_val\n",
    "        elif schedule == 'linear_decay':\n",
    "            return init_val * (1.0 - 0.9 * progress)\n",
    "        elif schedule == 'cosine':\n",
    "            return init_val * (0.5 * (1 + np.cos(np.pi * progress)))\n",
    "        elif schedule == 'layer_adaptive':\n",
    "            return init_val * layer_factor\n",
    "        else:\n",
    "            return init_val\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        \"\"\"Zero all gradients.\"\"\"\n",
    "        for param in self.params:\n",
    "            if param.grad is not None:\n",
    "                param.grad.zero_()\n",
    "    \n",
    "    def step(\n",
    "        self,\n",
    "        loss_fn: Callable[[], torch.Tensor],\n",
    "        model: nn.Module\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Perform one QAOA-inspired optimization step.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        loss_fn : Callable\n",
    "            Function that computes loss (will be called multiple times)\n",
    "        model : nn.Module\n",
    "            Model being optimized\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Dict[str, Any]\n",
    "            Optimization statistics\n",
    "        \"\"\"\n",
    "        self._step += 1\n",
    "        stats = {\n",
    "            'step': self._step,\n",
    "            'gammas': [],\n",
    "            'betas': [],\n",
    "            'layer_losses': []\n",
    "        }\n",
    "        \n",
    "        # Apply p layers of alternating operators\n",
    "        for p in range(self.p_layers):\n",
    "            # Get scheduled parameters\n",
    "            gamma = self._get_scheduled_value(self.gammas[p], self.gamma_schedule, p)\n",
    "            beta = self._get_scheduled_value(self.betas[p], self.beta_schedule, p)\n",
    "            \n",
    "            stats['gammas'].append(gamma)\n",
    "            stats['betas'].append(beta)\n",
    "            \n",
    "            # Cost operator (gradient step)\n",
    "            loss = loss_fn()\n",
    "            stats['layer_losses'].append(loss.item())\n",
    "            \n",
    "            # Compute gradients\n",
    "            grads = torch.autograd.grad(\n",
    "                loss, model.parameters(),\n",
    "                create_graph=False,\n",
    "                retain_graph=False\n",
    "            )\n",
    "            \n",
    "            # Apply cost operator (gradient descent)\n",
    "            effective_lr = self.lr * gamma\n",
    "            with torch.no_grad():\n",
    "                for param, grad in zip(model.parameters(), grads):\n",
    "                    if grad is not None:\n",
    "                        # Gradient clipping\n",
    "                        grad = torch.clamp(grad, -1.0, 1.0)\n",
    "                        param.sub_(effective_lr * grad)\n",
    "            \n",
    "            # Mixing operator (exploration)\n",
    "            if beta > 0:\n",
    "                with torch.no_grad():\n",
    "                    for param in model.parameters():\n",
    "                        noise = torch.randn_like(param) * beta\n",
    "                        param.add_(noise)\n",
    "        \n",
    "        # Final loss\n",
    "        with torch.no_grad():\n",
    "            final_loss = loss_fn()\n",
    "        stats['final_loss'] = final_loss.item()\n",
    "        \n",
    "        return stats\n",
    "\n",
    "\n",
    "print(\"QAOA Optimizer defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Enhanced Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Cell: Loss Functions\n",
    "Purpose: World model loss computation\n",
    "\"\"\"\n",
    "\n",
    "def compute_world_model_loss(\n",
    "    model_output: Dict[str, torch.Tensor],\n",
    "    obs_seq: torch.Tensor,\n",
    "    reward_seq: torch.Tensor,\n",
    "    continue_seq: torch.Tensor,\n",
    "    kl_weight: float = 1.0,\n",
    "    free_nats: float = 1.0\n",
    ") -> Dict[str, torch.Tensor]:\n",
    "    \"\"\"Compute world model training loss.\"\"\"\n",
    "    recon_loss = F.mse_loss(model_output['recon_obs'], obs_seq)\n",
    "    reward_loss = F.mse_loss(model_output['pred_rewards'], reward_seq)\n",
    "    continue_loss = F.binary_cross_entropy_with_logits(\n",
    "        model_output['pred_continues'], continue_seq\n",
    "    )\n",
    "    \n",
    "    kl_losses = []\n",
    "    for prior, posterior in zip(model_output['priors'], model_output['posteriors']):\n",
    "        kl = torch.distributions.kl_divergence(posterior, prior).sum(-1)\n",
    "        kl = torch.clamp(kl, min=free_nats).mean()\n",
    "        kl_losses.append(kl)\n",
    "    kl_loss = torch.stack(kl_losses).mean()\n",
    "    \n",
    "    total_loss = kl_weight * kl_loss + recon_loss + reward_loss + continue_loss\n",
    "    \n",
    "    return {\n",
    "        'total': total_loss, 'kl': kl_loss,\n",
    "        'recon': recon_loss, 'reward': reward_loss, 'continue': continue_loss\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Cell: Replay Buffer\n",
    "Purpose: Episode storage and sampling\n",
    "\"\"\"\n",
    "\n",
    "@dataclass\n",
    "class Episode:\n",
    "    observations: np.ndarray\n",
    "    actions: np.ndarray\n",
    "    rewards: np.ndarray\n",
    "    dones: np.ndarray\n",
    "    def __len__(self): return len(self.observations)\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity: int = 1000):\n",
    "        self.capacity = capacity\n",
    "        self.episodes = []\n",
    "        self.total_steps = 0\n",
    "    \n",
    "    def add_episode(self, episode: Episode):\n",
    "        if len(self.episodes) >= self.capacity:\n",
    "            self.total_steps -= len(self.episodes.pop(0))\n",
    "        self.episodes.append(episode)\n",
    "        self.total_steps += len(episode)\n",
    "    \n",
    "    def sample_sequences(self, batch_size: int, seq_len: int):\n",
    "        valid = [ep for ep in self.episodes if len(ep) >= seq_len]\n",
    "        obs_b, act_b, rew_b, cont_b = [], [], [], []\n",
    "        for _ in range(batch_size):\n",
    "            ep = valid[np.random.randint(len(valid))]\n",
    "            start = np.random.randint(0, len(ep) - seq_len + 1)\n",
    "            obs_b.append(ep.observations[start:start+seq_len])\n",
    "            act_b.append(ep.actions[start:start+seq_len])\n",
    "            rew_b.append(ep.rewards[start:start+seq_len])\n",
    "            cont_b.append(1.0 - ep.dones[start:start+seq_len])\n",
    "        return np.stack(obs_b), np.stack(act_b), np.stack(rew_b), np.stack(cont_b)\n",
    "    \n",
    "    def __len__(self): return len(self.episodes)\n",
    "\n",
    "\n",
    "def collect_episodes(env_name: str, num_episodes: int, seed: int = 42):\n",
    "    env = gym.make(env_name)\n",
    "    episodes = []\n",
    "    for ep_idx in tqdm(range(num_episodes), desc=\"Collecting\"):\n",
    "        obs_l, act_l, rew_l, done_l = [], [], [], []\n",
    "        obs, _ = env.reset(seed=seed+ep_idx)\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = env.action_space.sample()\n",
    "            obs_l.append(obs)\n",
    "            if isinstance(env.action_space, gym.spaces.Discrete):\n",
    "                act_oh = np.zeros(env.action_space.n)\n",
    "                act_oh[action] = 1.0\n",
    "                act_l.append(act_oh)\n",
    "            else:\n",
    "                act_l.append(action)\n",
    "            obs, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            rew_l.append(reward)\n",
    "            done_l.append(float(done))\n",
    "        episodes.append(Episode(\n",
    "            np.array(obs_l, dtype=np.float32),\n",
    "            np.array(act_l, dtype=np.float32),\n",
    "            np.array(rew_l, dtype=np.float32),\n",
    "            np.array(done_l, dtype=np.float32)\n",
    "        ))\n",
    "    env.close()\n",
    "    return episodes\n",
    "\n",
    "\n",
    "# Collect data\n",
    "print(\"Collecting training data...\")\n",
    "episodes = collect_episodes(\"CartPole-v1\", num_episodes=100, seed=SEED)\n",
    "buffer = ReplayBuffer(capacity=1000)\n",
    "for ep in episodes:\n",
    "    buffer.add_episode(ep)\n",
    "print(f\"Buffer: {len(buffer)} episodes, {buffer.total_steps} steps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Cell: QAOA Trainer\n",
    "Purpose: Training loop with QAOA optimizer\n",
    "\"\"\"\n",
    "\n",
    "class QAOAWorldModelTrainer:\n",
    "    \"\"\"\n",
    "    Trainer using QAOA-inspired optimization.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model : WorldModel\n",
    "        World model to train\n",
    "    replay_buffer : ReplayBuffer\n",
    "        Experience buffer\n",
    "    p_layers : int\n",
    "        Number of QAOA layers\n",
    "    lr : float\n",
    "        Learning rate\n",
    "    gamma_init : float\n",
    "        Initial gamma\n",
    "    beta_init : float\n",
    "        Initial beta\n",
    "    device : torch.device\n",
    "        Compute device\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model: WorldModel,\n",
    "        replay_buffer: ReplayBuffer,\n",
    "        p_layers: int = 3,\n",
    "        lr: float = 3e-4,\n",
    "        gamma_init: float = 1.0,\n",
    "        beta_init: float = 0.05,\n",
    "        total_steps: int = 10000,\n",
    "        device: torch.device = DEVICE\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.replay_buffer = replay_buffer\n",
    "        self.device = device\n",
    "        \n",
    "        self.qaoa_optimizer = QAOAOptimizer(\n",
    "            model.parameters(),\n",
    "            p_layers=p_layers,\n",
    "            lr=lr,\n",
    "            gamma_init=gamma_init,\n",
    "            beta_init=beta_init,\n",
    "            gamma_schedule='constant',\n",
    "            beta_schedule='linear_decay',\n",
    "            total_steps=total_steps\n",
    "        )\n",
    "        \n",
    "        self.logger = MetricLogger(name=\"qaoa_world_model\")\n",
    "    \n",
    "    def train_step(\n",
    "        self,\n",
    "        batch_size: int = 32,\n",
    "        seq_len: int = 20,\n",
    "        kl_weight: float = 1.0\n",
    "    ) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Perform one QAOA training step.\n",
    "        \"\"\"\n",
    "        self.model.train()\n",
    "        \n",
    "        # Sample batch\n",
    "        obs, actions, rewards, continues = self.replay_buffer.sample_sequences(\n",
    "            batch_size, seq_len\n",
    "        )\n",
    "        obs = torch.tensor(obs, dtype=torch.float32, device=self.device)\n",
    "        actions = torch.tensor(actions, dtype=torch.float32, device=self.device)\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32, device=self.device)\n",
    "        continues = torch.tensor(continues, dtype=torch.float32, device=self.device)\n",
    "        \n",
    "        # Define loss function for QAOA optimizer\n",
    "        def loss_fn():\n",
    "            output = self.model(obs, actions)\n",
    "            losses = compute_world_model_loss(\n",
    "                output, obs, rewards, continues, kl_weight=kl_weight\n",
    "            )\n",
    "            return losses['total']\n",
    "        \n",
    "        # QAOA optimization step\n",
    "        qaoa_stats = self.qaoa_optimizer.step(loss_fn, self.model)\n",
    "        \n",
    "        # Get final losses for logging\n",
    "        with torch.no_grad():\n",
    "            output = self.model(obs, actions)\n",
    "            losses = compute_world_model_loss(\n",
    "                output, obs, rewards, continues, kl_weight=kl_weight\n",
    "            )\n",
    "        \n",
    "        result = {k: v.item() for k, v in losses.items()}\n",
    "        result['p_layers'] = len(qaoa_stats['gammas'])\n",
    "        result['avg_gamma'] = np.mean(qaoa_stats['gammas'])\n",
    "        result['avg_beta'] = np.mean(qaoa_stats['betas'])\n",
    "        \n",
    "        self.logger.log(**result)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def train(\n",
    "        self,\n",
    "        num_steps: int = 5000,\n",
    "        batch_size: int = 32,\n",
    "        seq_len: int = 20,\n",
    "        log_every: int = 100,\n",
    "        kl_weight: float = 1.0\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"Train the world model with QAOA optimization.\"\"\"\n",
    "        pbar = tqdm(range(num_steps), desc=\"QAOA Training\")\n",
    "        \n",
    "        for step in pbar:\n",
    "            losses = self.train_step(batch_size, seq_len, kl_weight)\n",
    "            \n",
    "            if step % log_every == 0:\n",
    "                pbar.set_postfix({\n",
    "                    'total': f\"{self.logger.get_mean('total', 100):.4f}\",\n",
    "                    'recon': f\"{self.logger.get_mean('recon', 100):.4f}\",\n",
    "                    'beta': f\"{self.logger.get_mean('avg_beta', 100):.4f}\"\n",
    "                })\n",
    "        \n",
    "        return self.logger.to_dataframe()\n",
    "\n",
    "\n",
    "print(\"QAOA Trainer defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Cell: Train QAOA Model\n",
    "Purpose: Train world model with QAOA optimizer\n",
    "\"\"\"\n",
    "\n",
    "# Create world model\n",
    "set_seed(SEED)  # Reset seed for fair comparison\n",
    "\n",
    "qaoa_model = WorldModel(\n",
    "    obs_dim=4,\n",
    "    action_dim=2,\n",
    "    stoch_dim=32,\n",
    "    deter_dim=128,\n",
    "    hidden_dim=128,\n",
    "    encoder_hidden=[128, 128],\n",
    "    decoder_hidden=[128, 128],\n",
    "    predictor_hidden=[128, 128]\n",
    ").to(DEVICE)\n",
    "\n",
    "print(f\"QAOA World Model Parameters: {sum(p.numel() for p in qaoa_model.parameters()):,}\")\n",
    "\n",
    "# Create QAOA trainer\n",
    "qaoa_trainer = QAOAWorldModelTrainer(\n",
    "    model=qaoa_model,\n",
    "    replay_buffer=buffer,\n",
    "    p_layers=3,\n",
    "    lr=3e-4,\n",
    "    gamma_init=1.0,\n",
    "    beta_init=0.05,\n",
    "    total_steps=5000,\n",
    "    device=DEVICE\n",
    ")\n",
    "\n",
    "# Train\n",
    "print(\"\\nTraining with QAOA optimizer...\")\n",
    "qaoa_history = qaoa_trainer.train(\n",
    "    num_steps=5000,\n",
    "    batch_size=32,\n",
    "    seq_len=20,\n",
    "    log_every=100,\n",
    "    kl_weight=1.0\n",
    ")\n",
    "\n",
    "print(f\"\\nQAOA Training complete!\")\n",
    "print(f\"  Final total loss: {qaoa_history['total'].iloc[-100:].mean():.4f}\")\n",
    "print(f\"  Final recon loss: {qaoa_history['recon'].iloc[-100:].mean():.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Cell: Train Baseline for Comparison\n",
    "Purpose: Train classical baseline for fair comparison\n",
    "\"\"\"\n",
    "\n",
    "# Classical baseline trainer (from Phase 2)\n",
    "class ClassicalTrainer:\n",
    "    def __init__(self, model, replay_buffer, lr=3e-4, device=DEVICE):\n",
    "        self.model = model\n",
    "        self.replay_buffer = replay_buffer\n",
    "        self.device = device\n",
    "        self.optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "        self.logger = MetricLogger(name=\"classical\")\n",
    "    \n",
    "    def train_step(self, batch_size=32, seq_len=20, kl_weight=1.0):\n",
    "        self.model.train()\n",
    "        obs, actions, rewards, continues = self.replay_buffer.sample_sequences(batch_size, seq_len)\n",
    "        obs = torch.tensor(obs, dtype=torch.float32, device=self.device)\n",
    "        actions = torch.tensor(actions, dtype=torch.float32, device=self.device)\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32, device=self.device)\n",
    "        continues = torch.tensor(continues, dtype=torch.float32, device=self.device)\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        output = self.model(obs, actions)\n",
    "        losses = compute_world_model_loss(output, obs, rewards, continues, kl_weight=kl_weight)\n",
    "        losses['total'].backward()\n",
    "        nn.utils.clip_grad_norm_(self.model.parameters(), 100.0)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        result = {k: v.item() for k, v in losses.items()}\n",
    "        self.logger.log(**result)\n",
    "        return result\n",
    "    \n",
    "    def train(self, num_steps=5000, batch_size=32, seq_len=20, log_every=100, kl_weight=1.0):\n",
    "        pbar = tqdm(range(num_steps), desc=\"Classical Training\")\n",
    "        for step in pbar:\n",
    "            self.train_step(batch_size, seq_len, kl_weight)\n",
    "            if step % log_every == 0:\n",
    "                pbar.set_postfix({\n",
    "                    'total': f\"{self.logger.get_mean('total', 100):.4f}\",\n",
    "                    'recon': f\"{self.logger.get_mean('recon', 100):.4f}\"\n",
    "                })\n",
    "        return self.logger.to_dataframe()\n",
    "\n",
    "\n",
    "# Create and train classical baseline\n",
    "set_seed(SEED)  # Same seed for fair comparison\n",
    "\n",
    "baseline_model = WorldModel(\n",
    "    obs_dim=4, action_dim=2, stoch_dim=32, deter_dim=128, hidden_dim=128,\n",
    "    encoder_hidden=[128, 128], decoder_hidden=[128, 128], predictor_hidden=[128, 128]\n",
    ").to(DEVICE)\n",
    "\n",
    "baseline_trainer = ClassicalTrainer(baseline_model, buffer, lr=3e-4, device=DEVICE)\n",
    "\n",
    "print(\"\\nTraining classical baseline...\")\n",
    "baseline_history = baseline_trainer.train(\n",
    "    num_steps=5000, batch_size=32, seq_len=20, log_every=100, kl_weight=1.0\n",
    ")\n",
    "\n",
    "print(f\"\\nBaseline Training complete!\")\n",
    "print(f\"  Final total loss: {baseline_history['total'].iloc[-100:].mean():.4f}\")\n",
    "print(f\"  Final recon loss: {baseline_history['recon'].iloc[-100:].mean():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Comparison with Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Cell: Compare Methods\n",
    "Purpose: Statistical comparison of QAOA vs baseline\n",
    "\"\"\"\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "def compare_methods(history1: pd.DataFrame, history2: pd.DataFrame,\n",
    "                   name1: str = \"Method 1\", name2: str = \"Method 2\",\n",
    "                   window: int = 100) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Statistically compare two training methods.\n",
    "    \"\"\"\n",
    "    # Get final losses (last window steps)\n",
    "    final1 = history1['total'].iloc[-window:].values\n",
    "    final2 = history2['total'].iloc[-window:].values\n",
    "    \n",
    "    # Mann-Whitney U test\n",
    "    stat, p_value = stats.mannwhitneyu(final1, final2, alternative='two-sided')\n",
    "    \n",
    "    # Effect size (Cohen's d)\n",
    "    pooled_std = np.sqrt((final1.std()**2 + final2.std()**2) / 2)\n",
    "    cohens_d = (final1.mean() - final2.mean()) / pooled_std if pooled_std > 0 else 0\n",
    "    \n",
    "    # Convergence speed (steps to reach 90% of final performance)\n",
    "    def steps_to_convergence(hist, threshold_pct=0.9):\n",
    "        final_loss = hist['total'].iloc[-100:].mean()\n",
    "        threshold = final_loss / threshold_pct\n",
    "        smoothed = hist['total'].rolling(50).mean()\n",
    "        crossed = smoothed < threshold\n",
    "        if crossed.any():\n",
    "            return crossed.idxmax()\n",
    "        return len(hist)\n",
    "    \n",
    "    conv1 = steps_to_convergence(history1)\n",
    "    conv2 = steps_to_convergence(history2)\n",
    "    \n",
    "    return {\n",
    "        'method1': name1,\n",
    "        'method2': name2,\n",
    "        'mean_loss_1': final1.mean(),\n",
    "        'mean_loss_2': final2.mean(),\n",
    "        'std_loss_1': final1.std(),\n",
    "        'std_loss_2': final2.std(),\n",
    "        'p_value': p_value,\n",
    "        'cohens_d': cohens_d,\n",
    "        'significant': p_value < 0.05,\n",
    "        'convergence_steps_1': conv1,\n",
    "        'convergence_steps_2': conv2,\n",
    "        'speedup': conv2 / conv1 if conv1 > 0 else 1.0\n",
    "    }\n",
    "\n",
    "\n",
    "# Compare methods\n",
    "comparison = compare_methods(qaoa_history, baseline_history, \"QAOA\", \"Baseline\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMPARISON: QAOA vs Classical Baseline\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nFinal Loss (last 100 steps):\")\n",
    "print(f\"  QAOA:     {comparison['mean_loss_1']:.4f} ± {comparison['std_loss_1']:.4f}\")\n",
    "print(f\"  Baseline: {comparison['mean_loss_2']:.4f} ± {comparison['std_loss_2']:.4f}\")\n",
    "print(f\"\\nStatistical Significance:\")\n",
    "print(f\"  Mann-Whitney U p-value: {comparison['p_value']:.4f}\")\n",
    "print(f\"  Cohen's d: {comparison['cohens_d']:.3f}\")\n",
    "print(f\"  Significant (p<0.05): {comparison['significant']}\")\n",
    "print(f\"\\nConvergence:\")\n",
    "print(f\"  QAOA steps to 90%: {comparison['convergence_steps_1']}\")\n",
    "print(f\"  Baseline steps to 90%: {comparison['convergence_steps_2']}\")\n",
    "print(f\"  Speedup: {comparison['speedup']:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Cell: Comparison Plots\n",
    "Purpose: Visualize QAOA vs baseline comparison\n",
    "\"\"\"\n",
    "\n",
    "def plot_comparison(\n",
    "    qaoa_hist: pd.DataFrame,\n",
    "    baseline_hist: pd.DataFrame,\n",
    "    figsize: Tuple[int, int] = (14, 10)\n",
    "):\n",
    "    \"\"\"Plot comparison of QAOA and baseline training.\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=figsize)\n",
    "    window = 50\n",
    "    \n",
    "    # Total loss comparison\n",
    "    ax = axes[0, 0]\n",
    "    ax.plot(qaoa_hist['total'].rolling(window).mean(), \n",
    "           color=COLORS['qaoa'], label='QAOA', linewidth=2)\n",
    "    ax.plot(baseline_hist['total'].rolling(window).mean(),\n",
    "           color=COLORS['baseline'], label='Baseline', linewidth=2)\n",
    "    ax.set_xlabel('Step')\n",
    "    ax.set_ylabel('Loss')\n",
    "    ax.set_title('Total Loss')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Reconstruction loss\n",
    "    ax = axes[0, 1]\n",
    "    ax.plot(qaoa_hist['recon'].rolling(window).mean(),\n",
    "           color=COLORS['qaoa'], label='QAOA', linewidth=2)\n",
    "    ax.plot(baseline_hist['recon'].rolling(window).mean(),\n",
    "           color=COLORS['baseline'], label='Baseline', linewidth=2)\n",
    "    ax.set_xlabel('Step')\n",
    "    ax.set_ylabel('Loss')\n",
    "    ax.set_title('Reconstruction Loss')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # KL loss\n",
    "    ax = axes[1, 0]\n",
    "    ax.plot(qaoa_hist['kl'].rolling(window).mean(),\n",
    "           color=COLORS['qaoa'], label='QAOA', linewidth=2)\n",
    "    ax.plot(baseline_hist['kl'].rolling(window).mean(),\n",
    "           color=COLORS['baseline'], label='Baseline', linewidth=2)\n",
    "    ax.set_xlabel('Step')\n",
    "    ax.set_ylabel('Loss')\n",
    "    ax.set_title('KL Divergence Loss')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # QAOA parameters\n",
    "    ax = axes[1, 1]\n",
    "    if 'avg_beta' in qaoa_hist.columns:\n",
    "        ax.plot(qaoa_hist['avg_beta'].rolling(window).mean(),\n",
    "               color=COLORS['superposition'], label='Beta (mixing)', linewidth=2)\n",
    "    if 'avg_gamma' in qaoa_hist.columns:\n",
    "        ax.plot(qaoa_hist['avg_gamma'].rolling(window).mean(),\n",
    "               color=COLORS['gates'], label='Gamma (cost)', linewidth=2)\n",
    "    ax.set_xlabel('Step')\n",
    "    ax.set_ylabel('Value')\n",
    "    ax.set_title('QAOA Parameters (Scheduled)')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    fig.suptitle('QAOA vs Classical Baseline Comparison', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "\n",
    "fig = plot_comparison(qaoa_history, baseline_history)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Cell: Convergence Analysis\n",
    "Purpose: Analyze convergence speed\n",
    "\"\"\"\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Cumulative minimum loss (best so far)\n",
    "qaoa_cummin = qaoa_history['total'].rolling(50).mean().cummin()\n",
    "baseline_cummin = baseline_history['total'].rolling(50).mean().cummin()\n",
    "\n",
    "ax.plot(qaoa_cummin, color=COLORS['qaoa'], label='QAOA', linewidth=2)\n",
    "ax.plot(baseline_cummin, color=COLORS['baseline'], label='Baseline', linewidth=2)\n",
    "\n",
    "ax.set_xlabel('Training Step')\n",
    "ax.set_ylabel('Best Loss So Far')\n",
    "ax.set_title('Convergence Analysis: Cumulative Minimum Loss')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Cell: Save Results\n",
    "Purpose: Save QAOA model and comparison results\n",
    "\"\"\"\n",
    "\n",
    "# Save results\n",
    "results_dir = PROJECT_ROOT / \"experiments\" / \"results\" / \"qaoa\"\n",
    "results_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save model\n",
    "checkpoint = {\n",
    "    'model_state_dict': qaoa_model.state_dict(),\n",
    "    'config': {\n",
    "        'p_layers': 3,\n",
    "        'gamma_init': 1.0,\n",
    "        'beta_init': 0.05,\n",
    "        'obs_dim': 4,\n",
    "        'action_dim': 2\n",
    "    },\n",
    "    'comparison': comparison\n",
    "}\n",
    "torch.save(checkpoint, results_dir / \"cartpole_qaoa.pt\")\n",
    "\n",
    "# Save histories\n",
    "qaoa_history.to_csv(results_dir / \"qaoa_training_history.csv\", index=False)\n",
    "\n",
    "print(f\"Results saved to: {results_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Cell: Summary\n",
    "Purpose: Display phase summary\n",
    "\"\"\"\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"PHASE 3 COMPLETE: QAOA-ENHANCED TRAINING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n[1] Components Implemented\")\n",
    "print(\"    - CostOperator: Gradient-based updates (analog to U(C,γ))\")\n",
    "print(\"    - MixingOperator: Exploration perturbations (analog to U(B,β))\")\n",
    "print(\"    - QAOAOptimizer: Full QAOA-inspired training loop\")\n",
    "print(\"    - QAOAWorldModelTrainer: Integration with world model\")\n",
    "\n",
    "print(\"\\n[2] QAOA Configuration\")\n",
    "print(f\"    - p_layers: 3\")\n",
    "print(f\"    - gamma_init: 1.0\")\n",
    "print(f\"    - beta_init: 0.05\")\n",
    "print(f\"    - beta_schedule: linear_decay\")\n",
    "\n",
    "print(\"\\n[3] Comparison Results\")\n",
    "print(f\"    - QAOA Final Loss: {comparison['mean_loss_1']:.4f}\")\n",
    "print(f\"    - Baseline Final Loss: {comparison['mean_loss_2']:.4f}\")\n",
    "print(f\"    - Statistical Significance: p={comparison['p_value']:.4f}\")\n",
    "print(f\"    - Effect Size (Cohen's d): {comparison['cohens_d']:.3f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"NEXT STEPS: Phase 4 - Superposition-Enhanced Experience Replay\")\n",
    "print(\"=\"*60)\n",
    "print(\"\"\"\n",
    "In Phase 4, we will:\n",
    "  1. Implement quantum-inspired weighted experience sampling\n",
    "  2. Create amplitude-based prioritization\n",
    "  3. Add interference effects for better sample efficiency\n",
    "  4. Compare with standard replay buffer\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
