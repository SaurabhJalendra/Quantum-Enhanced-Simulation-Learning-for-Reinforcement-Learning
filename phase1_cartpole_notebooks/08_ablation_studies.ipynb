{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 8: Ablation Studies\n",
    "\n",
    "**Quantum-Enhanced Simulation Learning for Reinforcement Learning**\n",
    "\n",
    "Author: Saurabh Jalendra  \n",
    "Institution: BITS Pilani (WILP Division)  \n",
    "Date: November 2025\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook performs **systematic ablation studies** to understand the contribution\n",
    "of individual components in each quantum-inspired approach.\n",
    "\n",
    "### Ablation Categories\n",
    "\n",
    "1. **QAOA Ablations**: Cost vs Mixing operators, number of layers (p), scheduling\n",
    "2. **Superposition Ablations**: Amplitude weighting, interference effects, prioritization\n",
    "3. **Gate Ablations**: Individual gate types, number of layers, residual connections\n",
    "4. **Error Correction Ablations**: Ensemble size, correction method, diversity weight\n",
    "\n",
    "### Methodology\n",
    "\n",
    "For each ablation:\n",
    "1. Remove/modify a single component\n",
    "2. Train with identical configuration\n",
    "3. Compare performance to full method\n",
    "4. Quantify component contribution\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1 Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "project_root = Path.cwd().parent\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "import math\n",
    "import time\n",
    "from typing import Dict, List, Tuple, Optional, Callable\n",
    "from dataclasses import dataclass, field\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import gymnasium as gym\n",
    "\n",
    "from src.utils import set_seed, get_device, MetricLogger, Timer, COLORS\n",
    "\n",
    "set_seed(42)\n",
    "device = get_device()\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2 Base Model and Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseWorldModel(nn.Module):\n",
    "    \"\"\"Base world model for ablation studies.\"\"\"\n",
    "    \n",
    "    def __init__(self, obs_dim, action_dim, hidden_dim=512, deter_dim=512, stoch_dim=64):\n",
    "        super().__init__()\n",
    "        self.obs_dim = obs_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.deter_dim = deter_dim\n",
    "        self.stoch_dim = stoch_dim\n",
    "        self.state_dim = deter_dim + stoch_dim\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(obs_dim, hidden_dim), nn.LayerNorm(hidden_dim), nn.ELU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2), nn.ELU()\n",
    "        )\n",
    "        \n",
    "        self.gru = nn.GRUCell(stoch_dim + action_dim, deter_dim)\n",
    "        self.prior = nn.Sequential(\n",
    "            nn.Linear(deter_dim, hidden_dim // 2), nn.ELU(),\n",
    "            nn.Linear(hidden_dim // 2, stoch_dim * 2)\n",
    "        )\n",
    "        self.posterior = nn.Sequential(\n",
    "            nn.Linear(deter_dim + hidden_dim // 2, hidden_dim // 2), nn.ELU(),\n",
    "            nn.Linear(hidden_dim // 2, stoch_dim * 2)\n",
    "        )\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(self.state_dim, hidden_dim), nn.ELU(),\n",
    "            nn.Linear(hidden_dim, obs_dim * 2)\n",
    "        )\n",
    "        self.reward_pred = nn.Sequential(\n",
    "            nn.Linear(self.state_dim, hidden_dim // 2), nn.ELU(),\n",
    "            nn.Linear(hidden_dim // 2, 1)\n",
    "        )\n",
    "    \n",
    "    def initial_state(self, batch_size, device):\n",
    "        return {\n",
    "            'deter': torch.zeros(batch_size, self.deter_dim, device=device),\n",
    "            'stoch': torch.zeros(batch_size, self.stoch_dim, device=device)\n",
    "        }\n",
    "    \n",
    "    def forward(self, obs_seq, action_seq):\n",
    "        batch_size, seq_len = obs_seq.shape[:2]\n",
    "        device = obs_seq.device\n",
    "        state = self.initial_state(batch_size, device)\n",
    "        \n",
    "        states, prior_means, prior_stds, post_means, post_stds = [], [], [], [], []\n",
    "        \n",
    "        for t in range(seq_len):\n",
    "            embed = self.encoder(obs_seq[:, t])\n",
    "            gru_input = torch.cat([state['stoch'], action_seq[:, t]], dim=-1)\n",
    "            deter = self.gru(gru_input, state['deter'])\n",
    "            \n",
    "            prior_stats = self.prior(deter)\n",
    "            prior_mean, prior_log_std = torch.chunk(prior_stats, 2, dim=-1)\n",
    "            prior_std = F.softplus(prior_log_std) + 0.1\n",
    "            \n",
    "            post_input = torch.cat([deter, embed], dim=-1)\n",
    "            post_stats = self.posterior(post_input)\n",
    "            post_mean, post_log_std = torch.chunk(post_stats, 2, dim=-1)\n",
    "            post_std = F.softplus(post_log_std) + 0.1\n",
    "            \n",
    "            stoch = post_mean + post_std * torch.randn_like(post_std)\n",
    "            state = {'deter': deter, 'stoch': stoch}\n",
    "            \n",
    "            states.append(torch.cat([deter, stoch], dim=-1))\n",
    "            prior_means.append(prior_mean)\n",
    "            prior_stds.append(prior_std)\n",
    "            post_means.append(post_mean)\n",
    "            post_stds.append(post_std)\n",
    "        \n",
    "        states = torch.stack(states, dim=1)\n",
    "        flat_states = states.reshape(-1, self.state_dim)\n",
    "        \n",
    "        dec_output = self.decoder(flat_states)\n",
    "        obs_mean, obs_log_std = torch.chunk(dec_output, 2, dim=-1)\n",
    "        obs_mean = obs_mean.reshape(batch_size, seq_len, -1)\n",
    "        obs_log_std = obs_log_std.clamp(-10, 2).reshape(batch_size, seq_len, -1)\n",
    "        \n",
    "        reward_pred = self.reward_pred(flat_states).reshape(batch_size, seq_len)\n",
    "        \n",
    "        return {\n",
    "            'states': states, 'obs_mean': obs_mean, 'obs_log_std': obs_log_std,\n",
    "            'reward_pred': reward_pred,\n",
    "            'prior_mean': torch.stack(prior_means, dim=1),\n",
    "            'prior_std': torch.stack(prior_stds, dim=1),\n",
    "            'post_mean': torch.stack(post_means, dim=1),\n",
    "            'post_std': torch.stack(post_stds, dim=1)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_data(env_name='CartPole-v1', num_episodes=100, max_steps=200):\n",
    "    \"\"\"Collect episodes from environment.\"\"\"\n",
    "    env = gym.make(env_name)\n",
    "    episodes = []\n",
    "    \n",
    "    for _ in range(num_episodes):\n",
    "        obs_list, action_list, reward_list = [], [], []\n",
    "        obs, _ = env.reset()\n",
    "        obs_list.append(obs)\n",
    "        \n",
    "        for _ in range(max_steps):\n",
    "            action = env.action_space.sample()\n",
    "            next_obs, reward, terminated, truncated, _ = env.step(action)\n",
    "            \n",
    "            action_list.append([float(action)] if isinstance(action, (int, np.integer)) else action)\n",
    "            reward_list.append(reward)\n",
    "            obs_list.append(next_obs)\n",
    "            \n",
    "            if terminated or truncated:\n",
    "                break\n",
    "            obs = next_obs\n",
    "        \n",
    "        obs_list = obs_list[:-1]\n",
    "        if len(obs_list) > 10:\n",
    "            episodes.append({\n",
    "                'obs': np.array(obs_list, dtype=np.float32),\n",
    "                'actions': np.array(action_list, dtype=np.float32),\n",
    "                'rewards': np.array(reward_list, dtype=np.float32)\n",
    "            })\n",
    "    \n",
    "    env.close()\n",
    "    return episodes\n",
    "\n",
    "\n",
    "def create_batches(episodes, batch_size=32, seq_len=20):\n",
    "    \"\"\"Create training batches.\"\"\"\n",
    "    sequences = []\n",
    "    for ep in episodes:\n",
    "        ep_len = len(ep['obs'])\n",
    "        for start in range(0, ep_len - seq_len, seq_len // 2):\n",
    "            sequences.append({\n",
    "                'obs': ep['obs'][start:start+seq_len],\n",
    "                'actions': ep['actions'][start:start+seq_len],\n",
    "                'rewards': ep['rewards'][start:start+seq_len]\n",
    "            })\n",
    "    \n",
    "    np.random.shuffle(sequences)\n",
    "    batches = []\n",
    "    \n",
    "    for i in range(0, len(sequences) - batch_size, batch_size):\n",
    "        batch_seqs = sequences[i:i+batch_size]\n",
    "        batches.append((\n",
    "            torch.tensor(np.stack([s['obs'] for s in batch_seqs]), dtype=torch.float32, device=device),\n",
    "            torch.tensor(np.stack([s['actions'] for s in batch_seqs]), dtype=torch.float32, device=device),\n",
    "            torch.tensor(np.stack([s['rewards'] for s in batch_seqs]), dtype=torch.float32, device=device)\n",
    "        ))\n",
    "    \n",
    "    return batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.3 QAOA Ablations\n",
    "\n",
    "Study the contribution of cost operator, mixing operator, and number of layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QAOAAblationTrainer:\n",
    "    \"\"\"\n",
    "    QAOA trainer with configurable ablations.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model : nn.Module\n",
    "        World model\n",
    "    use_cost : bool\n",
    "        Enable cost operator\n",
    "    use_mixing : bool\n",
    "        Enable mixing operator\n",
    "    p : int\n",
    "        Number of QAOA layers\n",
    "    use_scheduling : bool\n",
    "        Enable parameter scheduling\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        lr=3e-4,\n",
    "        gamma=0.5,\n",
    "        beta=0.1,\n",
    "        use_cost=True,\n",
    "        use_mixing=True,\n",
    "        p=3,\n",
    "        use_scheduling=True\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.gamma = gamma\n",
    "        self.beta = beta\n",
    "        self.use_cost = use_cost\n",
    "        self.use_mixing = use_mixing\n",
    "        self.p = p\n",
    "        self.use_scheduling = use_scheduling\n",
    "        self.optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "        self.step_count = 0\n",
    "    \n",
    "    def compute_loss(self, obs, actions, rewards, outputs):\n",
    "        obs_dist = torch.distributions.Normal(\n",
    "            outputs['obs_mean'], torch.exp(outputs['obs_log_std'])\n",
    "        )\n",
    "        recon_loss = -obs_dist.log_prob(obs).mean()\n",
    "        \n",
    "        prior_dist = torch.distributions.Normal(outputs['prior_mean'], outputs['prior_std'])\n",
    "        post_dist = torch.distributions.Normal(outputs['post_mean'], outputs['post_std'])\n",
    "        kl_loss = torch.distributions.kl_divergence(post_dist, prior_dist).mean()\n",
    "        kl_loss = torch.maximum(kl_loss, torch.tensor(3.0, device=kl_loss.device))\n",
    "        \n",
    "        reward_loss = F.mse_loss(outputs['reward_pred'], rewards)\n",
    "        \n",
    "        return recon_loss + kl_loss + reward_loss\n",
    "    \n",
    "    def train_step(self, obs, actions, rewards):\n",
    "        self.model.train()\n",
    "        self.step_count += 1\n",
    "        \n",
    "        # Determine phase\n",
    "        phase = self.step_count % (2 * self.p)\n",
    "        is_mixing = (phase % 2 == 1)\n",
    "        \n",
    "        # Scheduling\n",
    "        if self.use_scheduling:\n",
    "            decay = 0.99 ** (self.step_count // 100)\n",
    "            gamma = self.gamma * decay\n",
    "            beta = self.beta * decay\n",
    "        else:\n",
    "            gamma = self.gamma\n",
    "            beta = self.beta\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        outputs = self.model(obs, actions)\n",
    "        loss = self.compute_loss(obs, actions, rewards, outputs)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Apply operators based on ablation settings\n",
    "        if is_mixing and self.use_mixing:\n",
    "            with torch.no_grad():\n",
    "                for param in self.model.parameters():\n",
    "                    if param.grad is not None:\n",
    "                        noise = torch.randn_like(param) * beta\n",
    "                        param.grad.add_(noise)\n",
    "        elif not is_mixing and self.use_cost:\n",
    "            with torch.no_grad():\n",
    "                for param in self.model.parameters():\n",
    "                    if param.grad is not None:\n",
    "                        param.grad.mul_(gamma)\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(self.model.parameters(), 100.0)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return {'loss': loss.item()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_qaoa_ablation(batches, obs_dim, action_dim, num_epochs=50, seed=42):\n",
    "    \"\"\"Run QAOA ablation experiments.\"\"\"\n",
    "    \n",
    "    ablations = {\n",
    "        'Full QAOA': {'use_cost': True, 'use_mixing': True, 'p': 3, 'use_scheduling': True},\n",
    "        'No Cost': {'use_cost': False, 'use_mixing': True, 'p': 3, 'use_scheduling': True},\n",
    "        'No Mixing': {'use_cost': True, 'use_mixing': False, 'p': 3, 'use_scheduling': True},\n",
    "        'p=1': {'use_cost': True, 'use_mixing': True, 'p': 1, 'use_scheduling': True},\n",
    "        'p=5': {'use_cost': True, 'use_mixing': True, 'p': 5, 'use_scheduling': True},\n",
    "        'No Scheduling': {'use_cost': True, 'use_mixing': True, 'p': 3, 'use_scheduling': False},\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for name, config in ablations.items():\n",
    "        print(f\"Running ablation: {name}\")\n",
    "        set_seed(seed)\n",
    "        \n",
    "        model = BaseWorldModel(obs_dim, action_dim).to(device)\n",
    "        trainer = QAOAAblationTrainer(model, **config)\n",
    "        \n",
    "        history = []\n",
    "        for epoch in range(num_epochs):\n",
    "            epoch_losses = []\n",
    "            for obs, actions, rewards in batches:\n",
    "                metrics = trainer.train_step(obs, actions, rewards)\n",
    "                epoch_losses.append(metrics['loss'])\n",
    "            history.append(np.mean(epoch_losses))\n",
    "        \n",
    "        results[name] = {\n",
    "            'history': history,\n",
    "            'final_loss': np.mean(history[-5:]),\n",
    "            'config': config\n",
    "        }\n",
    "        print(f\"  Final loss: {results[name]['final_loss']:.4f}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect data and run QAOA ablations\n",
    "print(\"Collecting data...\")\n",
    "episodes = collect_data('CartPole-v1', num_episodes=100)\n",
    "batches = create_batches(episodes)\n",
    "obs_dim = episodes[0]['obs'].shape[1]\n",
    "action_dim = episodes[0]['actions'].shape[1]\n",
    "\n",
    "print(f\"\\nRunning QAOA Ablations...\")\n",
    "qaoa_results = run_qaoa_ablation(batches, obs_dim, action_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize QAOA ablations\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "colors = plt.cm.Set2(np.linspace(0, 1, len(qaoa_results)))\n",
    "\n",
    "# Learning curves\n",
    "ax = axes[0]\n",
    "for (name, result), color in zip(qaoa_results.items(), colors):\n",
    "    ax.plot(result['history'], label=name, color=color, linewidth=2)\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Training Loss')\n",
    "ax.set_title('QAOA Ablation: Learning Curves')\n",
    "ax.legend(fontsize=8)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Bar chart\n",
    "ax = axes[1]\n",
    "names = list(qaoa_results.keys())\n",
    "final_losses = [qaoa_results[n]['final_loss'] for n in names]\n",
    "bars = ax.bar(range(len(names)), final_losses, color=colors)\n",
    "ax.set_xticks(range(len(names)))\n",
    "ax.set_xticklabels(names, rotation=45, ha='right', fontsize=9)\n",
    "ax.set_ylabel('Final Loss')\n",
    "ax.set_title('QAOA Ablation: Final Performance')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/figures/ablation_qaoa.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Contribution analysis\n",
    "full_loss = qaoa_results['Full QAOA']['final_loss']\n",
    "print(\"\\nQAOA Component Contributions:\")\n",
    "print(\"=\"*50)\n",
    "for name, result in qaoa_results.items():\n",
    "    if name != 'Full QAOA':\n",
    "        diff = result['final_loss'] - full_loss\n",
    "        pct = (diff / full_loss) * 100\n",
    "        print(f\"{name:20s}: +{diff:+.4f} ({pct:+.1f}% worse)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.4 Superposition Replay Ablations\n",
    "\n",
    "Study amplitude weighting, interference effects, and prioritization components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SuperpositionAblationTrainer:\n",
    "    \"\"\"\n",
    "    Superposition replay trainer with configurable ablations.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    use_amplitude : bool\n",
    "        Use amplitude-based prioritization\n",
    "    use_importance_sampling : bool\n",
    "        Use importance sampling correction\n",
    "    alpha : float\n",
    "        Prioritization exponent\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        lr=3e-4,\n",
    "        alpha=0.6,\n",
    "        beta_start=0.4,\n",
    "        use_amplitude=True,\n",
    "        use_importance_sampling=True\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta_start\n",
    "        self.use_amplitude = use_amplitude\n",
    "        self.use_importance_sampling = use_importance_sampling\n",
    "        self.optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "        self.step_count = 0\n",
    "    \n",
    "    def compute_loss(self, obs, actions, rewards, outputs, weights=None):\n",
    "        obs_dist = torch.distributions.Normal(\n",
    "            outputs['obs_mean'], torch.exp(outputs['obs_log_std'])\n",
    "        )\n",
    "        recon_loss = -obs_dist.log_prob(obs)\n",
    "        \n",
    "        prior_dist = torch.distributions.Normal(outputs['prior_mean'], outputs['prior_std'])\n",
    "        post_dist = torch.distributions.Normal(outputs['post_mean'], outputs['post_std'])\n",
    "        kl_loss = torch.distributions.kl_divergence(post_dist, prior_dist)\n",
    "        \n",
    "        reward_loss = (outputs['reward_pred'] - rewards) ** 2\n",
    "        \n",
    "        if weights is not None:\n",
    "            weights = weights.unsqueeze(-1)\n",
    "            recon_loss = (recon_loss * weights.unsqueeze(-1)).mean()\n",
    "            kl_loss = (kl_loss * weights.unsqueeze(-1)).mean()\n",
    "            reward_loss = (reward_loss * weights).mean()\n",
    "        else:\n",
    "            recon_loss = recon_loss.mean()\n",
    "            kl_loss = kl_loss.mean()\n",
    "            reward_loss = reward_loss.mean()\n",
    "        \n",
    "        kl_loss = torch.maximum(kl_loss, torch.tensor(3.0, device=kl_loss.device))\n",
    "        return recon_loss + kl_loss + reward_loss\n",
    "    \n",
    "    def train_step(self, obs, actions, rewards):\n",
    "        self.model.train()\n",
    "        self.step_count += 1\n",
    "        self.beta = min(1.0, 0.4 + self.step_count * 0.001)\n",
    "        \n",
    "        weights = None\n",
    "        \n",
    "        if self.use_amplitude:\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(obs, actions)\n",
    "                td_errors = (outputs['reward_pred'] - rewards).abs().mean(dim=-1)\n",
    "                priorities = (td_errors + 1e-6) ** self.alpha\n",
    "                amplitudes = priorities / priorities.sum()\n",
    "            \n",
    "            if self.use_importance_sampling:\n",
    "                N = obs.shape[0]\n",
    "                weights = (N * amplitudes) ** (-self.beta)\n",
    "                weights = weights / weights.max()\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        outputs = self.model(obs, actions)\n",
    "        loss = self.compute_loss(obs, actions, rewards, outputs, weights)\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.model.parameters(), 100.0)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return {'loss': loss.item()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_superposition_ablation(batches, obs_dim, action_dim, num_epochs=50, seed=42):\n",
    "    \"\"\"Run superposition ablation experiments.\"\"\"\n",
    "    \n",
    "    ablations = {\n",
    "        'Full Superposition': {'use_amplitude': True, 'use_importance_sampling': True, 'alpha': 0.6},\n",
    "        'No Amplitude': {'use_amplitude': False, 'use_importance_sampling': False, 'alpha': 0.6},\n",
    "        'No IS Correction': {'use_amplitude': True, 'use_importance_sampling': False, 'alpha': 0.6},\n",
    "        'Alpha=0.3': {'use_amplitude': True, 'use_importance_sampling': True, 'alpha': 0.3},\n",
    "        'Alpha=0.9': {'use_amplitude': True, 'use_importance_sampling': True, 'alpha': 0.9},\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for name, config in ablations.items():\n",
    "        print(f\"Running ablation: {name}\")\n",
    "        set_seed(seed)\n",
    "        \n",
    "        model = BaseWorldModel(obs_dim, action_dim).to(device)\n",
    "        trainer = SuperpositionAblationTrainer(model, **config)\n",
    "        \n",
    "        history = []\n",
    "        for epoch in range(num_epochs):\n",
    "            epoch_losses = []\n",
    "            for obs, actions, rewards in batches:\n",
    "                metrics = trainer.train_step(obs, actions, rewards)\n",
    "                epoch_losses.append(metrics['loss'])\n",
    "            history.append(np.mean(epoch_losses))\n",
    "        \n",
    "        results[name] = {\n",
    "            'history': history,\n",
    "            'final_loss': np.mean(history[-5:]),\n",
    "            'config': config\n",
    "        }\n",
    "        print(f\"  Final loss: {results[name]['final_loss']:.4f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"\\nRunning Superposition Ablations...\")\n",
    "superposition_results = run_superposition_ablation(batches, obs_dim, action_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize superposition ablations\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "colors = plt.cm.Set3(np.linspace(0, 1, len(superposition_results)))\n",
    "\n",
    "ax = axes[0]\n",
    "for (name, result), color in zip(superposition_results.items(), colors):\n",
    "    ax.plot(result['history'], label=name, color=color, linewidth=2)\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Training Loss')\n",
    "ax.set_title('Superposition Ablation: Learning Curves')\n",
    "ax.legend(fontsize=8)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "ax = axes[1]\n",
    "names = list(superposition_results.keys())\n",
    "final_losses = [superposition_results[n]['final_loss'] for n in names]\n",
    "bars = ax.bar(range(len(names)), final_losses, color=colors)\n",
    "ax.set_xticks(range(len(names)))\n",
    "ax.set_xticklabels(names, rotation=45, ha='right', fontsize=9)\n",
    "ax.set_ylabel('Final Loss')\n",
    "ax.set_title('Superposition Ablation: Final Performance')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/figures/ablation_superposition.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Contribution analysis\n",
    "full_loss = superposition_results['Full Superposition']['final_loss']\n",
    "print(\"\\nSuperposition Component Contributions:\")\n",
    "print(\"=\"*50)\n",
    "for name, result in superposition_results.items():\n",
    "    if name != 'Full Superposition':\n",
    "        diff = result['final_loss'] - full_loss\n",
    "        pct = (diff / full_loss) * 100\n",
    "        print(f\"{name:20s}: {diff:+.4f} ({pct:+.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.5 Gate Layer Ablations\n",
    "\n",
    "Study individual gate types and layer configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GateLayer(nn.Module):\n",
    "    \"\"\"Configurable gate layer for ablations.\"\"\"\n",
    "    \n",
    "    def __init__(self, dim, use_rotation=True, use_phase=True, use_residual=True):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.use_rotation = use_rotation\n",
    "        self.use_phase = use_phase\n",
    "        self.use_residual = use_residual\n",
    "        \n",
    "        if use_rotation:\n",
    "            num_rotations = dim // 2\n",
    "            self.angles = nn.Parameter(torch.randn(num_rotations, 3) * 0.1)\n",
    "            indices = torch.randperm(dim)[:num_rotations * 2]\n",
    "            self.register_buffer('idx1', indices[:num_rotations])\n",
    "            self.register_buffer('idx2', indices[num_rotations:])\n",
    "        \n",
    "        if use_phase:\n",
    "            self.phases = nn.Parameter(torch.randn(dim) * 0.1)\n",
    "            self.amplitude = nn.Parameter(torch.ones(dim))\n",
    "        \n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        y = x.clone()\n",
    "        \n",
    "        if self.use_rotation:\n",
    "            x1, x2 = x[:, self.idx1], x[:, self.idx2]\n",
    "            for i in range(3):\n",
    "                cos_t = torch.cos(self.angles[:, i])\n",
    "                sin_t = torch.sin(self.angles[:, i])\n",
    "                x1_new = cos_t * x1 - sin_t * x2\n",
    "                x2_new = sin_t * x1 + cos_t * x2\n",
    "                x1, x2 = x1_new, x2_new\n",
    "            y = y.scatter(1, self.idx1.unsqueeze(0).expand(x.shape[0], -1), x1)\n",
    "            y = y.scatter(1, self.idx2.unsqueeze(0).expand(x.shape[0], -1), x2)\n",
    "        \n",
    "        if self.use_phase:\n",
    "            cos_p = torch.cos(self.phases)\n",
    "            sin_p = torch.sin(self.phases)\n",
    "            y = self.amplitude * (y * cos_p + torch.abs(y) * sin_p)\n",
    "        \n",
    "        if self.use_residual:\n",
    "            return self.norm(x + y)\n",
    "        else:\n",
    "            return self.norm(y)\n",
    "\n",
    "\n",
    "class GateAblationModel(nn.Module):\n",
    "    \"\"\"World model with configurable gate layers.\"\"\"\n",
    "    \n",
    "    def __init__(self, obs_dim, action_dim, hidden_dim=512, deter_dim=512, stoch_dim=64,\n",
    "                 num_gate_layers=2, use_rotation=True, use_phase=True, use_residual=True):\n",
    "        super().__init__()\n",
    "        self.obs_dim = obs_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.deter_dim = deter_dim\n",
    "        self.stoch_dim = stoch_dim\n",
    "        self.state_dim = deter_dim + stoch_dim\n",
    "        \n",
    "        # Encoder with gate layers\n",
    "        encoder_layers = [nn.Linear(obs_dim, hidden_dim), nn.ELU()]\n",
    "        for _ in range(num_gate_layers):\n",
    "            encoder_layers.append(GateLayer(hidden_dim, use_rotation, use_phase, use_residual))\n",
    "        encoder_layers.extend([nn.Linear(hidden_dim, hidden_dim // 2), nn.ELU()])\n",
    "        self.encoder = nn.Sequential(*encoder_layers)\n",
    "        \n",
    "        self.gru = nn.GRUCell(stoch_dim + action_dim, deter_dim)\n",
    "        self.prior = nn.Sequential(\n",
    "            nn.Linear(deter_dim, hidden_dim // 2), nn.ELU(),\n",
    "            nn.Linear(hidden_dim // 2, stoch_dim * 2)\n",
    "        )\n",
    "        self.posterior = nn.Sequential(\n",
    "            nn.Linear(deter_dim + hidden_dim // 2, hidden_dim // 2), nn.ELU(),\n",
    "            nn.Linear(hidden_dim // 2, stoch_dim * 2)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(self.state_dim, hidden_dim), nn.ELU(),\n",
    "            nn.Linear(hidden_dim, obs_dim * 2)\n",
    "        )\n",
    "        self.reward_pred = nn.Sequential(\n",
    "            nn.Linear(self.state_dim, hidden_dim // 2), nn.ELU(),\n",
    "            nn.Linear(hidden_dim // 2, 1)\n",
    "        )\n",
    "    \n",
    "    def initial_state(self, batch_size, device):\n",
    "        return {\n",
    "            'deter': torch.zeros(batch_size, self.deter_dim, device=device),\n",
    "            'stoch': torch.zeros(batch_size, self.stoch_dim, device=device)\n",
    "        }\n",
    "    \n",
    "    def forward(self, obs_seq, action_seq):\n",
    "        batch_size, seq_len = obs_seq.shape[:2]\n",
    "        device = obs_seq.device\n",
    "        state = self.initial_state(batch_size, device)\n",
    "        \n",
    "        states, prior_means, prior_stds, post_means, post_stds = [], [], [], [], []\n",
    "        \n",
    "        for t in range(seq_len):\n",
    "            embed = self.encoder(obs_seq[:, t])\n",
    "            gru_input = torch.cat([state['stoch'], action_seq[:, t]], dim=-1)\n",
    "            deter = self.gru(gru_input, state['deter'])\n",
    "            \n",
    "            prior_stats = self.prior(deter)\n",
    "            prior_mean, prior_log_std = torch.chunk(prior_stats, 2, dim=-1)\n",
    "            prior_std = F.softplus(prior_log_std) + 0.1\n",
    "            \n",
    "            post_input = torch.cat([deter, embed], dim=-1)\n",
    "            post_stats = self.posterior(post_input)\n",
    "            post_mean, post_log_std = torch.chunk(post_stats, 2, dim=-1)\n",
    "            post_std = F.softplus(post_log_std) + 0.1\n",
    "            \n",
    "            stoch = post_mean + post_std * torch.randn_like(post_std)\n",
    "            state = {'deter': deter, 'stoch': stoch}\n",
    "            \n",
    "            states.append(torch.cat([deter, stoch], dim=-1))\n",
    "            prior_means.append(prior_mean)\n",
    "            prior_stds.append(prior_std)\n",
    "            post_means.append(post_mean)\n",
    "            post_stds.append(post_std)\n",
    "        \n",
    "        states = torch.stack(states, dim=1)\n",
    "        flat_states = states.reshape(-1, self.state_dim)\n",
    "        \n",
    "        dec_output = self.decoder(flat_states)\n",
    "        obs_mean, obs_log_std = torch.chunk(dec_output, 2, dim=-1)\n",
    "        obs_mean = obs_mean.reshape(batch_size, seq_len, -1)\n",
    "        obs_log_std = obs_log_std.clamp(-10, 2).reshape(batch_size, seq_len, -1)\n",
    "        \n",
    "        reward_pred = self.reward_pred(flat_states).reshape(batch_size, seq_len)\n",
    "        \n",
    "        return {\n",
    "            'states': states, 'obs_mean': obs_mean, 'obs_log_std': obs_log_std,\n",
    "            'reward_pred': reward_pred,\n",
    "            'prior_mean': torch.stack(prior_means, dim=1),\n",
    "            'prior_std': torch.stack(prior_stds, dim=1),\n",
    "            'post_mean': torch.stack(post_means, dim=1),\n",
    "            'post_std': torch.stack(post_stds, dim=1)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_gate_ablation(batches, obs_dim, action_dim, num_epochs=50, seed=42):\n",
    "    \"\"\"Run gate layer ablation experiments.\"\"\"\n",
    "    \n",
    "    ablations = {\n",
    "        'Full Gates': {'num_gate_layers': 2, 'use_rotation': True, 'use_phase': True, 'use_residual': True},\n",
    "        'No Rotation': {'num_gate_layers': 2, 'use_rotation': False, 'use_phase': True, 'use_residual': True},\n",
    "        'No Phase': {'num_gate_layers': 2, 'use_rotation': True, 'use_phase': False, 'use_residual': True},\n",
    "        'No Residual': {'num_gate_layers': 2, 'use_rotation': True, 'use_phase': True, 'use_residual': False},\n",
    "        '1 Layer': {'num_gate_layers': 1, 'use_rotation': True, 'use_phase': True, 'use_residual': True},\n",
    "        '4 Layers': {'num_gate_layers': 4, 'use_rotation': True, 'use_phase': True, 'use_residual': True},\n",
    "    }\n",
    "    \n",
    "    class SimpleTrainer:\n",
    "        def __init__(self, model, lr=3e-4):\n",
    "            self.model = model\n",
    "            self.optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "        \n",
    "        def train_step(self, obs, actions, rewards):\n",
    "            self.model.train()\n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            outputs = self.model(obs, actions)\n",
    "            obs_dist = torch.distributions.Normal(\n",
    "                outputs['obs_mean'], torch.exp(outputs['obs_log_std'])\n",
    "            )\n",
    "            recon_loss = -obs_dist.log_prob(obs).mean()\n",
    "            \n",
    "            prior_dist = torch.distributions.Normal(outputs['prior_mean'], outputs['prior_std'])\n",
    "            post_dist = torch.distributions.Normal(outputs['post_mean'], outputs['post_std'])\n",
    "            kl_loss = torch.distributions.kl_divergence(post_dist, prior_dist).mean()\n",
    "            kl_loss = torch.maximum(kl_loss, torch.tensor(3.0, device=kl_loss.device))\n",
    "            \n",
    "            reward_loss = F.mse_loss(outputs['reward_pred'], rewards)\n",
    "            loss = recon_loss + kl_loss + reward_loss\n",
    "            \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 100.0)\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            return {'loss': loss.item()}\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for name, config in ablations.items():\n",
    "        print(f\"Running ablation: {name}\")\n",
    "        set_seed(seed)\n",
    "        \n",
    "        model = GateAblationModel(obs_dim, action_dim, **config).to(device)\n",
    "        trainer = SimpleTrainer(model)\n",
    "        \n",
    "        history = []\n",
    "        for epoch in range(num_epochs):\n",
    "            epoch_losses = []\n",
    "            for obs, actions, rewards in batches:\n",
    "                metrics = trainer.train_step(obs, actions, rewards)\n",
    "                epoch_losses.append(metrics['loss'])\n",
    "            history.append(np.mean(epoch_losses))\n",
    "        \n",
    "        results[name] = {\n",
    "            'history': history,\n",
    "            'final_loss': np.mean(history[-5:]),\n",
    "            'config': config\n",
    "        }\n",
    "        print(f\"  Final loss: {results[name]['final_loss']:.4f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"\\nRunning Gate Ablations...\")\n",
    "gate_results = run_gate_ablation(batches, obs_dim, action_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize gate ablations\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "colors = plt.cm.Paired(np.linspace(0, 1, len(gate_results)))\n",
    "\n",
    "ax = axes[0]\n",
    "for (name, result), color in zip(gate_results.items(), colors):\n",
    "    ax.plot(result['history'], label=name, color=color, linewidth=2)\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Training Loss')\n",
    "ax.set_title('Gate Ablation: Learning Curves')\n",
    "ax.legend(fontsize=8)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "ax = axes[1]\n",
    "names = list(gate_results.keys())\n",
    "final_losses = [gate_results[n]['final_loss'] for n in names]\n",
    "bars = ax.bar(range(len(names)), final_losses, color=colors)\n",
    "ax.set_xticks(range(len(names)))\n",
    "ax.set_xticklabels(names, rotation=45, ha='right', fontsize=9)\n",
    "ax.set_ylabel('Final Loss')\n",
    "ax.set_title('Gate Ablation: Final Performance')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/figures/ablation_gates.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Contribution analysis\n",
    "full_loss = gate_results['Full Gates']['final_loss']\n",
    "print(\"\\nGate Component Contributions:\")\n",
    "print(\"=\"*50)\n",
    "for name, result in gate_results.items():\n",
    "    if name != 'Full Gates':\n",
    "        diff = result['final_loss'] - full_loss\n",
    "        pct = (diff / full_loss) * 100\n",
    "        print(f\"{name:20s}: {diff:+.4f} ({pct:+.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.6 Error Correction Ablations\n",
    "\n",
    "Study ensemble size, correction method, and diversity weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnsembleAblationModel(nn.Module):\n",
    "    \"\"\"Ensemble with configurable error correction.\"\"\"\n",
    "    \n",
    "    def __init__(self, obs_dim, action_dim, num_models=5, correction='weighted'):\n",
    "        super().__init__()\n",
    "        self.num_models = num_models\n",
    "        self.correction = correction\n",
    "        \n",
    "        self.models = nn.ModuleList([\n",
    "            BaseWorldModel(obs_dim, action_dim, hidden_dim=512, deter_dim=512, stoch_dim=64)\n",
    "            for _ in range(num_models)\n",
    "        ])\n",
    "        \n",
    "        for i, model in enumerate(self.models):\n",
    "            torch.manual_seed(42 + i * 1000)\n",
    "            for m in model.modules():\n",
    "                if hasattr(m, 'reset_parameters'):\n",
    "                    m.reset_parameters()\n",
    "    \n",
    "    def forward(self, obs_seq, action_seq):\n",
    "        all_outputs = [model(obs_seq, action_seq) for model in self.models]\n",
    "        \n",
    "        obs_means = torch.stack([o['obs_mean'] for o in all_outputs], dim=0)\n",
    "        \n",
    "        if self.correction == 'weighted':\n",
    "            ensemble_mean = obs_means.mean(dim=0)\n",
    "            deviations = (obs_means - ensemble_mean.unsqueeze(0)).abs().mean(dim=(2, 3))\n",
    "            weights = 1.0 / (deviations + 1e-8)\n",
    "            weights = weights / weights.sum(dim=0, keepdim=True)\n",
    "            obs_mean = (obs_means * weights.unsqueeze(-1).unsqueeze(-1)).sum(dim=0)\n",
    "        elif self.correction == 'median':\n",
    "            obs_mean = obs_means.median(dim=0).values\n",
    "        else:  # simple average\n",
    "            obs_mean = obs_means.mean(dim=0)\n",
    "        \n",
    "        obs_log_stds = torch.stack([o['obs_log_std'] for o in all_outputs], dim=0)\n",
    "        obs_log_std = obs_log_stds.mean(dim=0)\n",
    "        \n",
    "        reward_preds = torch.stack([o['reward_pred'] for o in all_outputs], dim=0)\n",
    "        reward_pred = reward_preds.mean(dim=0)\n",
    "        \n",
    "        return {\n",
    "            'obs_mean': obs_mean,\n",
    "            'obs_log_std': obs_log_std,\n",
    "            'reward_pred': reward_pred,\n",
    "            'prior_mean': torch.stack([o['prior_mean'] for o in all_outputs]).mean(0),\n",
    "            'prior_std': torch.stack([o['prior_std'] for o in all_outputs]).mean(0),\n",
    "            'post_mean': torch.stack([o['post_mean'] for o in all_outputs]).mean(0),\n",
    "            'post_std': torch.stack([o['post_std'] for o in all_outputs]).mean(0),\n",
    "            'states': torch.stack([o['states'] for o in all_outputs]).mean(0)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ensemble_ablation(batches, obs_dim, action_dim, num_epochs=50, seed=42):\n",
    "    \"\"\"Run ensemble ablation experiments.\"\"\"\n",
    "    \n",
    "    ablations = {\n",
    "        'Full (5, weighted)': {'num_models': 5, 'correction': 'weighted'},\n",
    "        '3 models': {'num_models': 3, 'correction': 'weighted'},\n",
    "        '7 models': {'num_models': 7, 'correction': 'weighted'},\n",
    "        'Median voting': {'num_models': 5, 'correction': 'median'},\n",
    "        'Simple average': {'num_models': 5, 'correction': 'average'},\n",
    "        'Single model': {'num_models': 1, 'correction': 'average'},\n",
    "    }\n",
    "    \n",
    "    class EnsembleTrainer:\n",
    "        def __init__(self, model, lr=3e-4, diversity_weight=0.1):\n",
    "            self.model = model\n",
    "            self.diversity_weight = diversity_weight\n",
    "            self.optimizers = [torch.optim.AdamW(m.parameters(), lr=lr) for m in model.models]\n",
    "        \n",
    "        def train_step(self, obs, actions, rewards):\n",
    "            for m in self.model.models:\n",
    "                m.train()\n",
    "            \n",
    "            all_outputs = [m(obs, actions) for m in self.model.models]\n",
    "            \n",
    "            total_loss = 0.0\n",
    "            for i, (m, outputs, opt) in enumerate(zip(self.model.models, all_outputs, self.optimizers)):\n",
    "                opt.zero_grad()\n",
    "                \n",
    "                obs_dist = torch.distributions.Normal(\n",
    "                    outputs['obs_mean'], torch.exp(outputs['obs_log_std'])\n",
    "                )\n",
    "                recon_loss = -obs_dist.log_prob(obs).mean()\n",
    "                \n",
    "                prior_dist = torch.distributions.Normal(outputs['prior_mean'], outputs['prior_std'])\n",
    "                post_dist = torch.distributions.Normal(outputs['post_mean'], outputs['post_std'])\n",
    "                kl_loss = torch.distributions.kl_divergence(post_dist, prior_dist).mean()\n",
    "                kl_loss = torch.maximum(kl_loss, torch.tensor(3.0, device=kl_loss.device))\n",
    "                \n",
    "                reward_loss = F.mse_loss(outputs['reward_pred'], rewards)\n",
    "                loss = recon_loss + kl_loss + reward_loss\n",
    "                \n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(m.parameters(), 100.0)\n",
    "                opt.step()\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "            \n",
    "            return {'loss': total_loss / len(self.model.models)}\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for name, config in ablations.items():\n",
    "        print(f\"Running ablation: {name}\")\n",
    "        set_seed(seed)\n",
    "        \n",
    "        model = EnsembleAblationModel(obs_dim, action_dim, **config).to(device)\n",
    "        trainer = EnsembleTrainer(model)\n",
    "        \n",
    "        history = []\n",
    "        for epoch in range(num_epochs):\n",
    "            epoch_losses = []\n",
    "            for obs, actions, rewards in batches:\n",
    "                metrics = trainer.train_step(obs, actions, rewards)\n",
    "                epoch_losses.append(metrics['loss'])\n",
    "            history.append(np.mean(epoch_losses))\n",
    "        \n",
    "        results[name] = {\n",
    "            'history': history,\n",
    "            'final_loss': np.mean(history[-5:]),\n",
    "            'config': config\n",
    "        }\n",
    "        print(f\"  Final loss: {results[name]['final_loss']:.4f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"\\nRunning Ensemble Ablations...\")\n",
    "ensemble_results = run_ensemble_ablation(batches, obs_dim, action_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize ensemble ablations\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "colors = plt.cm.Dark2(np.linspace(0, 1, len(ensemble_results)))\n",
    "\n",
    "ax = axes[0]\n",
    "for (name, result), color in zip(ensemble_results.items(), colors):\n",
    "    ax.plot(result['history'], label=name, color=color, linewidth=2)\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Training Loss')\n",
    "ax.set_title('Ensemble Ablation: Learning Curves')\n",
    "ax.legend(fontsize=8)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "ax = axes[1]\n",
    "names = list(ensemble_results.keys())\n",
    "final_losses = [ensemble_results[n]['final_loss'] for n in names]\n",
    "bars = ax.bar(range(len(names)), final_losses, color=colors)\n",
    "ax.set_xticks(range(len(names)))\n",
    "ax.set_xticklabels(names, rotation=45, ha='right', fontsize=9)\n",
    "ax.set_ylabel('Final Loss')\n",
    "ax.set_title('Ensemble Ablation: Final Performance')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/figures/ablation_ensemble.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Contribution analysis\n",
    "full_loss = ensemble_results['Full (5, weighted)']['final_loss']\n",
    "print(\"\\nEnsemble Component Contributions:\")\n",
    "print(\"=\"*50)\n",
    "for name, result in ensemble_results.items():\n",
    "    if name != 'Full (5, weighted)':\n",
    "        diff = result['final_loss'] - full_loss\n",
    "        pct = (diff / full_loss) * 100\n",
    "        print(f\"{name:20s}: {diff:+.4f} ({pct:+.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.7 Summary of Ablation Findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive summary visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "all_ablations = [\n",
    "    ('QAOA', qaoa_results, 'Full QAOA'),\n",
    "    ('Superposition', superposition_results, 'Full Superposition'),\n",
    "    ('Gates', gate_results, 'Full Gates'),\n",
    "    ('Ensemble', ensemble_results, 'Full (5, weighted)')\n",
    "]\n",
    "\n",
    "for ax, (title, results, full_name) in zip(axes.flatten(), all_ablations):\n",
    "    full_loss = results[full_name]['final_loss']\n",
    "    \n",
    "    names = []\n",
    "    diffs = []\n",
    "    \n",
    "    for name, result in results.items():\n",
    "        if name != full_name:\n",
    "            names.append(name)\n",
    "            diffs.append(result['final_loss'] - full_loss)\n",
    "    \n",
    "    colors = ['green' if d < 0 else 'red' for d in diffs]\n",
    "    ax.barh(range(len(names)), diffs, color=colors, alpha=0.7)\n",
    "    ax.set_yticks(range(len(names)))\n",
    "    ax.set_yticklabels(names, fontsize=9)\n",
    "    ax.axvline(x=0, color='black', linestyle='-', linewidth=0.5)\n",
    "    ax.set_xlabel('Loss Difference from Full Method')\n",
    "    ax.set_title(f'{title} Ablations')\n",
    "    ax.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/figures/ablation_summary.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary table\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ABLATION STUDY SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for title, results, full_name in all_ablations:\n",
    "    print(f\"\\n{title} Ablations:\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    full_loss = results[full_name]['final_loss']\n",
    "    print(f\"  Full method loss: {full_loss:.4f}\")\n",
    "    print()\n",
    "    \n",
    "    for name, result in results.items():\n",
    "        if name != full_name:\n",
    "            diff = result['final_loss'] - full_loss\n",
    "            pct = (diff / full_loss) * 100\n",
    "            impact = \"CRITICAL\" if pct > 10 else \"moderate\" if pct > 5 else \"minor\"\n",
    "            print(f\"  {name:30s}: {diff:+.4f} ({pct:+.1f}%) - {impact}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save ablation results\n",
    "results_dir = Path('../results/ablations')\n",
    "results_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save as DataFrames\n",
    "for title, results, _ in all_ablations:\n",
    "    df = pd.DataFrame([\n",
    "        {'ablation': name, 'final_loss': r['final_loss']}\n",
    "        for name, r in results.items()\n",
    "    ])\n",
    "    df.to_csv(results_dir / f'ablation_{title.lower()}.csv', index=False)\n",
    "\n",
    "print(f\"Ablation results saved to {results_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.8 Conclusions\n",
    "\n",
    "### Key Findings from Ablation Studies\n",
    "\n",
    "1. **QAOA**: Both cost and mixing operators contribute, with mixing being more critical\n",
    "2. **Superposition**: Amplitude weighting is the primary contributor; importance sampling helps stability\n",
    "3. **Gates**: Rotation operations provide the most benefit; residual connections are crucial\n",
    "4. **Ensemble**: Weighted averaging outperforms simple averaging; ensemble size shows diminishing returns\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Phase 9: Results & Analysis (final documentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Phase 8: Ablation Studies - COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nCompleted:\")\n",
    "print(\"  - QAOA ablations: cost, mixing, p, scheduling\")\n",
    "print(\"  - Superposition ablations: amplitude, IS, alpha\")\n",
    "print(\"  - Gate ablations: rotation, phase, residual, depth\")\n",
    "print(\"  - Ensemble ablations: size, correction, diversity\")\n",
    "print(\"  - Component contribution analysis\")\n",
    "print(\"  - Results saved to results/ablations/\")\n",
    "print(\"\\nReady for Phase 9: Results & Analysis\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}