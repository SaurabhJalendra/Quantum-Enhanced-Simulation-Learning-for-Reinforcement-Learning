{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 6B: Fully Integrated Quantum-Enhanced World Model\n",
    "\n",
    "**Quantum-Enhanced Simulation Learning for Reinforcement Learning**\n",
    "\n",
    "Author: Saurabh Jalendra  \n",
    "Institution: BITS Pilani (WILP Division)  \n",
    "Date: December 2025\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook implements the **Fully Integrated** approach that combines ALL quantum-inspired components:\n",
    "\n",
    "| Component | Source | Target Bottleneck |\n",
    "|-----------|--------|-------------------|\n",
    "| QAOA Optimizer | Notebook 03 | Local Minima |\n",
    "| Superposition Replay | Notebook 04 | Sample Inefficiency |\n",
    "| Gate-Enhanced Layers | Notebook 05 | Slow Convergence |\n",
    "| Error Correction Ensemble | Notebook 06 | Compounding Errors |\n",
    "\n",
    "**Research Question:** Does combining all quantum-inspired methods provide better results than any single method alone?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6B.1 Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Configuration: {'obs_dim': 4, 'action_dim': 2, 'stoch_dim': 64, 'deter_dim': 512, 'hidden_dim': 512, 'batch_size': 32, 'seq_len': 20, 'num_epochs': 50, 'learning_rate': 0.0003, 'num_episodes': 100, 'num_ensemble': 5, 'seeds': [42, 123, 456, 789, 1024]}\n",
      "Experiment seeds: [42, 123, 456, 789, 1024]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "from dataclasses import dataclass\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import json\n",
    "import time\n",
    "from scipy import stats\n",
    "\n",
    "# Device configuration\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {DEVICE}')\n",
    "\n",
    "# Standard seeds per CLAUDE.md\n",
    "EXPERIMENT_SEEDS = [42, 123, 456, 789, 1024]\n",
    "\n",
    "# Standard configuration (consistent with all notebooks)\n",
    "CONFIG = {\n",
    "    'obs_dim': 4,\n",
    "    'action_dim': 2,\n",
    "    'stoch_dim': 64,\n",
    "    'deter_dim': 512,\n",
    "    'hidden_dim': 512,\n",
    "    'batch_size': 32,\n",
    "    'seq_len': 20,\n",
    "    'num_epochs': 50,\n",
    "    'learning_rate': 3e-4,\n",
    "    'num_episodes': 100,\n",
    "    'num_ensemble': 5,\n",
    "    'seeds': EXPERIMENT_SEEDS,\n",
    "}\n",
    "\n",
    "print(f'Configuration: {CONFIG}')\n",
    "print(f'Experiment seeds: {EXPERIMENT_SEEDS}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6B.2 Component 1: Gate-Enhanced Layers\n",
    "\n",
    "Quantum gate-inspired neural network layers for richer representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gate-Enhanced Layers defined.\n"
     ]
    }
   ],
   "source": [
    "class HadamardLayer(nn.Module):\n",
    "    \"\"\"Hadamard-inspired feature mixing layer.\"\"\"\n",
    "    \n",
    "    def __init__(self, dim: int):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        # Learnable Hadamard-like transformation\n",
    "        self.weight = nn.Parameter(torch.randn(dim, dim) / np.sqrt(dim))\n",
    "        self.bias = nn.Parameter(torch.zeros(dim))\n",
    "        # Make it close to orthogonal\n",
    "        nn.init.orthogonal_(self.weight)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Apply Hadamard-like mixing\n",
    "        return F.linear(x, self.weight, self.bias)\n",
    "\n",
    "\n",
    "class PhaseLayer(nn.Module):\n",
    "    \"\"\"Phase gate-inspired modulation layer.\"\"\"\n",
    "    \n",
    "    def __init__(self, dim: int):\n",
    "        super().__init__()\n",
    "        self.phase = nn.Parameter(torch.zeros(dim))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Apply learnable phase modulation\n",
    "        return x * torch.cos(self.phase) + torch.roll(x, 1, dims=-1) * torch.sin(self.phase)\n",
    "\n",
    "\n",
    "class EntanglementLayer(nn.Module):\n",
    "    \"\"\"CNOT-inspired entanglement layer.\"\"\"\n",
    "    \n",
    "    def __init__(self, dim: int, num_pairs: int = 4):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.num_pairs = min(num_pairs, dim // 2)\n",
    "        self.control_weights = nn.Parameter(torch.randn(self.num_pairs))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Avoid inplace operations by building output piece by piece\n",
    "        # Split input into chunks and process each pair\n",
    "        chunks = list(x.split(1, dim=-1))\n",
    "        \n",
    "        for i in range(self.num_pairs):\n",
    "            control_idx = i * 2\n",
    "            target_idx = i * 2 + 1\n",
    "            if target_idx < self.dim:\n",
    "                # CNOT-like: target XOR control (soft version)\n",
    "                gate = torch.sigmoid(self.control_weights[i])\n",
    "                control = chunks[control_idx]\n",
    "                target = chunks[target_idx]\n",
    "                # Create new tensor instead of inplace modification\n",
    "                new_target = target * (1 - gate) + (target * control) * gate\n",
    "                chunks[target_idx] = new_target\n",
    "        \n",
    "        return torch.cat(chunks, dim=-1)\n",
    "\n",
    "\n",
    "class QuantumGateBlock(nn.Module):\n",
    "    \"\"\"Combined quantum gate block with all gate types.\"\"\"\n",
    "    \n",
    "    def __init__(self, dim: int, use_hadamard: bool = True, use_phase: bool = True, use_entangle: bool = True):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        if use_hadamard:\n",
    "            layers.append(HadamardLayer(dim))\n",
    "            layers.append(nn.ELU())\n",
    "        if use_phase:\n",
    "            layers.append(PhaseLayer(dim))\n",
    "        if use_entangle:\n",
    "            layers.append(EntanglementLayer(dim))\n",
    "        layers.append(nn.LayerNorm(dim))\n",
    "        self.block = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.block(x) + x  # Residual connection\n",
    "\n",
    "print('Gate-Enhanced Layers defined.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6B.3 Component 2: Gate-Enhanced World Model\n",
    "\n",
    "World model with quantum gate layers integrated into encoder, decoder, and RSSM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GateEnhancedWorldModel(nn.Module):\n",
    "    \"\"\"\n",
    "    World model with quantum gate-enhanced layers.\n",
    "\n",
    "    Matches baseline architecture from notebook 02 with quantum gate additions:\n",
    "    - input_proj: Linear(stoch + action, hidden) + ELU\n",
    "    - gru: GRUCell(hidden, deter)\n",
    "    - prior: 2-layer (same as baseline)\n",
    "    - posterior: 2-layer (same as baseline)\n",
    "    - encoder/decoder: [512, 512] with QuantumGateBlock\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, obs_dim: int, action_dim: int, config: Dict):\n",
    "        super().__init__()\n",
    "        self.obs_dim = obs_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.stoch_dim = config.get('stoch_dim', 64)\n",
    "        self.deter_dim = config.get('deter_dim', 512)\n",
    "        self.hidden_dim = config.get('hidden_dim', 512)\n",
    "        self.state_dim = self.stoch_dim + self.deter_dim\n",
    "\n",
    "        predictor_hidden = [512, 512]\n",
    "\n",
    "        # Gate-enhanced encoder (matching baseline [512, 512] architecture)\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(obs_dim, self.hidden_dim),\n",
    "            nn.ELU(),\n",
    "            QuantumGateBlock(self.hidden_dim),\n",
    "            nn.Linear(self.hidden_dim, self.hidden_dim),\n",
    "            nn.ELU(),\n",
    "        )\n",
    "\n",
    "        # RSSM - matches baseline EXACTLY\n",
    "        # Input projection (same as baseline)\n",
    "        self.input_proj = nn.Sequential(\n",
    "            nn.Linear(self.stoch_dim + action_dim, self.hidden_dim),\n",
    "            nn.ELU()\n",
    "        )\n",
    "\n",
    "        self.gru = nn.GRUCell(self.hidden_dim, self.deter_dim)\n",
    "\n",
    "        # Prior: 2-layer (matches baseline)\n",
    "        self.prior = nn.Sequential(\n",
    "            nn.Linear(self.deter_dim, self.hidden_dim),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(self.hidden_dim, self.stoch_dim * 2)\n",
    "        )\n",
    "\n",
    "        # Posterior: 2-layer (matches baseline)\n",
    "        self.posterior = nn.Sequential(\n",
    "            nn.Linear(self.deter_dim + self.hidden_dim, self.hidden_dim),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(self.hidden_dim, self.stoch_dim * 2)\n",
    "        )\n",
    "\n",
    "        # Gate-enhanced decoder (matching baseline [512, 512] architecture)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(self.state_dim, self.hidden_dim),\n",
    "            nn.ELU(),\n",
    "            QuantumGateBlock(self.hidden_dim),\n",
    "            nn.Linear(self.hidden_dim, self.hidden_dim),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(self.hidden_dim, obs_dim * 2)\n",
    "        )\n",
    "\n",
    "        # Reward predictor (using predictor_hidden)\n",
    "        rew_layers = []\n",
    "        in_d = self.state_dim\n",
    "        for h in predictor_hidden:\n",
    "            rew_layers.extend([nn.Linear(in_d, h), nn.ELU()])\n",
    "            in_d = h\n",
    "        rew_layers.append(nn.Linear(in_d, 1))\n",
    "        self.reward_pred = nn.Sequential(*rew_layers)\n",
    "\n",
    "    def initial_state(self, batch_size: int):\n",
    "        return {\n",
    "            'deter': torch.zeros(batch_size, self.deter_dim, device=DEVICE),\n",
    "            'stoch': torch.zeros(batch_size, self.stoch_dim, device=DEVICE)\n",
    "        }\n",
    "\n",
    "    def get_dist(self, stats):\n",
    "        mean, log_std = stats.chunk(2, dim=-1)\n",
    "        std = F.softplus(log_std) + 0.1\n",
    "        return torch.distributions.Normal(mean, std)\n",
    "\n",
    "    def forward(self, obs_seq, action_seq):\n",
    "        batch_size, seq_len = obs_seq.shape[:2]\n",
    "\n",
    "        state = self.initial_state(batch_size)\n",
    "\n",
    "        prior_means, prior_stds = [], []\n",
    "        post_means, post_stds = [], []\n",
    "        obs_means, obs_stds = [], []\n",
    "        rewards = []\n",
    "        states = []\n",
    "\n",
    "        for t in range(seq_len):\n",
    "            embed = self.encoder(obs_seq[:, t])\n",
    "\n",
    "            # Prior (before seeing observation)\n",
    "            prior_stats = self.prior(state['deter'])\n",
    "            prior_dist = self.get_dist(prior_stats)\n",
    "\n",
    "            # Posterior (after seeing observation)\n",
    "            post_input = torch.cat([state['deter'], embed], dim=-1)\n",
    "            post_stats = self.posterior(post_input)\n",
    "            post_dist = self.get_dist(post_stats)\n",
    "\n",
    "            # Sample stochastic state\n",
    "            stoch = post_dist.rsample()\n",
    "\n",
    "            # Update deterministic state (through input_proj like baseline)\n",
    "            gru_input = self.input_proj(torch.cat([stoch, action_seq[:, t]], dim=-1))\n",
    "            deter = self.gru(gru_input, state['deter'])\n",
    "\n",
    "            # Full state\n",
    "            full_state = torch.cat([deter, stoch], dim=-1)\n",
    "\n",
    "            # Decode observation\n",
    "            obs_stats = self.decoder(full_state)\n",
    "            obs_mean, obs_log_std = obs_stats.chunk(2, dim=-1)\n",
    "            obs_std = F.softplus(obs_log_std) + 0.1\n",
    "\n",
    "            # Predict reward\n",
    "            reward = self.reward_pred(full_state)\n",
    "\n",
    "            # Store outputs\n",
    "            prior_means.append(prior_dist.mean)\n",
    "            prior_stds.append(prior_dist.stddev)\n",
    "            post_means.append(post_dist.mean)\n",
    "            post_stds.append(post_dist.stddev)\n",
    "            obs_means.append(obs_mean)\n",
    "            obs_stds.append(obs_std)\n",
    "            rewards.append(reward)\n",
    "            states.append(full_state)\n",
    "\n",
    "            state = {'deter': deter, 'stoch': stoch}\n",
    "\n",
    "        return {\n",
    "            'prior_mean': torch.stack(prior_means, dim=1),\n",
    "            'prior_std': torch.stack(prior_stds, dim=1),\n",
    "            'post_mean': torch.stack(post_means, dim=1),\n",
    "            'post_std': torch.stack(post_stds, dim=1),\n",
    "            'obs_mean': torch.stack(obs_means, dim=1),\n",
    "            'obs_std': torch.stack(obs_stds, dim=1),\n",
    "            'reward': torch.stack(rewards, dim=1).squeeze(-1),\n",
    "            'states': torch.stack(states, dim=1)\n",
    "        }\n",
    "\n",
    "    def imagine(self, initial_state, actions):\n",
    "        \"\"\"Imagine future states given actions.\"\"\"\n",
    "        batch_size = actions.shape[0]\n",
    "        horizon = actions.shape[1]\n",
    "\n",
    "        state = {'deter': initial_state['deter'].clone(),\n",
    "                 'stoch': initial_state['stoch'].clone()}\n",
    "\n",
    "        states = []\n",
    "        obs_preds = []\n",
    "        reward_preds = []\n",
    "\n",
    "        for t in range(horizon):\n",
    "            gru_input = self.input_proj(torch.cat([state['stoch'], actions[:, t]], dim=-1))\n",
    "            deter = self.gru(gru_input, state['deter'])\n",
    "\n",
    "            prior_stats = self.prior(deter)\n",
    "            prior_dist = self.get_dist(prior_stats)\n",
    "            stoch = prior_dist.rsample()\n",
    "\n",
    "            full_state = torch.cat([deter, stoch], dim=-1)\n",
    "            states.append(full_state)\n",
    "\n",
    "            obs_stats = self.decoder(full_state)\n",
    "            obs_mean, _ = obs_stats.chunk(2, dim=-1)\n",
    "            obs_preds.append(obs_mean)\n",
    "\n",
    "            reward = self.reward_pred(full_state)\n",
    "            reward_preds.append(reward)\n",
    "\n",
    "            state = {'deter': deter, 'stoch': stoch}\n",
    "\n",
    "        return {\n",
    "            'states': torch.stack(states, dim=1),\n",
    "            'obs_pred': torch.stack(obs_preds, dim=1),\n",
    "            'reward_pred': torch.stack(reward_preds, dim=1).squeeze(-1)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6B.4 Component 3: Error Correction Ensemble\n",
    "\n",
    "Ensemble of gate-enhanced models with majority voting for robust predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ErrorCorrectionEnsemble total parameters: 22,217,045\n"
     ]
    }
   ],
   "source": [
    "class ErrorCorrectionEnsemble(nn.Module):\n",
    "    \"\"\"Ensemble of gate-enhanced world models with error correction.\"\"\"\n",
    "    \n",
    "    def __init__(self, obs_dim: int, action_dim: int, config: Dict, num_models: int = 5):\n",
    "        super().__init__()\n",
    "        self.num_models = num_models\n",
    "        self.models = nn.ModuleList([\n",
    "            GateEnhancedWorldModel(obs_dim, action_dim, config)\n",
    "            for _ in range(num_models)\n",
    "        ])\n",
    "        self.correction_method = config.get('correction_method', 'weighted')\n",
    "    \n",
    "    def forward(self, obs_seq, action_seq, return_all: bool = False):\n",
    "        # Get predictions from all models\n",
    "        all_outputs = [model(obs_seq, action_seq) for model in self.models]\n",
    "        \n",
    "        # Stack predictions\n",
    "        obs_preds = torch.stack([out['obs_mean'] for out in all_outputs], dim=0)\n",
    "        reward_preds = torch.stack([out['reward'] for out in all_outputs], dim=0)\n",
    "        \n",
    "        # Apply error correction\n",
    "        if self.correction_method == 'majority':\n",
    "            corrected_obs = obs_preds.median(dim=0).values\n",
    "            corrected_reward = reward_preds.median(dim=0).values\n",
    "        elif self.correction_method == 'weighted':\n",
    "            # Weight by inverse disagreement\n",
    "            disagreement = obs_preds.var(dim=0, keepdim=True)\n",
    "            weights = 1.0 / (disagreement.mean(dim=-1, keepdim=True) + 1e-6)\n",
    "            weights = weights / weights.sum(dim=0, keepdim=True)\n",
    "            corrected_obs = (obs_preds * weights).sum(dim=0)\n",
    "            corrected_reward = (reward_preds * weights.squeeze(-1)).sum(dim=0)\n",
    "        else:  # simple average\n",
    "            corrected_obs = obs_preds.mean(dim=0)\n",
    "            corrected_reward = reward_preds.mean(dim=0)\n",
    "        \n",
    "        # Calculate uncertainty\n",
    "        uncertainty = obs_preds.std(dim=0).mean(dim=-1)\n",
    "        \n",
    "        result = {\n",
    "            'obs_mean': corrected_obs,\n",
    "            'reward': corrected_reward,\n",
    "            'uncertainty': uncertainty,\n",
    "            'prior_mean': all_outputs[0]['prior_mean'],\n",
    "            'prior_std': all_outputs[0]['prior_std'],\n",
    "            'post_mean': all_outputs[0]['post_mean'],\n",
    "            'post_std': all_outputs[0]['post_std'],\n",
    "            'obs_std': all_outputs[0]['obs_std'],\n",
    "        }\n",
    "        \n",
    "        if return_all:\n",
    "            result['all_outputs'] = all_outputs\n",
    "        \n",
    "        return result\n",
    "\n",
    "# Test\n",
    "ensemble = ErrorCorrectionEnsemble(CONFIG['obs_dim'], CONFIG['action_dim'], CONFIG, num_models=5).to(DEVICE)\n",
    "print(f'ErrorCorrectionEnsemble total parameters: {sum(p.numel() for p in ensemble.parameters()):,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6B.5 Component 4: Superposition Replay Buffer\n",
    "\n",
    "Experience replay with quantum superposition-inspired sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SuperpositionReplayBuffer defined.\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class Episode:\n",
    "    observations: np.ndarray\n",
    "    actions: np.ndarray\n",
    "    rewards: np.ndarray\n",
    "    total_reward: float\n",
    "\n",
    "\n",
    "class SuperpositionReplayBuffer:\n",
    "    \"\"\"Replay buffer with quantum superposition-inspired sampling.\"\"\"\n",
    "    \n",
    "    def __init__(self, capacity: int = 1000, parallel_samples: int = 4):\n",
    "        self.capacity = capacity\n",
    "        self.parallel_samples = parallel_samples\n",
    "        self.episodes: List[Episode] = []\n",
    "        self.amplitudes: np.ndarray = np.array([])\n",
    "    \n",
    "    def add_episode(self, episode: Episode):\n",
    "        if len(self.episodes) >= self.capacity:\n",
    "            self.episodes.pop(0)\n",
    "        self.episodes.append(episode)\n",
    "        self._update_amplitudes()\n",
    "    \n",
    "    def _update_amplitudes(self):\n",
    "        \"\"\"Update quantum-like amplitudes based on episode quality.\"\"\"\n",
    "        if not self.episodes:\n",
    "            self.amplitudes = np.array([])\n",
    "            return\n",
    "        \n",
    "        rewards = np.array([ep.total_reward for ep in self.episodes])\n",
    "        # Normalize to create amplitude distribution\n",
    "        if rewards.std() > 0:\n",
    "            normalized = (rewards - rewards.mean()) / rewards.std()\n",
    "        else:\n",
    "            normalized = np.zeros_like(rewards)\n",
    "        \n",
    "        # Convert to probabilities (amplitude squared)\n",
    "        amplitudes = np.exp(normalized)\n",
    "        self.amplitudes = amplitudes / amplitudes.sum()\n",
    "    \n",
    "    def sample_superposition(self, batch_size: int, seq_len: int) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Sample using superposition-inspired parallel sampling.\"\"\"\n",
    "        if len(self.episodes) < self.parallel_samples:\n",
    "            return self.sample_standard(batch_size, seq_len)\n",
    "        \n",
    "        # Sample multiple episodes in parallel (superposition)\n",
    "        obs_batch, act_batch, rew_batch = [], [], []\n",
    "        \n",
    "        for _ in range(batch_size):\n",
    "            # Sample parallel_samples episodes based on amplitudes\n",
    "            indices = np.random.choice(\n",
    "                len(self.episodes), \n",
    "                size=self.parallel_samples,\n",
    "                p=self.amplitudes,\n",
    "                replace=True\n",
    "            )\n",
    "            \n",
    "            # \"Collapse\" to one by weighted combination\n",
    "            selected_amplitudes = self.amplitudes[indices]\n",
    "            collapse_probs = selected_amplitudes / selected_amplitudes.sum()\n",
    "            chosen_idx = np.random.choice(indices, p=collapse_probs)\n",
    "            \n",
    "            episode = self.episodes[chosen_idx]\n",
    "            \n",
    "            # Random start point\n",
    "            max_start = max(0, len(episode.observations) - seq_len)\n",
    "            start = np.random.randint(0, max_start + 1) if max_start > 0 else 0\n",
    "            end = min(start + seq_len, len(episode.observations))\n",
    "            \n",
    "            obs = episode.observations[start:end]\n",
    "            act = episode.actions[start:end]\n",
    "            rew = episode.rewards[start:end]\n",
    "            \n",
    "            # Pad if necessary\n",
    "            if len(obs) < seq_len:\n",
    "                pad_len = seq_len - len(obs)\n",
    "                obs = np.pad(obs, ((0, pad_len), (0, 0)), mode='edge')\n",
    "                act = np.pad(act, ((0, pad_len), (0, 0)), mode='edge')\n",
    "                rew = np.pad(rew, (0, pad_len), mode='edge')\n",
    "            \n",
    "            obs_batch.append(obs)\n",
    "            act_batch.append(act)\n",
    "            rew_batch.append(rew)\n",
    "        \n",
    "        return (\n",
    "            torch.FloatTensor(np.array(obs_batch)).to(DEVICE),\n",
    "            torch.FloatTensor(np.array(act_batch)).to(DEVICE),\n",
    "            torch.FloatTensor(np.array(rew_batch)).to(DEVICE)\n",
    "        )\n",
    "    \n",
    "    def sample_standard(self, batch_size: int, seq_len: int) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Standard uniform sampling.\"\"\"\n",
    "        obs_batch, act_batch, rew_batch = [], [], []\n",
    "        \n",
    "        for _ in range(batch_size):\n",
    "            episode = np.random.choice(self.episodes)\n",
    "            max_start = max(0, len(episode.observations) - seq_len)\n",
    "            start = np.random.randint(0, max_start + 1) if max_start > 0 else 0\n",
    "            end = min(start + seq_len, len(episode.observations))\n",
    "            \n",
    "            obs = episode.observations[start:end]\n",
    "            act = episode.actions[start:end]\n",
    "            rew = episode.rewards[start:end]\n",
    "            \n",
    "            if len(obs) < seq_len:\n",
    "                pad_len = seq_len - len(obs)\n",
    "                obs = np.pad(obs, ((0, pad_len), (0, 0)), mode='edge')\n",
    "                act = np.pad(act, ((0, pad_len), (0, 0)), mode='edge')\n",
    "                rew = np.pad(rew, (0, pad_len), mode='edge')\n",
    "            \n",
    "            obs_batch.append(obs)\n",
    "            act_batch.append(act)\n",
    "            rew_batch.append(rew)\n",
    "        \n",
    "        return (\n",
    "            torch.FloatTensor(np.array(obs_batch)).to(DEVICE),\n",
    "            torch.FloatTensor(np.array(act_batch)).to(DEVICE),\n",
    "            torch.FloatTensor(np.array(rew_batch)).to(DEVICE)\n",
    "        )\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.episodes)\n",
    "\n",
    "print('SuperpositionReplayBuffer defined.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6B.6 Component 5: QAOA-Inspired Optimizer\n",
    "\n",
    "Optimizer that alternates between cost and mixing operators to escape local minima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QAOAOptimizer defined.\n"
     ]
    }
   ],
   "source": [
    "class QAOAOptimizer:\n",
    "    \"\"\"QAOA-inspired optimizer with alternating cost/mixing phases.\"\"\"\n",
    "    \n",
    "    def __init__(self, params, lr: float = 3e-4, p_layers: int = 4,\n",
    "                 gamma_init: float = 0.1, beta_init: float = 0.001):\n",
    "        self.params = list(params)\n",
    "        self.lr = lr\n",
    "        self.p_layers = p_layers\n",
    "        \n",
    "        # QAOA parameters\n",
    "        self.gammas = [gamma_init] * p_layers\n",
    "        self.betas = [beta_init] * p_layers\n",
    "        \n",
    "        # Base optimizer\n",
    "        self.base_optimizer = torch.optim.Adam(self.params, lr=lr)\n",
    "        \n",
    "        # State tracking\n",
    "        self.step_count = 0\n",
    "        self.current_layer = 0\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        self.base_optimizer.zero_grad()\n",
    "    \n",
    "    def step(self):\n",
    "        \"\"\"Perform QAOA-inspired optimization step.\"\"\"\n",
    "        self.step_count += 1\n",
    "        layer_idx = self.current_layer % self.p_layers\n",
    "        \n",
    "        # Cost phase: gradient descent with gamma scaling\n",
    "        gamma = self.gammas[layer_idx]\n",
    "        with torch.no_grad():\n",
    "            for param in self.params:\n",
    "                if param.grad is not None:\n",
    "                    param.grad.mul_(gamma)\n",
    "        \n",
    "        self.base_optimizer.step()\n",
    "        \n",
    "        # Mixing phase: add exploration noise (every other step)\n",
    "        if self.step_count % 2 == 0:\n",
    "            beta = self.betas[layer_idx]\n",
    "            with torch.no_grad():\n",
    "                for param in self.params:\n",
    "                    if param.requires_grad:\n",
    "                        noise = torch.randn_like(param) * beta * self.lr\n",
    "                        param.add_(noise)\n",
    "            self.current_layer += 1\n",
    "    \n",
    "    def state_dict(self):\n",
    "        return {\n",
    "            'base_optimizer': self.base_optimizer.state_dict(),\n",
    "            'step_count': self.step_count,\n",
    "            'gammas': self.gammas,\n",
    "            'betas': self.betas,\n",
    "        }\n",
    "    \n",
    "    def load_state_dict(self, state_dict):\n",
    "        self.base_optimizer.load_state_dict(state_dict['base_optimizer'])\n",
    "        self.step_count = state_dict['step_count']\n",
    "        self.gammas = state_dict['gammas']\n",
    "        self.betas = state_dict['betas']\n",
    "\n",
    "print('QAOAOptimizer defined.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6B.7 Fully Integrated Trainer\n",
    "\n",
    "Training loop that combines all quantum-inspired components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FullyIntegratedTrainer defined.\n"
     ]
    }
   ],
   "source": [
    "class FullyIntegratedTrainer:\n",
    "    \"\"\"\n",
    "    Fully integrated trainer combining:\n",
    "    - QAOA optimizer (escapes local minima)\n",
    "    - Superposition replay (sample efficiency)\n",
    "    - Gate-enhanced layers (better representations)\n",
    "    - Error correction ensemble (robust predictions)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, ensemble: ErrorCorrectionEnsemble, buffer: SuperpositionReplayBuffer,\n",
    "                 config: Dict):\n",
    "        self.ensemble = ensemble\n",
    "        self.buffer = buffer\n",
    "        self.config = config\n",
    "        \n",
    "        # Create QAOA optimizer for each model in ensemble\n",
    "        self.optimizers = [\n",
    "            QAOAOptimizer(\n",
    "                model.parameters(),\n",
    "                lr=config.get('learning_rate', 3e-4),\n",
    "                p_layers=config.get('qaoa_layers', 4),\n",
    "                gamma_init=config.get('gamma_init', 0.1),\n",
    "                beta_init=config.get('beta_init', 0.001)\n",
    "            )\n",
    "            for model in ensemble.models\n",
    "        ]\n",
    "        \n",
    "        self.training_history = defaultdict(list)\n",
    "    \n",
    "    def compute_loss(self, outputs: Dict, obs_seq: torch.Tensor, reward_seq: torch.Tensor) -> Tuple[torch.Tensor, Dict]:\n",
    "        \"\"\"Compute world model loss.\"\"\"\n",
    "        # Reconstruction loss\n",
    "        recon_loss = F.mse_loss(outputs['obs_mean'], obs_seq)\n",
    "        \n",
    "        # KL divergence\n",
    "        prior_dist = torch.distributions.Normal(outputs['prior_mean'], outputs['prior_std'])\n",
    "        post_dist = torch.distributions.Normal(outputs['post_mean'], outputs['post_std'])\n",
    "        kl_loss = torch.distributions.kl_divergence(post_dist, prior_dist).mean()\n",
    "        kl_loss = torch.clamp(kl_loss, min=1.0)  # Free bits\n",
    "        \n",
    "        # Reward loss\n",
    "        reward_loss = F.mse_loss(outputs['reward'], reward_seq)\n",
    "        \n",
    "        # Total loss\n",
    "        total_loss = recon_loss + 0.1 * kl_loss + reward_loss\n",
    "        \n",
    "        metrics = {\n",
    "            'recon_loss': recon_loss.item(),\n",
    "            'kl_loss': kl_loss.item(),\n",
    "            'reward_loss': reward_loss.item(),\n",
    "            'total_loss': total_loss.item(),\n",
    "        }\n",
    "        \n",
    "        return total_loss, metrics\n",
    "    \n",
    "    def train_step(self, use_superposition: bool = True) -> Dict:\n",
    "        \"\"\"Single training step with all quantum components.\"\"\"\n",
    "        self.ensemble.train()\n",
    "        \n",
    "        # Sample using superposition replay\n",
    "        if use_superposition:\n",
    "            obs_seq, action_seq, reward_seq = self.buffer.sample_superposition(\n",
    "                self.config['batch_size'], self.config['seq_len']\n",
    "            )\n",
    "        else:\n",
    "            obs_seq, action_seq, reward_seq = self.buffer.sample_standard(\n",
    "                self.config['batch_size'], self.config['seq_len']\n",
    "            )\n",
    "        \n",
    "        total_loss = 0.0\n",
    "        all_metrics = defaultdict(list)\n",
    "        \n",
    "        # Train each model in ensemble with QAOA optimizer\n",
    "        for model, optimizer in zip(self.ensemble.models, self.optimizers):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(obs_seq, action_seq)\n",
    "            loss, metrics = self.compute_loss(outputs, obs_seq, reward_seq)\n",
    "            \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 100.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            for k, v in metrics.items():\n",
    "                all_metrics[k].append(v)\n",
    "        \n",
    "        # Average metrics\n",
    "        avg_metrics = {k: np.mean(v) for k, v in all_metrics.items()}\n",
    "        avg_metrics['ensemble_loss'] = total_loss / len(self.ensemble.models)\n",
    "        \n",
    "        return avg_metrics\n",
    "    \n",
    "    def train(self, num_epochs: int, use_superposition: bool = True) -> Dict:\n",
    "        \"\"\"Full training loop.\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            epoch_metrics = defaultdict(list)\n",
    "            \n",
    "            # Multiple steps per epoch\n",
    "            num_steps = max(1, len(self.buffer) // self.config['batch_size'])\n",
    "            for _ in range(num_steps):\n",
    "                metrics = self.train_step(use_superposition)\n",
    "                for k, v in metrics.items():\n",
    "                    epoch_metrics[k].append(v)\n",
    "            \n",
    "            # Average epoch metrics\n",
    "            for k, v in epoch_metrics.items():\n",
    "                self.training_history[k].append(np.mean(v))\n",
    "            \n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                print(f\"Epoch {epoch+1}/{num_epochs}: Loss = {self.training_history['total_loss'][-1]:.4f}\")\n",
    "        \n",
    "        training_time = time.time() - start_time\n",
    "        self.training_history['training_time'] = training_time\n",
    "        \n",
    "        return dict(self.training_history)\n",
    "    \n",
    "    def evaluate(self, test_obs: torch.Tensor, test_actions: torch.Tensor) -> Dict:\n",
    "        \"\"\"Evaluate the ensemble.\"\"\"\n",
    "        self.ensemble.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = self.ensemble(test_obs, test_actions, return_all=True)\n",
    "            \n",
    "            # Ensemble prediction error\n",
    "            ensemble_mse = F.mse_loss(outputs['obs_mean'], test_obs).item()\n",
    "            \n",
    "            # Individual model errors\n",
    "            individual_mses = []\n",
    "            for out in outputs['all_outputs']:\n",
    "                mse = F.mse_loss(out['obs_mean'], test_obs).item()\n",
    "                individual_mses.append(mse)\n",
    "            \n",
    "            # Uncertainty\n",
    "            mean_uncertainty = outputs['uncertainty'].mean().item()\n",
    "        \n",
    "        return {\n",
    "            'ensemble_mse': ensemble_mse,\n",
    "            'avg_individual_mse': np.mean(individual_mses),\n",
    "            'best_individual_mse': min(individual_mses),\n",
    "            'worst_individual_mse': max(individual_mses),\n",
    "            'mean_uncertainty': mean_uncertainty,\n",
    "        }\n",
    "\n",
    "print('FullyIntegratedTrainer defined.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6B.8 Long-Horizon Prediction Test\n",
    "\n",
    "Test multi-step prediction accuracy (critical for world models used in planning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long-horizon evaluation function defined.\n",
      "Standard horizons: [5, 10, 15, 20, 30, 40, 50]\n"
     ]
    }
   ],
   "source": [
    "# Standard horizons per CLAUDE.md\n",
    "LONG_HORIZON_HORIZONS = [5, 10, 15, 20, 30, 40, 50]\n",
    "\n",
    "def evaluate_long_horizon(model, obs_seq: torch.Tensor, action_seq: torch.Tensor, \n",
    "                          horizons: List[int] = None) -> Dict:\n",
    "    \"\"\"\n",
    "    Evaluate prediction accuracy at different horizons.\n",
    "    \n",
    "    This is critical because world models are used for PLANNING,\n",
    "    which requires accurate predictions many steps into the future.\n",
    "    \"\"\"\n",
    "    if horizons is None:\n",
    "        horizons = LONG_HORIZON_HORIZONS\n",
    "    \n",
    "    model.eval()\n",
    "    results = {}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Get model predictions\n",
    "        if hasattr(model, 'models'):  # Ensemble\n",
    "            outputs = model(obs_seq, action_seq)\n",
    "            pred_obs = outputs['obs_mean']\n",
    "        else:  # Single model\n",
    "            outputs = model(obs_seq, action_seq)\n",
    "            pred_obs = outputs['obs_mean']\n",
    "        \n",
    "        # Calculate error at each horizon\n",
    "        for h in horizons:\n",
    "            if h <= obs_seq.shape[1]:\n",
    "                # MSE at horizon h\n",
    "                mse_at_h = F.mse_loss(pred_obs[:, h-1:h], obs_seq[:, h-1:h]).item()\n",
    "                results[f'mse_horizon_{h}'] = mse_at_h\n",
    "        \n",
    "        # Average error over all horizons\n",
    "        results['avg_horizon_mse'] = np.mean([v for k, v in results.items() if 'mse_horizon' in k])\n",
    "        \n",
    "        # Error growth rate (how fast does error compound?)\n",
    "        if len(horizons) >= 2:\n",
    "            first_h = horizons[0]\n",
    "            last_h = min(horizons[-1], obs_seq.shape[1])\n",
    "            if f'mse_horizon_{first_h}' in results and f'mse_horizon_{last_h}' in results:\n",
    "                error_growth = results[f'mse_horizon_{last_h}'] / (results[f'mse_horizon_{first_h}'] + 1e-8)\n",
    "                results['error_growth_rate'] = error_growth\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(f'Long-horizon evaluation function defined.')\n",
    "print(f'Standard horizons: {LONG_HORIZON_HORIZONS}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6B.9 Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting training data...\n",
      "Collected 100 episodes, avg reward: 21.4, avg length: 21.4\n"
     ]
    }
   ],
   "source": [
    "def collect_episodes(env_name: str, num_episodes: int, seed: int = 42) -> List[Episode]:\n",
    "    \"\"\"Collect episodes from environment.\"\"\"\n",
    "    env = gym.make(env_name)\n",
    "    episodes = []\n",
    "    \n",
    "    for ep in range(num_episodes):\n",
    "        obs, _ = env.reset(seed=seed + ep)\n",
    "        observations, actions, rewards = [obs], [], []\n",
    "        \n",
    "        done = False\n",
    "        while not done:\n",
    "            action = env.action_space.sample()\n",
    "            next_obs, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            # One-hot encode action for discrete spaces\n",
    "            if isinstance(env.action_space, gym.spaces.Discrete):\n",
    "                action_onehot = np.zeros(env.action_space.n)\n",
    "                action_onehot[action] = 1\n",
    "                actions.append(action_onehot)\n",
    "            else:\n",
    "                actions.append(action)\n",
    "            \n",
    "            rewards.append(reward)\n",
    "            observations.append(next_obs)\n",
    "        \n",
    "        episodes.append(Episode(\n",
    "            observations=np.array(observations[:-1]),\n",
    "            actions=np.array(actions),\n",
    "            rewards=np.array(rewards),\n",
    "            total_reward=sum(rewards)\n",
    "        ))\n",
    "    \n",
    "    env.close()\n",
    "    avg_reward = np.mean([ep.total_reward for ep in episodes])\n",
    "    avg_length = np.mean([len(ep.observations) for ep in episodes])\n",
    "    print(f'Collected {num_episodes} episodes, avg reward: {avg_reward:.1f}, avg length: {avg_length:.1f}')\n",
    "    return episodes\n",
    "\n",
    "# Collect data\n",
    "print('Collecting training data...')\n",
    "episodes = collect_episodes('CartPole-v1', CONFIG['num_episodes'], seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6B.10 Classical Baseline for Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassicalWorldModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Standard world model without quantum enhancements (for comparison).\n",
    "\n",
    "    Matches baseline architecture from notebook 02 EXACTLY:\n",
    "    - input_proj: Linear(stoch + action, hidden) + ELU\n",
    "    - gru: GRUCell(hidden, deter)\n",
    "    - prior: 2-layer\n",
    "    - posterior: 2-layer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, obs_dim: int, action_dim: int, config: Dict):\n",
    "        super().__init__()\n",
    "        self.obs_dim = obs_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.stoch_dim = config.get('stoch_dim', 64)\n",
    "        self.deter_dim = config.get('deter_dim', 512)\n",
    "        self.hidden_dim = config.get('hidden_dim', 512)\n",
    "        self.state_dim = self.stoch_dim + self.deter_dim\n",
    "\n",
    "        predictor_hidden = [512, 512]\n",
    "\n",
    "        # Standard encoder (matching baseline [512, 512] architecture)\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(obs_dim, self.hidden_dim),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(self.hidden_dim, self.hidden_dim),\n",
    "            nn.ELU(),\n",
    "        )\n",
    "\n",
    "        # RSSM - matches baseline EXACTLY\n",
    "        self.input_proj = nn.Sequential(\n",
    "            nn.Linear(self.stoch_dim + action_dim, self.hidden_dim),\n",
    "            nn.ELU()\n",
    "        )\n",
    "\n",
    "        self.gru = nn.GRUCell(self.hidden_dim, self.deter_dim)\n",
    "\n",
    "        # Prior: 2-layer (matches baseline)\n",
    "        self.prior = nn.Sequential(\n",
    "            nn.Linear(self.deter_dim, self.hidden_dim),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(self.hidden_dim, self.stoch_dim * 2)\n",
    "        )\n",
    "\n",
    "        # Posterior: 2-layer (matches baseline)\n",
    "        self.posterior = nn.Sequential(\n",
    "            nn.Linear(self.deter_dim + self.hidden_dim, self.hidden_dim),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(self.hidden_dim, self.stoch_dim * 2)\n",
    "        )\n",
    "\n",
    "        # Standard decoder (matching baseline [512, 512] architecture)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(self.state_dim, self.hidden_dim),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(self.hidden_dim, self.hidden_dim),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(self.hidden_dim, obs_dim * 2)\n",
    "        )\n",
    "\n",
    "        # Reward predictor (using predictor_hidden)\n",
    "        rew_layers = []\n",
    "        in_d = self.state_dim\n",
    "        for h in predictor_hidden:\n",
    "            rew_layers.extend([nn.Linear(in_d, h), nn.ELU()])\n",
    "            in_d = h\n",
    "        rew_layers.append(nn.Linear(in_d, 1))\n",
    "        self.reward_pred = nn.Sequential(*rew_layers)\n",
    "\n",
    "    def initial_state(self, batch_size: int):\n",
    "        return {\n",
    "            'deter': torch.zeros(batch_size, self.deter_dim, device=DEVICE),\n",
    "            'stoch': torch.zeros(batch_size, self.stoch_dim, device=DEVICE)\n",
    "        }\n",
    "\n",
    "    def get_dist(self, stats):\n",
    "        mean, log_std = stats.chunk(2, dim=-1)\n",
    "        std = F.softplus(log_std) + 0.1\n",
    "        return torch.distributions.Normal(mean, std)\n",
    "\n",
    "    def forward(self, obs_seq, action_seq):\n",
    "        batch_size, seq_len = obs_seq.shape[:2]\n",
    "\n",
    "        state = self.initial_state(batch_size)\n",
    "\n",
    "        prior_means, prior_stds = [], []\n",
    "        post_means, post_stds = [], []\n",
    "        obs_means, obs_stds = [], []\n",
    "        rewards = []\n",
    "        states = []\n",
    "\n",
    "        for t in range(seq_len):\n",
    "            embed = self.encoder(obs_seq[:, t])\n",
    "\n",
    "            prior_stats = self.prior(state['deter'])\n",
    "            prior_dist = self.get_dist(prior_stats)\n",
    "\n",
    "            post_input = torch.cat([state['deter'], embed], dim=-1)\n",
    "            post_stats = self.posterior(post_input)\n",
    "            post_dist = self.get_dist(post_stats)\n",
    "\n",
    "            stoch = post_dist.rsample()\n",
    "\n",
    "            gru_input = self.input_proj(torch.cat([stoch, action_seq[:, t]], dim=-1))\n",
    "            deter = self.gru(gru_input, state['deter'])\n",
    "\n",
    "            full_state = torch.cat([deter, stoch], dim=-1)\n",
    "\n",
    "            obs_stats = self.decoder(full_state)\n",
    "            obs_mean, obs_log_std = obs_stats.chunk(2, dim=-1)\n",
    "            obs_std = F.softplus(obs_log_std) + 0.1\n",
    "\n",
    "            reward = self.reward_pred(full_state)\n",
    "\n",
    "            prior_means.append(prior_dist.mean)\n",
    "            prior_stds.append(prior_dist.stddev)\n",
    "            post_means.append(post_dist.mean)\n",
    "            post_stds.append(post_dist.stddev)\n",
    "            obs_means.append(obs_mean)\n",
    "            obs_stds.append(obs_std)\n",
    "            rewards.append(reward)\n",
    "            states.append(full_state)\n",
    "\n",
    "            state = {'deter': deter, 'stoch': stoch}\n",
    "\n",
    "        return {\n",
    "            'prior_mean': torch.stack(prior_means, dim=1),\n",
    "            'prior_std': torch.stack(prior_stds, dim=1),\n",
    "            'post_mean': torch.stack(post_means, dim=1),\n",
    "            'post_std': torch.stack(post_stds, dim=1),\n",
    "            'obs_mean': torch.stack(obs_means, dim=1),\n",
    "            'obs_std': torch.stack(obs_stds, dim=1),\n",
    "            'reward': torch.stack(rewards, dim=1).squeeze(-1),\n",
    "            'states': torch.stack(states, dim=1)\n",
    "        }\n",
    "\n",
    "    def imagine(self, initial_state, actions):\n",
    "        \"\"\"Imagine future states given actions.\"\"\"\n",
    "        batch_size = actions.shape[0]\n",
    "        horizon = actions.shape[1]\n",
    "\n",
    "        state = {'deter': initial_state['deter'].clone(),\n",
    "                 'stoch': initial_state['stoch'].clone()}\n",
    "\n",
    "        states = []\n",
    "        obs_preds = []\n",
    "        reward_preds = []\n",
    "\n",
    "        for t in range(horizon):\n",
    "            gru_input = self.input_proj(torch.cat([state['stoch'], actions[:, t]], dim=-1))\n",
    "            deter = self.gru(gru_input, state['deter'])\n",
    "\n",
    "            prior_stats = self.prior(deter)\n",
    "            prior_dist = self.get_dist(prior_stats)\n",
    "            stoch = prior_dist.rsample()\n",
    "\n",
    "            full_state = torch.cat([deter, stoch], dim=-1)\n",
    "            states.append(full_state)\n",
    "\n",
    "            obs_stats = self.decoder(full_state)\n",
    "            obs_mean, _ = obs_stats.chunk(2, dim=-1)\n",
    "            obs_preds.append(obs_mean)\n",
    "\n",
    "            reward = self.reward_pred(full_state)\n",
    "            reward_preds.append(reward)\n",
    "\n",
    "            state = {'deter': deter, 'stoch': stoch}\n",
    "\n",
    "        return {\n",
    "            'states': torch.stack(states, dim=1),\n",
    "            'obs_pred': torch.stack(obs_preds, dim=1),\n",
    "            'reward_pred': torch.stack(reward_preds, dim=1).squeeze(-1)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6B.11 Multi-Seed Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Running experiment with seed 42\n",
      "============================================================\n",
      "Collected 100 episodes, avg reward: 24.3, avg length: 24.3\n",
      "\n",
      "Training Classical Baseline...\n",
      "  Epoch 25: Loss = 0.1402\n",
      "  Epoch 50: Loss = 0.1243\n",
      "\n",
      "Training Fully Integrated (QAOA + Superposition + Gates + Ensemble)...\n",
      "Epoch 10/50: Loss = 0.1290\n",
      "Epoch 20/50: Loss = 0.1069\n",
      "Epoch 30/50: Loss = 0.1047\n",
      "Epoch 40/50: Loss = 0.1026\n",
      "Epoch 50/50: Loss = 0.1022\n",
      "\n",
      "Seed 42 Results:\n",
      "  Classical - Loss: 0.1243, MSE: 0.010802\n",
      "  Integrated - Loss: 0.1022, MSE: 1.279850\n",
      "\n",
      "============================================================\n",
      "Running experiment with seed 123\n",
      "============================================================\n",
      "Collected 100 episodes, avg reward: 21.2, avg length: 21.2\n",
      "\n",
      "Training Classical Baseline...\n",
      "  Epoch 25: Loss = 0.1423\n",
      "  Epoch 50: Loss = 0.1152\n",
      "\n",
      "Training Fully Integrated (QAOA + Superposition + Gates + Ensemble)...\n",
      "Epoch 10/50: Loss = 0.1238\n",
      "Epoch 20/50: Loss = 0.1092\n",
      "Epoch 30/50: Loss = 0.1051\n",
      "Epoch 40/50: Loss = 0.1039\n",
      "Epoch 50/50: Loss = 0.1031\n",
      "\n",
      "Seed 123 Results:\n",
      "  Classical - Loss: 0.1152, MSE: 0.008596\n",
      "  Integrated - Loss: 0.1031, MSE: 2.506308\n",
      "\n",
      "============================================================\n",
      "Running experiment with seed 456\n",
      "============================================================\n",
      "Collected 100 episodes, avg reward: 23.4, avg length: 23.4\n",
      "\n",
      "Training Classical Baseline...\n",
      "  Epoch 25: Loss = 0.1416\n",
      "  Epoch 50: Loss = 0.1212\n",
      "\n",
      "Training Fully Integrated (QAOA + Superposition + Gates + Ensemble)...\n",
      "Epoch 10/50: Loss = 0.1334\n",
      "Epoch 20/50: Loss = 0.1073\n",
      "Epoch 30/50: Loss = 0.1039\n",
      "Epoch 40/50: Loss = 0.1022\n",
      "Epoch 50/50: Loss = 0.1022\n",
      "\n",
      "Seed 456 Results:\n",
      "  Classical - Loss: 0.1212, MSE: 0.013214\n",
      "  Integrated - Loss: 0.1022, MSE: 2.450050\n",
      "\n",
      "============================================================\n",
      "Running experiment with seed 789\n",
      "============================================================\n",
      "Collected 100 episodes, avg reward: 23.0, avg length: 23.0\n",
      "\n",
      "Training Classical Baseline...\n",
      "  Epoch 25: Loss = 0.1353\n",
      "  Epoch 50: Loss = 0.1178\n",
      "\n",
      "Training Fully Integrated (QAOA + Superposition + Gates + Ensemble)...\n",
      "Epoch 10/50: Loss = 0.1313\n",
      "Epoch 20/50: Loss = 0.1081\n",
      "Epoch 30/50: Loss = 0.1038\n",
      "Epoch 40/50: Loss = 0.1027\n",
      "Epoch 50/50: Loss = 0.1020\n",
      "\n",
      "Seed 789 Results:\n",
      "  Classical - Loss: 0.1178, MSE: 0.010606\n",
      "  Integrated - Loss: 0.1020, MSE: 2.466587\n",
      "\n",
      "============================================================\n",
      "Running experiment with seed 1024\n",
      "============================================================\n",
      "Collected 100 episodes, avg reward: 21.6, avg length: 21.6\n",
      "\n",
      "Training Classical Baseline...\n",
      "  Epoch 25: Loss = 0.1371\n",
      "  Epoch 50: Loss = 0.1219\n",
      "\n",
      "Training Fully Integrated (QAOA + Superposition + Gates + Ensemble)...\n",
      "Epoch 10/50: Loss = 0.1338\n",
      "Epoch 20/50: Loss = 0.1084\n",
      "Epoch 30/50: Loss = 0.1051\n",
      "Epoch 40/50: Loss = 0.1031\n",
      "Epoch 50/50: Loss = 0.1024\n",
      "\n",
      "Seed 1024 Results:\n",
      "  Classical - Loss: 0.1219, MSE: 0.009464\n",
      "  Integrated - Loss: 0.1024, MSE: 1.818131\n"
     ]
    }
   ],
   "source": [
    "def run_experiment(seed: int, config: Dict) -> Dict:\n",
    "    \"\"\"Run single experiment with given seed.\"\"\"\n",
    "    print(f'\\n{\"=\"*60}')\n",
    "    print(f'Running experiment with seed {seed}')\n",
    "    print(f'{\"=\"*60}')\n",
    "    \n",
    "    # Set seeds\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Collect data\n",
    "    episodes = collect_episodes('CartPole-v1', config['num_episodes'], seed=seed)\n",
    "    \n",
    "    # Prepare buffer\n",
    "    buffer = SuperpositionReplayBuffer(capacity=1000, parallel_samples=4)\n",
    "    for ep in episodes:\n",
    "        buffer.add_episode(ep)\n",
    "    \n",
    "    # Prepare test data - filter episodes that are long enough\n",
    "    seq_len = config['seq_len']\n",
    "    valid_episodes = [ep for ep in episodes if len(ep.observations) >= seq_len]\n",
    "    test_episodes = valid_episodes[-10:] if len(valid_episodes) >= 10 else valid_episodes\n",
    "    \n",
    "    # Stack with consistent shapes\n",
    "    test_obs = torch.FloatTensor(np.stack([ep.observations[:seq_len] for ep in test_episodes])).to(DEVICE)\n",
    "    test_actions = torch.FloatTensor(np.stack([ep.actions[:seq_len] for ep in test_episodes])).to(DEVICE)\n",
    "    test_rewards = torch.FloatTensor(np.stack([ep.rewards[:seq_len] for ep in test_episodes])).to(DEVICE)\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # ============================================================\n",
    "    # Train Classical Baseline\n",
    "    # ============================================================\n",
    "    print('\\nTraining Classical Baseline...')\n",
    "    classical_model = ClassicalWorldModel(config['obs_dim'], config['action_dim'], config).to(DEVICE)\n",
    "    classical_optimizer = torch.optim.Adam(classical_model.parameters(), lr=config['learning_rate'])\n",
    "    \n",
    "    classical_start = time.time()\n",
    "    classical_losses = []\n",
    "    \n",
    "    for epoch in range(config['num_epochs']):\n",
    "        classical_model.train()\n",
    "        obs, act, rew = buffer.sample_standard(config['batch_size'], config['seq_len'])\n",
    "        \n",
    "        classical_optimizer.zero_grad()\n",
    "        outputs = classical_model(obs, act)\n",
    "        \n",
    "        recon_loss = F.mse_loss(outputs['obs_mean'], obs)\n",
    "        prior_dist = torch.distributions.Normal(outputs['prior_mean'], outputs['prior_std'])\n",
    "        post_dist = torch.distributions.Normal(outputs['post_mean'], outputs['post_std'])\n",
    "        kl_loss = torch.clamp(torch.distributions.kl_divergence(post_dist, prior_dist).mean(), min=1.0)\n",
    "        reward_loss = F.mse_loss(outputs['reward'], rew)\n",
    "        loss = recon_loss + 0.1 * kl_loss + reward_loss\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(classical_model.parameters(), 100.0)\n",
    "        classical_optimizer.step()\n",
    "        \n",
    "        classical_losses.append(loss.item())\n",
    "        \n",
    "        if (epoch + 1) % 25 == 0:\n",
    "            print(f'  Epoch {epoch+1}: Loss = {loss.item():.4f}')\n",
    "    \n",
    "    classical_time = time.time() - classical_start\n",
    "    \n",
    "    # Evaluate classical\n",
    "    classical_model.eval()\n",
    "    with torch.no_grad():\n",
    "        classical_outputs = classical_model(test_obs, test_actions)\n",
    "        classical_mse = F.mse_loss(classical_outputs['obs_mean'], test_obs).item()\n",
    "    \n",
    "    classical_horizon = evaluate_long_horizon(classical_model, test_obs, test_actions)\n",
    "    \n",
    "    results['classical'] = {\n",
    "        'final_loss': classical_losses[-1],\n",
    "        'test_mse': classical_mse,\n",
    "        'training_time': classical_time,\n",
    "        'long_horizon': classical_horizon,\n",
    "        'params': sum(p.numel() for p in classical_model.parameters()),\n",
    "        'history': classical_losses,  # Training loss history\n",
    "    }\n",
    "    \n",
    "    # ============================================================\n",
    "    # Train Fully Integrated\n",
    "    # ============================================================\n",
    "    print('\\nTraining Fully Integrated (QAOA + Superposition + Gates + Ensemble)...')\n",
    "    \n",
    "    integrated_ensemble = ErrorCorrectionEnsemble(\n",
    "        config['obs_dim'], config['action_dim'], config, num_models=config['num_ensemble']\n",
    "    ).to(DEVICE)\n",
    "    \n",
    "    integrated_trainer = FullyIntegratedTrainer(integrated_ensemble, buffer, config)\n",
    "    \n",
    "    integrated_start = time.time()\n",
    "    integrated_history = integrated_trainer.train(config['num_epochs'], use_superposition=True)\n",
    "    integrated_time = time.time() - integrated_start\n",
    "    \n",
    "    # Evaluate integrated\n",
    "    integrated_eval = integrated_trainer.evaluate(test_obs, test_actions)\n",
    "    integrated_horizon = evaluate_long_horizon(integrated_ensemble, test_obs, test_actions)\n",
    "    \n",
    "    results['integrated'] = {\n",
    "        'final_loss': integrated_history['total_loss'][-1],\n",
    "        'test_mse': integrated_eval['ensemble_mse'],\n",
    "        'training_time': integrated_time,\n",
    "        'long_horizon': integrated_horizon,\n",
    "        'uncertainty': integrated_eval['mean_uncertainty'],\n",
    "        'params': sum(p.numel() for p in integrated_ensemble.parameters()),\n",
    "        'history': integrated_history['total_loss'],  # Training loss history\n",
    "    }\n",
    "    \n",
    "    print(f'\\nSeed {seed} Results:')\n",
    "    print(f'  Classical - Loss: {results[\"classical\"][\"final_loss\"]:.4f}, MSE: {results[\"classical\"][\"test_mse\"]:.6f}')\n",
    "    print(f'  Integrated - Loss: {results[\"integrated\"][\"final_loss\"]:.4f}, MSE: {results[\"integrated\"][\"test_mse\"]:.6f}')\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run experiments\n",
    "all_results = []\n",
    "for seed in CONFIG['seeds']:\n",
    "    result = run_experiment(seed, CONFIG)\n",
    "    all_results.append(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6B.11b Test Set Evaluation\n",
    "\n",
    "Evaluate models on held-out data using a **different seed** (9999) to test generalization.\n",
    "This ensures the models can predict dynamics for data they've never seen during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "TEST SET EVALUATION\n",
      "======================================================================\n",
      "\n",
      "Collecting held-out test data with seed 9999 (never seen during training)...\n",
      "Collected 50 episodes, avg reward: 21.0, avg length: 21.0\n",
      "Test set: 20 episodes, shape: torch.Size([20, 20, 4])\n",
      "\n",
      "Training fresh models for test set evaluation...\n",
      "\n",
      "  Seed 42:\n",
      "Collected 100 episodes, avg reward: 22.9, avg length: 22.9\n",
      "    Classical test MSE: 0.012371\n",
      "Epoch 10/50: Loss = 0.1318\n",
      "Epoch 20/50: Loss = 0.1076\n",
      "Epoch 30/50: Loss = 0.1043\n",
      "Epoch 40/50: Loss = 0.1025\n",
      "Epoch 50/50: Loss = 0.1016\n",
      "    Integrated test MSE: 1.840492\n",
      "\n",
      "  Seed 123:\n",
      "Collected 100 episodes, avg reward: 21.7, avg length: 21.7\n",
      "    Classical test MSE: 0.011191\n",
      "Epoch 10/50: Loss = 0.1303\n",
      "Epoch 20/50: Loss = 0.1131\n",
      "Epoch 30/50: Loss = 0.1053\n",
      "Epoch 40/50: Loss = 0.1030\n",
      "Epoch 50/50: Loss = 0.1021\n",
      "    Integrated test MSE: 2.046876\n",
      "\n",
      "  Seed 456:\n",
      "Collected 100 episodes, avg reward: 24.1, avg length: 24.1\n",
      "    Classical test MSE: 0.010533\n",
      "Epoch 10/50: Loss = 0.1274\n",
      "Epoch 20/50: Loss = 0.1078\n",
      "Epoch 30/50: Loss = 0.1058\n",
      "Epoch 40/50: Loss = 0.1036\n",
      "Epoch 50/50: Loss = 0.1021\n",
      "    Integrated test MSE: 1.927561\n",
      "\n",
      "======================================================================\n",
      "TEST SET RESULTS (Generalization Performance)\n",
      "======================================================================\n",
      "\n",
      "Classical Test MSE:  0.011365 +/- 0.000760\n",
      "Integrated Test MSE: 1.938310 +/- 0.084598\n",
      "\n",
      "Generalization Gap (Test MSE - Train MSE):\n",
      "  Classical:  +0.000829 (overfitting)\n",
      "  Integrated: -0.165876 (good generalization)\n"
     ]
    }
   ],
   "source": [
    "# Test Set Evaluation - use different seed (9999) for held-out data\n",
    "print('='*70)\n",
    "print('TEST SET EVALUATION')\n",
    "print('='*70)\n",
    "print()\n",
    "print('Collecting held-out test data with seed 9999 (never seen during training)...')\n",
    "\n",
    "TEST_SEED = 9999\n",
    "test_episodes = collect_episodes('CartPole-v1', 50, seed=TEST_SEED)\n",
    "\n",
    "# Prepare test data\n",
    "seq_len = CONFIG['seq_len']\n",
    "valid_test_eps = [ep for ep in test_episodes if len(ep.observations) >= seq_len]\n",
    "test_batch = valid_test_eps[:20] if len(valid_test_eps) >= 20 else valid_test_eps\n",
    "\n",
    "test_obs = torch.FloatTensor(np.stack([ep.observations[:seq_len] for ep in test_batch])).to(DEVICE)\n",
    "test_actions = torch.FloatTensor(np.stack([ep.actions[:seq_len] for ep in test_batch])).to(DEVICE)\n",
    "test_rewards = torch.FloatTensor(np.stack([ep.rewards[:seq_len] for ep in test_batch])).to(DEVICE)\n",
    "\n",
    "print(f'Test set: {len(test_batch)} episodes, shape: {test_obs.shape}')\n",
    "print()\n",
    "\n",
    "# Evaluate all trained models from multi-seed experiments\n",
    "test_results = {'classical': [], 'integrated': []}\n",
    "\n",
    "# We need to retrain one model of each type to evaluate on test set\n",
    "# Or use the last trained models if still in memory\n",
    "print('Training fresh models for test set evaluation...')\n",
    "\n",
    "for seed in EXPERIMENT_SEEDS[:3]:  # Use first 3 seeds for efficiency\n",
    "    print(f'\\n  Seed {seed}:')\n",
    "    \n",
    "    # Set seed\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Collect training data\n",
    "    train_episodes = collect_episodes('CartPole-v1', CONFIG['num_episodes'], seed=seed)\n",
    "    \n",
    "    # Prepare buffer\n",
    "    buffer = SuperpositionReplayBuffer(capacity=1000, parallel_samples=4)\n",
    "    for ep in train_episodes:\n",
    "        buffer.add_episode(ep)\n",
    "    \n",
    "    # Train Classical\n",
    "    classical_model = ClassicalWorldModel(CONFIG['obs_dim'], CONFIG['action_dim'], CONFIG).to(DEVICE)\n",
    "    classical_optimizer = torch.optim.Adam(classical_model.parameters(), lr=CONFIG['learning_rate'])\n",
    "    \n",
    "    for epoch in range(CONFIG['num_epochs']):\n",
    "        classical_model.train()\n",
    "        obs, act, rew = buffer.sample_standard(CONFIG['batch_size'], CONFIG['seq_len'])\n",
    "        classical_optimizer.zero_grad()\n",
    "        outputs = classical_model(obs, act)\n",
    "        recon_loss = F.mse_loss(outputs['obs_mean'], obs)\n",
    "        prior_dist = torch.distributions.Normal(outputs['prior_mean'], outputs['prior_std'])\n",
    "        post_dist = torch.distributions.Normal(outputs['post_mean'], outputs['post_std'])\n",
    "        kl_loss = torch.clamp(torch.distributions.kl_divergence(post_dist, prior_dist).mean(), min=1.0)\n",
    "        reward_loss = F.mse_loss(outputs['reward'], rew)\n",
    "        loss = recon_loss + 0.1 * kl_loss + reward_loss\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(classical_model.parameters(), 100.0)\n",
    "        classical_optimizer.step()\n",
    "    \n",
    "    # Evaluate on TEST set\n",
    "    classical_model.eval()\n",
    "    with torch.no_grad():\n",
    "        classical_out = classical_model(test_obs, test_actions)\n",
    "        classical_test_mse = F.mse_loss(classical_out['obs_mean'], test_obs).item()\n",
    "    test_results['classical'].append(classical_test_mse)\n",
    "    print(f'    Classical test MSE: {classical_test_mse:.6f}')\n",
    "    \n",
    "    # Train Integrated\n",
    "    integrated_ensemble = ErrorCorrectionEnsemble(\n",
    "        CONFIG['obs_dim'], CONFIG['action_dim'], CONFIG, num_models=CONFIG['num_ensemble']\n",
    "    ).to(DEVICE)\n",
    "    integrated_trainer = FullyIntegratedTrainer(integrated_ensemble, buffer, CONFIG)\n",
    "    integrated_trainer.train(CONFIG['num_epochs'], use_superposition=True)\n",
    "    \n",
    "    # Evaluate on TEST set\n",
    "    integrated_eval = integrated_trainer.evaluate(test_obs, test_actions)\n",
    "    test_results['integrated'].append(integrated_eval['ensemble_mse'])\n",
    "    print(f'    Integrated test MSE: {integrated_eval[\"ensemble_mse\"]:.6f}')\n",
    "\n",
    "# Compute test set statistics\n",
    "print()\n",
    "print('='*70)\n",
    "print('TEST SET RESULTS (Generalization Performance)')\n",
    "print('='*70)\n",
    "print()\n",
    "print(f'Classical Test MSE:  {np.mean(test_results[\"classical\"]):.6f} +/- {np.std(test_results[\"classical\"]):.6f}')\n",
    "print(f'Integrated Test MSE: {np.mean(test_results[\"integrated\"]):.6f} +/- {np.std(test_results[\"integrated\"]):.6f}')\n",
    "\n",
    "# Compute generalization gap (difference between training MSE and test MSE)\n",
    "train_classical_mse = np.mean([r['classical']['test_mse'] for r in all_results])\n",
    "train_integrated_mse = np.mean([r['integrated']['test_mse'] for r in all_results])\n",
    "\n",
    "classical_gap = np.mean(test_results['classical']) - train_classical_mse\n",
    "integrated_gap = np.mean(test_results['integrated']) - train_integrated_mse\n",
    "\n",
    "print()\n",
    "print('Generalization Gap (Test MSE - Train MSE):')\n",
    "print(f'  Classical:  {classical_gap:+.6f} ({\"overfitting\" if classical_gap > 0 else \"good generalization\"})')\n",
    "print(f'  Integrated: {integrated_gap:+.6f} ({\"overfitting\" if integrated_gap > 0 else \"good generalization\"})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6B.12 Statistical Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "STATISTICAL ANALYSIS\n",
      "======================================================================\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "1. FINAL LOSS COMPARISON\n",
      "----------------------------------------------------------------------\n",
      "Classical:  0.1201 +/- 0.0032\n",
      "Integrated: 0.1024 +/- 0.0004\n",
      "Mann-Whitney U: p=0.007937\n",
      "Cohen's d: 7.7098\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "2. TEST MSE COMPARISON\n",
      "----------------------------------------------------------------------\n",
      "Classical:  0.010536 +/- 0.001560\n",
      "Integrated: 2.104185 +/- 0.484565\n",
      "Mann-Whitney U: p=0.007937\n",
      "Improvement: -19870.7%\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "3. LONG-HORIZON PREDICTION (Error Growth Rate)\n",
      "----------------------------------------------------------------------\n",
      "Classical error growth:  1.8359 +/- 0.3928\n",
      "Integrated error growth: 3.3061 +/- 1.5044\n",
      "(Lower is better - errors compound slower)\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "4. TRAINING TIME\n",
      "----------------------------------------------------------------------\n",
      "Classical:  2.68s +/- 0.16s\n",
      "Integrated: 117.08s +/- 0.12s\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "5. COMPUTATIONAL COST\n",
      "----------------------------------------------------------------------\n",
      "Classical params:  3,915,017\n",
      "Integrated params: 22,217,045\n",
      "Ratio: 5.7x\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "6. BONFERRONI-CORRECTED SIGNIFICANCE\n",
      "----------------------------------------------------------------------\n",
      "Bonferroni-corrected alpha: 0.025\n",
      "Loss comparison: p=0.007937 - SIGNIFICANT\n",
      "MSE comparison:  p=0.007937 - SIGNIFICANT\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "7. 95% CONFIDENCE INTERVALS\n",
      "----------------------------------------------------------------------\n",
      "Classical Loss:  0.1201 [0.1173, 0.1229]\n",
      "Integrated Loss: 0.1024 [0.1020, 0.1027]\n",
      "Classical MSE:   0.010536 [0.009169, 0.011903]\n",
      "Integrated MSE:  2.104185 [1.679445, 2.528925]\n"
     ]
    }
   ],
   "source": [
    "print('='*70)\n",
    "print('STATISTICAL ANALYSIS')\n",
    "print('='*70)\n",
    "\n",
    "# Extract metrics\n",
    "classical_losses = [r['classical']['final_loss'] for r in all_results]\n",
    "integrated_losses = [r['integrated']['final_loss'] for r in all_results]\n",
    "classical_mses = [r['classical']['test_mse'] for r in all_results]\n",
    "integrated_mses = [r['integrated']['test_mse'] for r in all_results]\n",
    "classical_times = [r['classical']['training_time'] for r in all_results]\n",
    "integrated_times = [r['integrated']['training_time'] for r in all_results]\n",
    "\n",
    "# Long-horizon metrics\n",
    "classical_horizon_growth = [r['classical']['long_horizon'].get('error_growth_rate', 1.0) for r in all_results]\n",
    "integrated_horizon_growth = [r['integrated']['long_horizon'].get('error_growth_rate', 1.0) for r in all_results]\n",
    "\n",
    "print('\\n' + '-'*70)\n",
    "print('1. FINAL LOSS COMPARISON')\n",
    "print('-'*70)\n",
    "print(f'Classical:  {np.mean(classical_losses):.4f} +/- {np.std(classical_losses):.4f}')\n",
    "print(f'Integrated: {np.mean(integrated_losses):.4f} +/- {np.std(integrated_losses):.4f}')\n",
    "\n",
    "# Mann-Whitney U test\n",
    "stat, p_loss = stats.mannwhitneyu(classical_losses, integrated_losses, alternative='two-sided')\n",
    "print(f'Mann-Whitney U: p={p_loss:.6f}')\n",
    "\n",
    "# Cohen's d\n",
    "pooled_std = np.sqrt((np.std(classical_losses)**2 + np.std(integrated_losses)**2) / 2)\n",
    "cohens_d = (np.mean(classical_losses) - np.mean(integrated_losses)) / pooled_std\n",
    "print(f\"Cohen's d: {cohens_d:.4f}\")\n",
    "\n",
    "print('\\n' + '-'*70)\n",
    "print('2. TEST MSE COMPARISON')\n",
    "print('-'*70)\n",
    "print(f'Classical:  {np.mean(classical_mses):.6f} +/- {np.std(classical_mses):.6f}')\n",
    "print(f'Integrated: {np.mean(integrated_mses):.6f} +/- {np.std(integrated_mses):.6f}')\n",
    "\n",
    "stat, p_mse = stats.mannwhitneyu(classical_mses, integrated_mses, alternative='two-sided')\n",
    "print(f'Mann-Whitney U: p={p_mse:.6f}')\n",
    "\n",
    "improvement = (np.mean(classical_mses) - np.mean(integrated_mses)) / np.mean(classical_mses) * 100\n",
    "print(f'Improvement: {improvement:.1f}%')\n",
    "\n",
    "print('\\n' + '-'*70)\n",
    "print('3. LONG-HORIZON PREDICTION (Error Growth Rate)')\n",
    "print('-'*70)\n",
    "print(f'Classical error growth:  {np.mean(classical_horizon_growth):.4f} +/- {np.std(classical_horizon_growth):.4f}')\n",
    "print(f'Integrated error growth: {np.mean(integrated_horizon_growth):.4f} +/- {np.std(integrated_horizon_growth):.4f}')\n",
    "print('(Lower is better - errors compound slower)')\n",
    "\n",
    "print('\\n' + '-'*70)\n",
    "print('4. TRAINING TIME')\n",
    "print('-'*70)\n",
    "print(f'Classical:  {np.mean(classical_times):.2f}s +/- {np.std(classical_times):.2f}s')\n",
    "print(f'Integrated: {np.mean(integrated_times):.2f}s +/- {np.std(integrated_times):.2f}s')\n",
    "\n",
    "print('\\n' + '-'*70)\n",
    "print('5. COMPUTATIONAL COST')\n",
    "print('-'*70)\n",
    "print(f'Classical params:  {all_results[0][\"classical\"][\"params\"]:,}')\n",
    "print(f'Integrated params: {all_results[0][\"integrated\"][\"params\"]:,}')\n",
    "print(f'Ratio: {all_results[0][\"integrated\"][\"params\"] / all_results[0][\"classical\"][\"params\"]:.1f}x')\n",
    "\n",
    "print()\n",
    "print('-'*70)\n",
    "print('6. BONFERRONI-CORRECTED SIGNIFICANCE')\n",
    "print('-'*70)\n",
    "bonferroni_alpha = 0.025  # 0.05 / 2 for two primary comparisons\n",
    "print(f'Bonferroni-corrected alpha: {bonferroni_alpha}')\n",
    "print(f'Loss comparison: p={p_loss:.6f} - {\"SIGNIFICANT\" if p_loss < bonferroni_alpha else \"not significant\"}')\n",
    "print(f'MSE comparison:  p={p_mse:.6f} - {\"SIGNIFICANT\" if p_mse < bonferroni_alpha else \"not significant\"}')\n",
    "\n",
    "print()\n",
    "print('-'*70)\n",
    "print('7. 95% CONFIDENCE INTERVALS')\n",
    "print('-'*70)\n",
    "n = len(classical_losses)\n",
    "# Classical Loss CI\n",
    "cl_mean, cl_std = np.mean(classical_losses), np.std(classical_losses)\n",
    "cl_ci = (cl_mean - 1.96 * cl_std / np.sqrt(n), cl_mean + 1.96 * cl_std / np.sqrt(n))\n",
    "print(f'Classical Loss:  {cl_mean:.4f} [{cl_ci[0]:.4f}, {cl_ci[1]:.4f}]')\n",
    "\n",
    "# Integrated Loss CI\n",
    "il_mean, il_std = np.mean(integrated_losses), np.std(integrated_losses)\n",
    "il_ci = (il_mean - 1.96 * il_std / np.sqrt(n), il_mean + 1.96 * il_std / np.sqrt(n))\n",
    "print(f'Integrated Loss: {il_mean:.4f} [{il_ci[0]:.4f}, {il_ci[1]:.4f}]')\n",
    "\n",
    "# Classical MSE CI\n",
    "cm_mean, cm_std = np.mean(classical_mses), np.std(classical_mses)\n",
    "cm_ci = (cm_mean - 1.96 * cm_std / np.sqrt(n), cm_mean + 1.96 * cm_std / np.sqrt(n))\n",
    "print(f'Classical MSE:   {cm_mean:.6f} [{cm_ci[0]:.6f}, {cm_ci[1]:.6f}]')\n",
    "\n",
    "# Integrated MSE CI\n",
    "im_mean, im_std = np.mean(integrated_mses), np.std(integrated_mses)\n",
    "im_ci = (im_mean - 1.96 * im_std / np.sqrt(n), im_mean + 1.96 * im_std / np.sqrt(n))\n",
    "print(f'Integrated MSE:  {im_mean:.6f} [{im_ci[0]:.6f}, {im_ci[1]:.6f}]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6B.13 Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to ..\\experiments\\results\\fully_integrated\\complete_metrics.json\n",
      "Training history saved to ..\\experiments\\results\\fully_integrated\\cartpole_training_history.csv\n",
      "Multi-seed summary saved to ..\\experiments\\results\\fully_integrated\\multi_seed_summary.csv\n"
     ]
    }
   ],
   "source": [
    "# Save results\n",
    "import pandas as pd\n",
    "\n",
    "results_dir = Path('../experiments/results/fully_integrated')\n",
    "results_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Helper function to convert numpy types to Python native types for JSON serialization\n",
    "def to_python_type(obj):\n",
    "    \"\"\"Convert numpy types to Python native types.\"\"\"\n",
    "    if isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    elif isinstance(obj, (np.float32, np.float64)):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, (np.int32, np.int64)):\n",
    "        return int(obj)\n",
    "    elif isinstance(obj, (np.bool_,)):\n",
    "        return bool(obj)\n",
    "    elif isinstance(obj, list):\n",
    "        return [to_python_type(x) for x in obj]\n",
    "    elif isinstance(obj, dict):\n",
    "        return {k: to_python_type(v) for k, v in obj.items()}\n",
    "    return obj\n",
    "\n",
    "# Prepare results dict\n",
    "save_results = {\n",
    "    'metadata': {\n",
    "        'phase': '6b',\n",
    "        'experiment': 'Fully Integrated Quantum-Enhanced World Model',\n",
    "        'environment': 'CartPole-v1',\n",
    "        'num_seeds': len(CONFIG['seeds']),\n",
    "        'seeds': CONFIG['seeds'],\n",
    "        'num_epochs': CONFIG['num_epochs'],\n",
    "        'batch_size': CONFIG['batch_size'],\n",
    "        'seq_len': CONFIG['seq_len'],\n",
    "        'learning_rate': CONFIG['learning_rate'],\n",
    "        'stoch_dim': CONFIG['stoch_dim'],\n",
    "        'deter_dim': CONFIG['deter_dim'],\n",
    "        'hidden_dim': CONFIG['hidden_dim'],\n",
    "        'num_ensemble': CONFIG['num_ensemble'],\n",
    "        'bonferroni_alpha': 0.025,\n",
    "    },\n",
    "    'final_performance': {\n",
    "        'classical_loss_mean': float(np.mean(classical_losses)),\n",
    "        'classical_loss_std': float(np.std(classical_losses)),\n",
    "        'integrated_loss_mean': float(np.mean(integrated_losses)),\n",
    "        'integrated_loss_std': float(np.std(integrated_losses)),\n",
    "    },\n",
    "    'prediction_accuracy': {\n",
    "        'classical_test_mse_mean': float(np.mean(classical_mses)),\n",
    "        'classical_test_mse_std': float(np.std(classical_mses)),\n",
    "        'integrated_test_mse_mean': float(np.mean(integrated_mses)),\n",
    "        'integrated_test_mse_std': float(np.std(integrated_mses)),\n",
    "    },\n",
    "    'training_speed': {\n",
    "        'classical_mean_time': float(np.mean(classical_times)),\n",
    "        'classical_std_time': float(np.std(classical_times)),\n",
    "        'integrated_mean_time': float(np.mean(integrated_times)),\n",
    "        'integrated_std_time': float(np.std(integrated_times)),\n",
    "    },\n",
    "    'computational_cost': {\n",
    "        'classical_params': int(all_results[0]['classical']['params']),\n",
    "        'integrated_params': int(all_results[0]['integrated']['params']),\n",
    "    },\n",
    "    'statistical_tests': {\n",
    "        'loss_comparison': {\n",
    "            'p_value': float(p_loss),\n",
    "            'cohens_d': float(cohens_d),\n",
    "            'significant_bonferroni': bool(p_loss < 0.025),\n",
    "            'winner': 'Integrated' if np.mean(integrated_losses) < np.mean(classical_losses) else 'Classical',\n",
    "        },\n",
    "        'mse_comparison': {\n",
    "            'p_value': float(p_mse),\n",
    "            'improvement_percent': float(improvement),\n",
    "            'significant_bonferroni': bool(p_mse < 0.025),\n",
    "            'winner': 'Integrated' if np.mean(integrated_mses) < np.mean(classical_mses) else 'Classical',\n",
    "        },\n",
    "    },\n",
    "    'confidence_intervals': {\n",
    "        'classical_loss_ci': [\n",
    "            float(np.mean(classical_losses) - 1.96*np.std(classical_losses)/np.sqrt(len(classical_losses))),\n",
    "            float(np.mean(classical_losses) + 1.96*np.std(classical_losses)/np.sqrt(len(classical_losses)))\n",
    "        ],\n",
    "        'integrated_loss_ci': [\n",
    "            float(np.mean(integrated_losses) - 1.96*np.std(integrated_losses)/np.sqrt(len(integrated_losses))),\n",
    "            float(np.mean(integrated_losses) + 1.96*np.std(integrated_losses)/np.sqrt(len(integrated_losses)))\n",
    "        ],\n",
    "        'classical_mse_ci': [\n",
    "            float(np.mean(classical_mses) - 1.96*np.std(classical_mses)/np.sqrt(len(classical_mses))),\n",
    "            float(np.mean(classical_mses) + 1.96*np.std(classical_mses)/np.sqrt(len(classical_mses)))\n",
    "        ],\n",
    "        'integrated_mse_ci': [\n",
    "            float(np.mean(integrated_mses) - 1.96*np.std(integrated_mses)/np.sqrt(len(integrated_mses))),\n",
    "            float(np.mean(integrated_mses) + 1.96*np.std(integrated_mses)/np.sqrt(len(integrated_mses)))\n",
    "        ],\n",
    "    },\n",
    "    'long_horizon': {\n",
    "        'classical_error_growth': [float(x) for x in classical_horizon_growth],\n",
    "        'integrated_error_growth': [float(x) for x in integrated_horizon_growth],\n",
    "    },\n",
    "    'raw_results': {\n",
    "        'classical_final_losses': [float(x) for x in classical_losses],\n",
    "        'integrated_final_losses': [float(x) for x in integrated_losses],\n",
    "        'classical_test_mses': [float(x) for x in classical_mses],\n",
    "        'integrated_test_mses': [float(x) for x in integrated_mses],\n",
    "        'classical_training_times': [float(x) for x in classical_times],\n",
    "        'integrated_training_times': [float(x) for x in integrated_times],\n",
    "    },\n",
    "}\n",
    "\n",
    "# Convert all numpy types to Python native types\n",
    "save_results = to_python_type(save_results)\n",
    "\n",
    "# Save JSON\n",
    "with open(results_dir / 'complete_metrics.json', 'w') as f:\n",
    "    json.dump(save_results, f, indent=2)\n",
    "print(f'Results saved to {results_dir / \"complete_metrics.json\"}')\n",
    "\n",
    "# Save training history CSV\n",
    "history_data = []\n",
    "for seed_idx, seed in enumerate(CONFIG['seeds']):\n",
    "    if seed_idx < len(all_results):\n",
    "        classical_history = all_results[seed_idx]['classical'].get('history', [])\n",
    "        integrated_history = all_results[seed_idx]['integrated'].get('history', [])\n",
    "        max_len = max(len(classical_history), len(integrated_history)) if classical_history or integrated_history else 0\n",
    "        for epoch in range(max_len):\n",
    "            history_data.append({\n",
    "                'seed': seed,\n",
    "                'epoch': epoch,\n",
    "                'classical_loss': float(classical_history[epoch]) if epoch < len(classical_history) else None,\n",
    "                'integrated_loss': float(integrated_history[epoch]) if epoch < len(integrated_history) else None\n",
    "            })\n",
    "\n",
    "if history_data:\n",
    "    history_df = pd.DataFrame(history_data)\n",
    "    history_df.to_csv(results_dir / 'cartpole_training_history.csv', index=False)\n",
    "    print(f'Training history saved to {results_dir / \"cartpole_training_history.csv\"}')\n",
    "\n",
    "# Save multi-seed summary CSV\n",
    "summary_df = pd.DataFrame({\n",
    "    'seed': CONFIG['seeds'],\n",
    "    'classical_final_loss': [float(x) for x in classical_losses],\n",
    "    'integrated_final_loss': [float(x) for x in integrated_losses],\n",
    "    'classical_test_mse': [float(x) for x in classical_mses],\n",
    "    'integrated_test_mse': [float(x) for x in integrated_mses],\n",
    "    'classical_training_time': [float(x) for x in classical_times],\n",
    "    'integrated_training_time': [float(x) for x in integrated_times]\n",
    "})\n",
    "summary_df.to_csv(results_dir / 'multi_seed_summary.csv', index=False)\n",
    "print(f'Multi-seed summary saved to {results_dir / \"multi_seed_summary.csv\"}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6B.14 Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x1000 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Figure saved to ..\\experiments\\results\\fully_integrated\\comparison_plots.png\n"
     ]
    }
   ],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 1. Final Loss Comparison\n",
    "ax = axes[0, 0]\n",
    "x_seeds = np.arange(len(CONFIG['seeds']))\n",
    "width = 0.35\n",
    "ax.bar(x_seeds - width/2, classical_losses, width, label='Classical', color='steelblue', alpha=0.8)\n",
    "ax.bar(x_seeds + width/2, integrated_losses, width, label='Integrated', color='darkorange', alpha=0.8)\n",
    "ax.set_xlabel('Seed')\n",
    "ax.set_ylabel('Final Loss')\n",
    "ax.set_title('Final Loss by Seed')\n",
    "ax.set_xticks(x_seeds)\n",
    "ax.set_xticklabels(CONFIG['seeds'])\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Test MSE Comparison\n",
    "ax = axes[0, 1]\n",
    "ax.bar(x_seeds - width/2, classical_mses, width, label='Classical', color='steelblue', alpha=0.8)\n",
    "ax.bar(x_seeds + width/2, integrated_mses, width, label='Integrated', color='darkorange', alpha=0.8)\n",
    "ax.set_xlabel('Seed')\n",
    "ax.set_ylabel('Test MSE')\n",
    "ax.set_title('Test MSE by Seed')\n",
    "ax.set_xticks(x_seeds)\n",
    "ax.set_xticklabels(CONFIG['seeds'])\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Summary Statistics\n",
    "ax = axes[1, 0]\n",
    "metrics = ['Loss', 'MSE', 'Time (s)']\n",
    "classical_means = [np.mean(classical_losses), np.mean(classical_mses)*100, np.mean(classical_times)]\n",
    "integrated_means = [np.mean(integrated_losses), np.mean(integrated_mses)*100, np.mean(integrated_times)]\n",
    "x_metrics = np.arange(len(metrics))\n",
    "ax.bar(x_metrics - width/2, classical_means, width, label='Classical', color='steelblue', alpha=0.8)\n",
    "ax.bar(x_metrics + width/2, integrated_means, width, label='Integrated', color='darkorange', alpha=0.8)\n",
    "ax.set_xticks(x_metrics)\n",
    "ax.set_xticklabels(metrics)\n",
    "ax.set_title('Summary Comparison')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Long-Horizon Error Growth\n",
    "ax = axes[1, 1]\n",
    "n_horizon = len(classical_horizon_growth)\n",
    "x_horizon = np.arange(n_horizon)\n",
    "ax.bar(x_horizon - width/2, classical_horizon_growth, width, \n",
    "       label='Classical', color='steelblue', alpha=0.8)\n",
    "ax.bar(x_horizon + width/2, integrated_horizon_growth, width, \n",
    "       label='Integrated', color='darkorange', alpha=0.8)\n",
    "ax.set_xlabel('Seed Index')\n",
    "ax.set_ylabel('Error Growth Rate')\n",
    "ax.set_title('Long-Horizon Error Growth (Lower is Better)')\n",
    "ax.set_xticks(x_horizon)\n",
    "ax.set_xticklabels([f'Seed {i+1}' for i in range(n_horizon)])\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(results_dir / 'comparison_plots.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f'Figure saved to {results_dir / \"comparison_plots.png\"}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6B.15 Summary\n",
    "\n",
    "### What This Notebook Combines\n",
    "\n",
    "| Component | From | Solves |\n",
    "|-----------|------|--------|\n",
    "| QAOA Optimizer | Notebook 03 | Local minima |\n",
    "| Superposition Replay | Notebook 04 | Sample inefficiency |\n",
    "| Gate-Enhanced Layers | Notebook 05 | Slow convergence |\n",
    "| Error Correction Ensemble | Notebook 06 | Compounding errors |\n",
    "\n",
    "### Key Metrics Measured\n",
    "\n",
    "1. **Training Efficiency**: Wall-clock time\n",
    "2. **Sample Efficiency**: Final loss with same data\n",
    "3. **Final Performance**: Test MSE\n",
    "4. **Long-Horizon Prediction**: Error growth rate\n",
    "5. **Computational Cost**: Parameter count\n",
    "6. **Statistical Significance**: Mann-Whitney U, Cohen's d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Phase 6B: Fully Integrated - COMPLETE\n",
      "======================================================================\n",
      "\n",
      "Components integrated:\n",
      "  - QAOA Optimizer (escapes local minima)\n",
      "  - Superposition Replay (sample efficiency)\n",
      "  - Gate-Enhanced Layers (better representations)\n",
      "  - Error Correction Ensemble (robust predictions)\n",
      "\n",
      "Metrics measured:\n",
      "  - Training efficiency (time)\n",
      "  - Sample efficiency (loss)\n",
      "  - Final performance (MSE)\n",
      "  - Long-horizon prediction (error growth)\n",
      "  - Computational cost (params)\n",
      "  - Statistical significance (p-values)\n",
      "\n",
      "Ready for comparison with individual approaches!\n"
     ]
    }
   ],
   "source": [
    "print('='*70)\n",
    "print('Phase 6B: Fully Integrated - COMPLETE')\n",
    "print('='*70)\n",
    "print()\n",
    "print('Components integrated:')\n",
    "print('  - QAOA Optimizer (escapes local minima)')\n",
    "print('  - Superposition Replay (sample efficiency)')\n",
    "print('  - Gate-Enhanced Layers (better representations)')\n",
    "print('  - Error Correction Ensemble (robust predictions)')\n",
    "print()\n",
    "print('Metrics measured:')\n",
    "print('  - Training efficiency (time)')\n",
    "print('  - Sample efficiency (loss)')\n",
    "print('  - Final performance (MSE)')\n",
    "print('  - Long-horizon prediction (error growth)')\n",
    "print('  - Computational cost (params)')\n",
    "print('  - Statistical significance (p-values)')\n",
    "print()\n",
    "print('Ready for comparison with individual approaches!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
