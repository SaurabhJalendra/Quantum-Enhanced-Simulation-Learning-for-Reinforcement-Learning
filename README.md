# Quantum-Enhanced Simulation Learning for Reinforcement Learning

<div align="center">

**A Comparative Analysis of World Model Training Approaches**

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-ee4c2c.svg)](https://pytorch.org/)
[![License](https://img.shields.io/badge/License-Proprietary-red.svg)](LICENSE)
[![BITS Pilani](https://img.shields.io/badge/Institution-BITS%20Pilani-orange.svg)](https://www.bits-pilani.ac.in/)

</div>

---

## üìö Table of Contents

- [Overview](#overview)
- [Research Questions](#research-questions)
- [Key Features](#key-features)
- [Architecture](#architecture)
- [Installation](#installation)
- [Quick Start](#quick-start)
- [Project Structure](#project-structure)
- [Approaches Implemented](#approaches-implemented)
- [Environments](#environments)
- [Evaluation Metrics](#evaluation-metrics)
- [Experiments](#experiments)
- [Results](#results)
- [Documentation](#documentation)
- [Contributing](#contributing)
- [Citation](#citation)
- [Contact](#contact)
- [Acknowledgments](#acknowledgments)
- [License](#license)

---

## üéØ Overview

This repository contains the complete implementation and analysis for the M.Tech dissertation titled **"Quantum-Enhanced Simulation Learning for Reinforcement Learning: A Comparative Analysis of World Model Training Approaches"**.

### What is This Research About?

**World models** are neural networks that learn to predict how environments behave. They allow reinforcement learning (RL) agents to learn through "imagination" rather than expensive real-world trial-and-error. However, training these models is computationally expensive and time-consuming.

This dissertation investigates whether **quantum-inspired algorithms**‚Äîclassical algorithms inspired by quantum computing principles‚Äîcan make world model training more efficient, faster, and more robust.

### The Big Picture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                     REINFORCEMENT LEARNING                   ‚îÇ
‚îÇ                                                              ‚îÇ
‚îÇ  Agent ‚Üê‚Üí Environment                                        ‚îÇ
‚îÇ    ‚Üë                                                         ‚îÇ
‚îÇ    ‚îÇ learns from                                             ‚îÇ
‚îÇ    ‚Üì                                                         ‚îÇ
‚îÇ  World Model (Neural Network)                                ‚îÇ
‚îÇ    - Predicts next states                                    ‚îÇ
‚îÇ    - Predicts rewards                                        ‚îÇ
‚îÇ    - Enables "imagination"                                   ‚îÇ
‚îÇ                                                              ‚îÇ
‚îÇ  PROBLEM: Training is slow and expensive                     ‚îÇ
‚îÇ                                                              ‚îÇ
‚îÇ  OUR SOLUTION: Quantum-inspired training methods             ‚îÇ
‚îÇ    ‚úì QAOA-enhanced optimization                              ‚îÇ
‚îÇ    ‚úì Superposition-based replay                              ‚îÇ
‚îÇ    ‚úì Quantum gate transformations                            ‚îÇ
‚îÇ    ‚úì Error correction ensembles                              ‚îÇ
‚îÇ                                                              ‚îÇ
‚îÇ  GOAL: Faster, more efficient world model training           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Key Innovation

**No quantum computers needed!** All implementations run on standard CPUs/GPUs using quantum-*inspired* algorithms‚Äîwe borrow ideas from quantum computing and adapt them for classical hardware.

---

## ‚ùì Research Questions

This dissertation seeks to answer:

1. **Do quantum-inspired methods improve world model training efficiency?**
2. **Which quantum principles (QAOA, superposition, gates, error correction) work best?**
3. **Under what conditions do these methods provide advantages?**
4. **What are the computational trade-offs?**
5. **Can quantum-inspired methods improve robustness in stochastic environments?**
6. **How do learned representations differ from classical methods?**

---

## ‚ú® Key Features

### üî¨ Research Features
- ‚úÖ **Systematic Comparison**: 6 different training approaches
- ‚úÖ **Multiple Environments**: DMControl Suite, Atari, Simple Control
- ‚úÖ **Statistical Rigor**: Multiple seeds, significance testing, effect sizes
- ‚úÖ **Reproducible**: Complete code, configs, and random seeds provided
- ‚úÖ **Practical Focus**: Classical hardware only, no quantum computers needed

### üíª Implementation Features
- ‚úÖ **Modular Architecture**: Easy to extend and modify
- ‚úÖ **Well-Documented**: Comprehensive docstrings and comments
- ‚úÖ **Configurable**: YAML-based configuration system
- ‚úÖ **Logging**: TensorBoard integration for training visualization
- ‚úÖ **Testing**: Unit tests for all major components

### üìä Analysis Features
- ‚úÖ **Comprehensive Metrics**: Sample efficiency, training speed, accuracy, robustness
- ‚úÖ **Statistical Analysis**: Hypothesis testing, confidence intervals
- ‚úÖ **Visualizations**: Learning curves, latent space plots, comparison charts
- ‚úÖ **Ablation Studies**: Component-wise analysis

---

## üèóÔ∏è Architecture

### High-Level System Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                        TRAINING SYSTEM                          ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                 ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ   RL ENV     ‚îÇ      ‚îÇ  WORLD MODEL ‚îÇ      ‚îÇ   TRAINER    ‚îÇ ‚îÇ
‚îÇ  ‚îÇ              ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí‚îÇ              ‚îÇ‚Üê‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ              ‚îÇ ‚îÇ
‚îÇ  ‚îÇ - DMControl  ‚îÇ      ‚îÇ - Encoder    ‚îÇ      ‚îÇ - Classical  ‚îÇ ‚îÇ
‚îÇ  ‚îÇ - Atari      ‚îÇ      ‚îÇ - Dynamics   ‚îÇ      ‚îÇ - QAOA       ‚îÇ ‚îÇ
‚îÇ  ‚îÇ - CartPole   ‚îÇ      ‚îÇ - Reward     ‚îÇ      ‚îÇ - Quantum-   ‚îÇ ‚îÇ
‚îÇ  ‚îÇ - Pendulum   ‚îÇ      ‚îÇ - Decoder    ‚îÇ      ‚îÇ   Inspired   ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ         ‚Üì                      ‚Üì                      ‚Üì         ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ                    EXPERIENCE BUFFER                      ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  - Standard Replay                                        ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  - Superposition-Enhanced Replay                          ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ                               ‚Üì                                 ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ                    METRICS & LOGGING                      ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  - TensorBoard  - Weights & Biases  - CSV Logs           ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### World Model Architecture (DreamerV3-Style)

```
Input Observation (o_t)
        ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ     ENCODER       ‚îÇ  ‚Üí Converts observations to latent representations
‚îÇ  (CNN/MLP)        ‚îÇ     z_t = encoder(o_t)
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
        ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  RECURRENT MODEL  ‚îÇ  ‚Üí Maintains hidden state over time
‚îÇ  (GRU/LSTM)       ‚îÇ     h_t = f(h_{t-1}, z_t, a_{t-1})
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
        ‚Üì
        ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚Üì                 ‚Üì                 ‚Üì                 ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  DYNAMICS    ‚îÇ  ‚îÇ   REWARD     ‚îÇ  ‚îÇ   CONTINUE   ‚îÇ  ‚îÇ   DECODER    ‚îÇ
‚îÇ  PREDICTOR   ‚îÇ  ‚îÇ  PREDICTOR   ‚îÇ  ‚îÇ  PREDICTOR   ‚îÇ  ‚îÇ              ‚îÇ
‚îÇ              ‚îÇ  ‚îÇ              ‚îÇ  ‚îÇ              ‚îÇ  ‚îÇ              ‚îÇ
‚îÇ ·∫ë_{t+1}      ‚îÇ  ‚îÇ rÃÇ_t          ‚îÇ  ‚îÇ ƒâ_t          ‚îÇ  ‚îÇ √¥_t          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Quantum-Inspired Enhancements

Each approach modifies different parts of the training process:

| Approach | Modification Point | Key Component |
|----------|-------------------|---------------|
| **QAOA-Enhanced** | Optimization loop | Alternating operators |
| **Superposition** | Experience replay | Weighted combinations |
| **Gate-Enhanced** | Neural network layers | Hadamard/CNOT-inspired ops |
| **Error Correction** | Ensemble predictions | Syndrome detection |

---

## üì¶ Installation

### Prerequisites

- Python 3.8 or higher
- pip package manager
- (Optional) CUDA-capable GPU for faster training

### Step 1: Clone the Repository

```bash
git clone https://github.com/yourusername/dissertation-quantum-world-models.git
cd dissertation-quantum-world-models
```

### Step 2: Create Virtual Environment

```bash
# Using venv
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# OR using conda
conda create -n quantum-wm python=3.8
conda activate quantum-wm
```

### Step 3: Install Dependencies

```bash
# Install all required packages
pip install -r requirements.txt

# Install in development mode (recommended for contributors)
pip install -e .
```

### Step 4: Install RL Environments

```bash
# DMControl Suite
pip install dm_control

# Atari environments
pip install "gymnasium[atari,accept-rom-license]"

# Verify installation
python -c "import gymnasium; import dm_control; print('‚úì Environments installed successfully')"
```

### Step 5: (Optional) Install Additional Tools

```bash
# For experiment tracking
pip install wandb
wandb login

# For advanced visualization
pip install plotly
```

### Verify Installation

```bash
# Run tests
pytest tests/

# Run quick demo
python examples/demo_baseline.py
```

---

## üöÄ Quick Start

### 1. Train Classical Baseline

```bash
# Train on CartPole (simple environment for testing)
python experiments/scripts/train_baseline.py \
    --env CartPole-v1 \
    --steps 100000 \
    --seed 42

# Train on DMControl Walker
python experiments/scripts/train_baseline.py \
    --env dm_control:Walker-walk \
    --steps 1000000 \
    --seed 42
```

### 2. Train Quantum-Inspired Approaches

```bash
# QAOA-Enhanced
python experiments/scripts/train_qaoa.py \
    --env dm_control:Walker-walk \
    --p_layers 3 \
    --steps 1000000

# Superposition-Enhanced
python experiments/scripts/train_superposition.py \
    --env dm_control:Walker-walk \
    --parallel_samples 8 \
    --steps 1000000

# Gate-Enhanced
python experiments/scripts/train_gates.py \
    --env dm_control:Walker-walk \
    --gate_layers 4 \
    --steps 1000000

# Error Correction Ensemble
python experiments/scripts/train_error_correction.py \
    --env dm_control:Walker-walk \
    --num_models 3 \
    --steps 1000000
```

### 3. Run All Experiments

```bash
# Run comprehensive comparison with multiple seeds
python experiments/scripts/run_all_experiments.py \
    --config experiments/configs/full_comparison.yaml \
    --num_seeds 5
```

### 4. Evaluate Trained Models

```bash
# Evaluate single model
python src/evaluation/evaluate.py \
    --checkpoint results/checkpoints/baseline_walker_seed42.pt \
    --env dm_control:Walker-walk \
    --episodes 100

# Compare all approaches
python src/evaluation/compare_all.py \
    --results_dir results/ \
    --output results/comparison_report.html
```

### 5. Visualize Results

```bash
# Launch TensorBoard
tensorboard --logdir experiments/logs/

# Generate comparison plots
python analysis/scripts/generate_figures.py \
    --results_dir results/ \
    --output dissertation/figures/

# Interactive analysis notebook
jupyter notebook analysis/notebooks/results_analysis.ipynb
```

---

## üìÇ Project Structure

```
dissertation-quantum-world-models/
‚îÇ
‚îú‚îÄ‚îÄ README.md                          # This file
‚îú‚îÄ‚îÄ CLAUDE.md                          # AI assistant context & memory
‚îú‚îÄ‚îÄ requirements.txt                   # Python dependencies
‚îú‚îÄ‚îÄ setup.py                           # Package installation
‚îú‚îÄ‚îÄ .gitignore                         # Git ignore rules
‚îú‚îÄ‚îÄ LICENSE                            # License information
‚îÇ
‚îú‚îÄ‚îÄ docs/                              # üìö Documentation
‚îÇ   ‚îú‚îÄ‚îÄ abstract-report.pdf            # Approved dissertation abstract
‚îÇ   ‚îú‚îÄ‚îÄ literature-review.md           # Literature survey
‚îÇ   ‚îú‚îÄ‚îÄ methodology.md                 # Detailed methodology
‚îÇ   ‚îú‚îÄ‚îÄ theoretical-background.md      # Quantum computing background
‚îÇ   ‚îú‚îÄ‚îÄ implementation-guide.md        # Implementation details
‚îÇ   ‚îú‚îÄ‚îÄ experimental-protocol.md       # Experiment procedures
‚îÇ   ‚îú‚îÄ‚îÄ results-summary.md             # Results overview
‚îÇ   ‚îú‚îÄ‚îÄ progress-log.md                # Weekly progress updates
‚îÇ   ‚îî‚îÄ‚îÄ api-reference.md               # Code API documentation
‚îÇ
‚îú‚îÄ‚îÄ src/                               # üíª Source Code
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ models/                        # World model implementations
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ base_world_model.py        # Abstract base class
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ baseline.py                # Classical DreamerV3-style
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ qaoa_enhanced.py           # QAOA-inspired optimization
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ superposition.py           # Superposition-based replay
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ gate_enhanced.py           # Quantum gate transformations
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ error_correction.py        # Error correction ensemble
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ integrated.py              # Fully integrated approach
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ components/                # Shared components
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ encoder.py             # Observation encoder
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ dynamics.py            # Dynamics predictor
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ reward.py              # Reward predictor
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ decoder.py             # Reconstruction decoder
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ rssm.py                # Recurrent state-space model
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ training/                      # Training procedures
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ base_trainer.py            # Abstract trainer class
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ classical_trainer.py       # Standard training loop
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ qaoa_trainer.py            # QAOA training loop
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ quantum_inspired_trainer.py # Quantum-inspired training
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ loss_functions.py          # Loss computations
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ optimizers.py              # Custom optimizers
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ schedulers.py              # Learning rate schedules
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ replay/                        # Experience replay buffers
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ standard_buffer.py         # Standard replay buffer
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ superposition_buffer.py    # Quantum-inspired replay
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ prioritized_buffer.py      # Prioritized experience replay
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ utils.py                   # Buffer utilities
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ environments/                  # Environment wrappers
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ dm_control_wrapper.py      # DMControl environments
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ atari_wrapper.py           # Atari environments
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ gymnasium_wrapper.py       # Gymnasium environments
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ preprocessing.py           # Observation preprocessing
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ evaluation/                    # Evaluation code
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ metrics.py                 # Performance metrics
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ evaluator.py               # Model evaluation
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ statistical_tests.py       # Statistical analysis
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ visualizations.py          # Plotting functions
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ comparison.py              # Multi-model comparison
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ utils/                         # Utilities
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îú‚îÄ‚îÄ config.py                  # Configuration management
‚îÇ       ‚îú‚îÄ‚îÄ logging_utils.py           # Logging helpers
‚îÇ       ‚îú‚îÄ‚îÄ reproducibility.py         # Seed setting, determinism
‚îÇ       ‚îú‚îÄ‚îÄ checkpointing.py           # Model save/load
‚îÇ       ‚îî‚îÄ‚îÄ data_processing.py         # Data utilities
‚îÇ
‚îú‚îÄ‚îÄ experiments/                       # üß™ Experiments
‚îÇ   ‚îú‚îÄ‚îÄ configs/                       # Configuration files
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ baseline/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ cartpole.yaml
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ walker.yaml
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ atari_pong.yaml
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ qaoa/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ superposition/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ gates/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ error_correction/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ full_comparison.yaml       # All methods comparison
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ scripts/                       # Experiment scripts
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ train_baseline.py          # Train classical baseline
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ train_qaoa.py              # Train QAOA-enhanced
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ train_superposition.py     # Train superposition
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ train_gates.py             # Train gate-enhanced
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ train_error_correction.py  # Train error correction
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ train_integrated.py        # Train integrated
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ run_all_experiments.py     # Run comprehensive suite
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ablation_study.py          # Ablation experiments
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ hyperparameter_sweep.py    # Hyperparameter tuning
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ logs/                          # Training logs (gitignored)
‚îÇ       ‚îî‚îÄ‚îÄ .gitkeep
‚îÇ
‚îú‚îÄ‚îÄ analysis/                          # üìä Analysis
‚îÇ   ‚îú‚îÄ‚îÄ notebooks/                     # Jupyter notebooks
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 01_exploratory_analysis.ipynb
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 02_learning_curves.ipynb
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 03_statistical_tests.ipynb
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 04_latent_space_viz.ipynb
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 05_ablation_analysis.ipynb
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ 06_final_comparison.ipynb
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ scripts/                       # Analysis scripts
‚îÇ       ‚îú‚îÄ‚îÄ compute_statistics.py      # Statistical computations
‚îÇ       ‚îú‚îÄ‚îÄ generate_figures.py        # Create all figures
‚îÇ       ‚îú‚îÄ‚îÄ ablation_analysis.py       # Ablation study analysis
‚îÇ       ‚îú‚îÄ‚îÄ latent_analysis.py         # Latent space analysis
‚îÇ       ‚îî‚îÄ‚îÄ export_tables.py           # Generate result tables
‚îÇ
‚îú‚îÄ‚îÄ results/                           # üìà Results (gitignored)
‚îÇ   ‚îú‚îÄ‚îÄ checkpoints/                   # Saved model checkpoints
‚îÇ   ‚îú‚îÄ‚îÄ data/                          # Raw experimental data
‚îÇ   ‚îú‚îÄ‚îÄ figures/                       # Generated plots
‚îÇ   ‚îú‚îÄ‚îÄ tables/                        # Result tables
‚îÇ   ‚îî‚îÄ‚îÄ summary.md                     # Results summary
‚îÇ
‚îú‚îÄ‚îÄ dissertation/                      # üìù Dissertation Document
‚îÇ   ‚îú‚îÄ‚îÄ chapters/                      # Chapter source files
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 01_introduction.md
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 02_literature_review.md
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 03_background.md
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 04_methodology.md
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 05_implementation.md
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 06_experiments.md
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 07_results.md
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 08_discussion.md
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 09_conclusion.md
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ 10_appendices.md
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ figures/                       # Dissertation figures
‚îÇ   ‚îú‚îÄ‚îÄ tables/                        # Dissertation tables
‚îÇ   ‚îú‚îÄ‚îÄ references.bib                 # Bibliography
‚îÇ   ‚îú‚îÄ‚îÄ main.tex                       # LaTeX main file
‚îÇ   ‚îî‚îÄ‚îÄ compiled/                      # Compiled PDFs
‚îÇ       ‚îî‚îÄ‚îÄ dissertation.pdf           # Final dissertation
‚îÇ
‚îú‚îÄ‚îÄ tests/                             # ‚úÖ Unit Tests
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ test_models.py                 # Model tests
‚îÇ   ‚îú‚îÄ‚îÄ test_training.py               # Training tests
‚îÇ   ‚îú‚îÄ‚îÄ test_replay.py                 # Replay buffer tests
‚îÇ   ‚îú‚îÄ‚îÄ test_evaluation.py             # Evaluation tests
‚îÇ   ‚îú‚îÄ‚îÄ test_environments.py           # Environment tests
‚îÇ   ‚îî‚îÄ‚îÄ test_utils.py                  # Utility tests
‚îÇ
‚îú‚îÄ‚îÄ examples/                          # üìñ Examples
‚îÇ   ‚îú‚îÄ‚îÄ demo_baseline.py               # Simple baseline demo
‚îÇ   ‚îú‚îÄ‚îÄ demo_qaoa.py                   # QAOA approach demo
‚îÇ   ‚îú‚îÄ‚îÄ demo_visualization.py          # Visualization demo
‚îÇ   ‚îî‚îÄ‚îÄ notebooks/
‚îÇ       ‚îî‚îÄ‚îÄ getting_started.ipynb      # Getting started guide
‚îÇ
‚îî‚îÄ‚îÄ scripts/                           # üõ†Ô∏è Utility Scripts
    ‚îú‚îÄ‚îÄ setup_environment.sh           # Environment setup
    ‚îú‚îÄ‚îÄ download_datasets.sh           # Download pre-trained models
    ‚îú‚îÄ‚îÄ run_tests.sh                   # Run all tests
    ‚îî‚îÄ‚îÄ generate_documentation.sh      # Generate API docs
```

---

## üéØ Approaches Implemented

### 1. Classical Baseline

**File:** `src/models/baseline.py`

Standard DreamerV3-style world model training with:
- Recurrent state-space model (RSSM)
- KL-divergence regularization
- Reconstruction loss
- Reward prediction loss
- Continue prediction

**Training:**
```bash
python experiments/scripts/train_baseline.py --env dm_control:Walker-walk
```

**Key Hyperparameters:**
- Learning rate: 3e-4
- Batch size: 50
- Sequence length: 50
- KL weight: 1.0

---

### 2. QAOA-Enhanced Approach

**File:** `src/models/qaoa_enhanced.py`

Quantum Approximate Optimization Algorithm-inspired training:

**Algorithm:**
```python
for p in range(p_layers):
    # Cost layer (problem-specific)
    loss = compute_loss(model, batch)
    grads = compute_gradients(loss)
    
    # Mixing layer (exploration)
    noise = generate_exploration_noise(beta_params)
    perturbed_params = params + noise
    
    # Update
    params = optimizer.step(perturbed_params, grads)
```

**Key Components:**
- Alternating cost and mixing operators
- Parameter-dependent mixing angles
- Adaptive layer depth

**Training:**
```bash
python experiments/scripts/train_qaoa.py \
    --env dm_control:Walker-walk \
    --p_layers 3 \
    --mixing_strength 0.1
```

**Key Hyperparameters:**
- p_layers: 2-4
- mixing_strength: 0.05-0.2
- beta_init: 0.1

---

### 3. Superposition-Enhanced Approach

**File:** `src/models/superposition.py`

Quantum superposition-inspired experience replay:

**Algorithm:**
```python
# Sample multiple trajectories in parallel
trajectories = []
for _ in range(num_parallel):
    traj = replay_buffer.sample()
    weight = compute_quantum_weight(traj)
    trajectories.append((traj, weight))

# Create superposed batch (weighted combination)
batch = weighted_combination(trajectories)

# Train on superposed batch
loss = model.compute_loss(batch)
```

**Key Components:**
- Parallel trajectory sampling
- Quantum-inspired weighting scheme
- Interference-like aggregation

**Training:**
```bash
python experiments/scripts/train_superposition.py \
    --env dm_control:Walker-walk \
    --parallel_samples 8 \
    --weight_decay 0.99
```

**Key Hyperparameters:**
- parallel_samples: 4-16
- weight_decay: 0.95-0.99
- aggregation: 'weighted_mean'

---

### 4. Gate-Enhanced Approach

**File:** `src/models/gate_enhanced.py`

Quantum gate-inspired neural network transformations:

**Architecture:**
```python
class QuantumInspiredLayer(nn.Module):
    def forward(self, x):
        # Hadamard-inspired: uniform mixing
        x = self.hadamard_transform(x)
        
        # CNOT-inspired: feature correlation
        x = self.controlled_transform(x, control_features)
        
        # Phase gate-inspired: learned rotation
        x = x * torch.exp(1j * self.phase_angles)
        
        # Measurement-inspired: projection
        x = self.measurement_projection(x.real)
        
        return x
```

**Key Components:**
- Hadamard-like transformations
- CNOT-like conditional operations
- Phase rotation layers
- Measurement projection

**Training:**
```bash
python experiments/scripts/train_gates.py \
    --env dm_control:Walker-walk \
    --gate_layers 4 \
    --phase_init uniform
```

**Key Hyperparameters:**
- gate_layers: 2-6
- hidden_dim: 256-512
- phase_init: 'uniform', 'normal', 'zeros'

---

### 5. Error Correction Ensemble

**File:** `src/models/error_correction.py`

Quantum error correction-inspired ensemble:

**Algorithm:**
```python
# Multiple redundant predictors
predictors = [WorldModel() for _ in range(num_models)]

# Forward pass through all models
predictions = [m.predict(state, action) for m in predictors]

# Syndrome detection (identify disagreements)
syndrome = detect_disagreement(predictions)

# Error correction (voting/weighting)
if syndrome.any():
    corrected = majority_voting(predictions, syndrome)
else:
    corrected = mean(predictions)
```

**Key Components:**
- Ensemble of 3-5 world models
- Syndrome measurement
- Majority voting / weighted correction
- Redundant encoding

**Training:**
```bash
python experiments/scripts/train_error_correction.py \
    --env dm_control:Walker-walk \
    --num_models 3 \
    --syndrome_threshold 0.1
```

**Key Hyperparameters:**
- num_models: 3-5
- syndrome_threshold: 0.05-0.2
- correction_method: 'majority', 'weighted'

---

### 6. Fully Integrated Approach (Optional)

**File:** `src/models/integrated.py`

Combines multiple quantum-inspired techniques:
- QAOA optimization
- Superposition replay
- Gate-enhanced encoder
- Error correction ensemble

**Training:**
```bash
python experiments/scripts/train_integrated.py \
    --env dm_control:Walker-walk \
    --enable_qaoa \
    --enable_superposition \
    --enable_gates \
    --enable_error_correction
```

---

## üåç Environments

### Simple Control (Baseline Validation)

| Environment | Observation | Action | Episodes |
|-------------|-------------|--------|----------|
| CartPole-v1 | 4D | Discrete (2) | 500 |
| Pendulum-v1 | 3D | Continuous (1D) | 500 |

**Purpose:** Quick validation and debugging

### DMControl Suite (Primary Benchmark)

| Environment | Observation | Action | Episode Length |
|-------------|-------------|--------|----------------|
| Walker-walk | 24D | Continuous (6D) | 1000 |
| Cheetah-run | 17D | Continuous (6D) | 1000 |
| Reacher-easy | 6D | Continuous (2D) | 1000 |
| Reacher-hard | 6D | Continuous (2D) | 1000 |

**Purpose:** Complex continuous control, primary evaluation

### Atari (Visual Complexity)

| Environment | Observation | Action | Max Steps |
|-------------|-------------|--------|-----------|
| Pong | 84√ó84√ó4 | Discrete (6) | 108,000 |
| Breakout | 84√ó84√ó4 | Discrete (4) | 108,000 |

**Purpose:** Visual representation learning, high-dimensional observations

---

## üìè Evaluation Metrics

### Primary Metrics

#### 1. Sample Efficiency
**Definition:** Number of environment steps to reach target performance

**Computation:**
```python
def compute_sample_efficiency(rewards, target_reward=threshold):
    """Returns number of steps to reach target reward"""
    for step, reward in enumerate(rewards):
        if reward >= target_reward:
            return step
    return None  # Target not reached
```

**Target Thresholds:**
- CartPole: 475
- Walker: 800
- Cheetah: 800
- Pong: 18

#### 2. Training Speed
**Definition:** Wall-clock time to convergence

**Measured:** Time from start to reaching 95% of final performance

#### 3. Prediction Accuracy
**Definition:** Mean Squared Error on held-out test trajectories

**Computation:**
```python
mse = torch.mean((predicted_states - true_states) ** 2)
```

#### 4. Final Performance
**Definition:** Average return over 100 test episodes

### Secondary Metrics

#### 5. Training Stability
**Definition:** Standard deviation across random seeds

**Lower is better** - indicates consistent training

#### 6. Robustness
**Definition:** Performance degradation under noise

**Test:** Add Gaussian noise to observations, measure performance drop

#### 7. Computational Cost
**Metrics:**
- FLOPs per training step
- Memory usage (peak)
- Training time per epoch

### Statistical Analysis

**Methods:**
- Mann-Whitney U test (non-parametric comparison)
- Cohen's d (effect size)
- 95% confidence intervals
- Bonferroni correction (multiple comparisons)

---

## üß™ Experiments

### Experiment 1: Baseline Performance

**Goal:** Establish classical baseline performance

**Environments:** All (CartPole, Walker, Cheetah, Reacher, Pong, Breakout)

**Seeds:** 5 per environment

**Config:** `experiments/configs/baseline/`

**Run:**
```bash
python experiments/scripts/run_all_experiments.py \
    --experiment baseline \
    --num_seeds 5
```

---

### Experiment 2: QAOA Comparison

**Goal:** Compare QAOA-enhanced vs baseline

**Variables:**
- p_layers: [1, 2, 3, 4]
- mixing_strength: [0.05, 0.1, 0.15, 0.2]

**Environments:** Walker, Cheetah

**Seeds:** 5 per configuration

**Run:**
```bash
python experiments/scripts/ablation_study.py \
    --approach qaoa \
    --ablate p_layers,mixing_strength
```

---

### Experiment 3: Superposition Comparison

**Goal:** Evaluate superposition-enhanced replay

**Variables:**
- parallel_samples: [2, 4, 8, 16]
- weight_decay: [0.95, 0.97, 0.99]

**Environments:** Walker, Cheetah

**Seeds:** 5 per configuration

**Run:**
```bash
python experiments/scripts/ablation_study.py \
    --approach superposition \
    --ablate parallel_samples,weight_decay
```

---

### Experiment 4: Gate Transformations

**Goal:** Test quantum gate-inspired layers

**Variables:**
- gate_layers: [2, 3, 4, 5, 6]
- hidden_dim: [256, 512]

**Environments:** Walker, Pong

**Seeds:** 5 per configuration

---

### Experiment 5: Error Correction

**Goal:** Evaluate ensemble error correction

**Variables:**
- num_models: [3, 4, 5]
- syndrome_threshold: [0.05, 0.1, 0.15, 0.2]

**Environments:** Walker (with added noise)

**Seeds:** 5 per configuration

---

### Experiment 6: Full Comparison

**Goal:** Systematic comparison of all approaches

**Approaches:** All 6 methods

**Environments:** All environments

**Seeds:** 10 per approach per environment

**Config:** `experiments/configs/full_comparison.yaml`

**Run:**
```bash
python experiments/scripts/run_all_experiments.py \
    --experiment full_comparison \
    --num_seeds 10
```

**Estimated Time:** ~2 weeks on single GPU

---

### Experiment 7: Robustness Testing

**Goal:** Test robustness to environmental noise

**Procedure:**
1. Train all models on clean environments
2. Evaluate with Gaussian noise: œÉ ‚àà [0.0, 0.05, 0.1, 0.15, 0.2]
3. Measure performance degradation

**Hypothesis:** Error correction approach should be most robust

---

### Experiment 8: Generalization

**Goal:** Test generalization to unseen environments

**Procedure:**
1. Train on: Walker, Cheetah
2. Test on: Humanoid, Quadruped (zero-shot)
3. Measure transfer performance

---

## üìä Results

Results will be populated as experiments complete.

### Preliminary Results

*(To be updated)*

### Learning Curves

*(Figures to be added)*

### Statistical Comparisons

*(Tables to be added)*

### Key Findings

*(To be documented)*

---

## üìö Documentation

### Available Documentation

- **[Abstract Report](docs/abstract-report.pdf)** - Approved dissertation abstract
- **[Literature Review](docs/literature-review.md)** - Comprehensive survey of related work
- **[Methodology](docs/methodology.md)** - Detailed research methodology
- **[Theoretical Background](docs/theoretical-background.md)** - Quantum computing primer
- **[Implementation Guide](docs/implementation-guide.md)** - Code walkthrough
- **[Experimental Protocol](docs/experimental-protocol.md)** - How experiments are run
- **[API Reference](docs/api-reference.md)** - Code documentation
- **[Progress Log](docs/progress-log.md)** - Weekly updates
- **[CLAUDE.md](CLAUDE.md)** - AI assistant context

### Generating Documentation

```bash
# Generate API documentation
python scripts/generate_documentation.sh

# Build HTML docs
cd docs && make html
```

---

## ü§ù Contributing

This is a dissertation project, but feedback and suggestions are welcome!

### Reporting Issues

Found a bug? Please open an issue with:
- Description of the problem
- Steps to reproduce
- Expected vs actual behavior
- System information (OS, Python version, GPU)

### Code Style

This project follows:
- PEP 8 style guide
- Type hints for all functions
- Docstrings in NumPy format

```python
def example_function(param1: int, param2: str) -> bool:
    """
    Brief description.

    Parameters
    ----------
    param1 : int
        Description of param1
    param2 : str
        Description of param2

    Returns
    -------
    bool
        Description of return value
    """
    pass
```

---

## üìñ Citation

If you use this code or findings in your research, please cite:

```bibtex
@mastersthesis{jalendra2026quantum,
  title={Quantum-Enhanced Simulation Learning for Reinforcement Learning: A Comparative Analysis of World Model Training Approaches},
  author={Jalendra, Saurabh},
  year={2026},
  school={Birla Institute of Technology and Science, Pilani},
  type={M.Tech Dissertation},
  note={Supervised by Gaurav Kumar}
}
```

---

## üìß Contact

**Student:** Saurabh Jalendra  
**Email:** 2023ac05912@wilp.bits-pilani.ac.in  
**BITS ID:** 2023AC05912  
**Institution:** BITS Pilani (WILP Division)

**Supervisor:** Gaurav Kumar  
**Position:** Deputy Director, PMA Directorate, IN-SPACe  
**Email:** gaurav.kumar45@inspace.gov.in

**Additional Examiner:** Rishabh Swami  
**Organization:** Orange Business Services India  
**Email:** rishabh.swami@orange.com

---

## üôè Acknowledgments

This research would not have been possible without:

- **BITS Pilani** - For the M.Tech program and research support
- **IN-SPACe** - For supervision and guidance (Gaurav Kumar)
- **S K Jalendra Marketing Services Pvt Ltd** - For organizational support
- **PyTorch Team** - For the deep learning framework
- **DeepMind** - For the DreamerV3 architecture and DMControl Suite
- **OpenAI** - For Gymnasium environments
- **Open-source community** - For countless tools and libraries

### Key References

This work builds upon:
- Hafner et al. (2023) - DreamerV3
- Farhi et al. (2014) - QAOA
- Wei et al. (2022) - Quantum-inspired RL
- Sutton & Barto (2018) - RL foundations

---

## üìÑ License

**Copyright ¬© 2025 Saurabh Jalendra, BITS Pilani**

This dissertation and associated code are proprietary to:
- Birla Institute of Technology and Science, Pilani
- S K Jalendra Marketing Services Pvt Ltd

**All rights reserved.**

This work is submitted in partial fulfillment of the requirements for the degree of Master of Technology in Artificial Intelligence & Machine Learning.

**For permissions regarding use of this code or findings, please contact the author.**

---

## üìÖ Timeline

| Phase | Duration | Status |
|-------|----------|--------|
| Literature Review & Baseline | 30 Oct - 20 Nov 2025 | üü° In Progress |
| Quantum-Inspired Development | 21 Nov - 24 Dec 2025 | ‚ö™ Pending |
| Experimental Evaluation | 25 Dec - 14 Jan 2026 | ‚ö™ Pending |
| Analysis & Documentation | 15 Jan - 22 Jan 2026 | ‚ö™ Pending |
| Review & Revision | 23 Jan - 28 Jan 2026 | ‚ö™ Pending |
| Final Submission | 29 Jan - 31 Jan 2026 | ‚ö™ Pending |

**Last Updated:** November 2025

---

## üîó Links

- **Institution:** [BITS Pilani](https://www.bits-pilani.ac.in/)
- **Program:** [WILP Division](https://www.bits-pilani.ac.in/wilp/)
- **Supervisor Organization:** [IN-SPACe](https://www.inspace.gov.in/)
- **PyTorch:** [https://pytorch.org/](https://pytorch.org/)
- **DMControl:** [https://github.com/deepmind/dm_control](https://github.com/deepmind/dm_control)
- **Gymnasium:** [https://gymnasium.farama.org/](https://gymnasium.farama.org/)

---

<div align="center">

**üöÄ Built with quantum-inspired algorithms on classical hardware üöÄ**

**Made with ‚ù§Ô∏è by Saurabh Jalendra**

[Report Issue](https://github.com/yourusername/dissertation-quantum-world-models/issues) ‚Ä¢ [Request Feature](https://github.com/yourusername/dissertation-quantum-world-models/issues) ‚Ä¢ [Dissertation Progress](docs/progress-log.md)

</div>
