"""
Script to create Phase 2 DMControl notebooks with full implementation.
"""
import json
import os

base_path = r"D:\Git Repos\Quantum-Enhanced-Simulation-Learning-for-Reinforcement-Learning\phase2_dmcontrol_notebooks"

def create_notebook(cells):
    """Create notebook structure."""
    return {
        "cells": cells,
        "metadata": {
            "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"},
            "language_info": {"name": "python", "version": "3.8.0"}
        },
        "nbformat": 4,
        "nbformat_minor": 4
    }

def md_cell(source):
    """Create markdown cell."""
    return {"cell_type": "markdown", "metadata": {}, "source": source if isinstance(source, list) else [source]}

def code_cell(source):
    """Create code cell."""
    return {"cell_type": "code", "metadata": {}, "source": source if isinstance(source, list) else [source], "outputs": [], "execution_count": None}

# ============================================================================
# NOTEBOOK 1: DMControl Setup
# ============================================================================

nb1_cells = [
    md_cell([
        "# Phase 2: DMControl Suite Setup\n\n",
        "**Notebook:** `01_dmcontrol_setup.ipynb`  \n",
        "**Phase:** 2 - DMControl Suite (Primary Benchmarks)  \n",
        "**Author:** Saurabh Jalendra  \n\n",
        "## Objectives\n",
        "1. Set up DeepMind Control Suite environments\n",
        "2. Create environment wrappers for RL training\n",
        "3. Validate Walker-walk, Cheetah-run, and Reacher environments\n",
        "4. Establish data collection utilities for higher-dimensional state spaces"
    ]),

    md_cell("---\n## 1. Imports and Configuration"),
    code_cell([
        '"""\n',
        'Cell: Imports and Configuration\n',
        'Purpose: Set up environment and imports for DMControl experiments\n',
        '"""\n',
        'import sys\n',
        'from pathlib import Path\n',
        'from typing import Dict, List, Tuple, Optional, Any, NamedTuple\n',
        'from dataclasses import dataclass, field\n',
        'from collections import defaultdict\n',
        'import warnings\n',
        'warnings.filterwarnings("ignore")\n\n',
        'import numpy as np\n',
        'import torch\n',
        'import torch.nn as nn\n',
        'import torch.nn.functional as F\n',
        'import matplotlib.pyplot as plt\n',
        'from tqdm.notebook import tqdm\n\n',
        '# Add project root to path\n',
        'PROJECT_ROOT = Path.cwd().parent\n',
        'sys.path.insert(0, str(PROJECT_ROOT / "src"))\n\n',
        'from utils import set_seed, get_device, MetricLogger, Timer, COLORS\n\n',
        '# Try importing dm_control\n',
        'try:\n',
        '    from dm_control import suite\n',
        '    from dm_control.rl.control import Environment\n',
        '    DM_CONTROL_AVAILABLE = True\n',
        '    print("dm_control imported successfully!")\n',
        'except ImportError:\n',
        '    DM_CONTROL_AVAILABLE = False\n',
        '    print("WARNING: dm_control not installed. Install with: pip install dm_control")\n\n',
        'SEED = 42\n',
        'set_seed(SEED)\n',
        'DEVICE = get_device()\n\n',
        '# Standard configuration\n',
        'CONFIG = {\n',
        '    "stoch_dim": 64,\n',
        '    "deter_dim": 512,\n',
        '    "hidden_dim": 512,\n',
        '    "batch_size": 32,\n',
        '    "seq_len": 20,\n',
        '    "lr": 3e-4,\n',
        '    "num_steps": 10000,\n',
        '}\n\n',
        'print(f"Device: {DEVICE}")\n',
        'print(f"Configuration: {CONFIG}")\n',
    ]),

    md_cell("---\n## 2. DMControl Environment Wrapper\n\nDeepMind Control Suite environments have a different API than Gymnasium.\nWe create a wrapper to standardize the interface."),
    code_cell([
        '"""\n',
        'Cell: DMControl Environment Wrapper\n',
        'Purpose: Wrap dm_control environments with Gymnasium-like interface\n',
        '"""\n\n',
        'class DMControlWrapper:\n',
        '    """\n',
        '    Wrapper for DeepMind Control Suite environments.\n',
        '    Provides a Gymnasium-like interface for consistency with Phase 1 code.\n',
        '    """\n',
        '    \n',
        '    def __init__(self, domain: str, task: str, seed: int = 42):\n',
        '        self.domain = domain\n',
        '        self.task = task\n',
        '        self._seed = seed\n',
        '        \n',
        '        if not DM_CONTROL_AVAILABLE:\n',
        '            raise ImportError("dm_control is not installed")\n',
        '        \n',
        '        # Create environment\n',
        '        self._env = suite.load(domain, task, task_kwargs={"random": seed})\n',
        '        \n',
        '        # Get observation and action specs\n',
        '        obs_spec = self._env.observation_spec()\n',
        '        action_spec = self._env.action_spec()\n',
        '        \n',
        '        # Calculate observation dimension (flatten all observations)\n',
        '        self._obs_keys = list(obs_spec.keys())\n',
        '        self._obs_dim = sum(\n',
        '            int(np.prod(obs_spec[key].shape)) \n',
        '            for key in self._obs_keys\n',
        '        )\n',
        '        \n',
        '        # Action dimension\n',
        '        self._action_dim = int(np.prod(action_spec.shape))\n',
        '        self._action_min = action_spec.minimum\n',
        '        self._action_max = action_spec.maximum\n',
        '        \n',
        '    @property\n',
        '    def obs_dim(self) -> int:\n',
        '        return self._obs_dim\n',
        '    \n',
        '    @property\n',
        '    def action_dim(self) -> int:\n',
        '        return self._action_dim\n',
        '    \n',
        '    def _flatten_obs(self, obs: dict) -> np.ndarray:\n',
        '        """Flatten observation dictionary to single array."""\n',
        '        obs_list = []\n',
        '        for key in self._obs_keys:\n',
        '            obs_list.append(obs[key].flatten())\n',
        '        return np.concatenate(obs_list).astype(np.float32)\n',
        '    \n',
        '    def reset(self) -> np.ndarray:\n',
        '        """Reset environment and return initial observation."""\n',
        '        time_step = self._env.reset()\n',
        '        return self._flatten_obs(time_step.observation)\n',
        '    \n',
        '    def step(self, action: np.ndarray) -> Tuple[np.ndarray, float, bool, dict]:\n',
        '        """Take action in environment."""\n',
        '        action = np.clip(action, self._action_min, self._action_max)\n',
        '        time_step = self._env.step(action)\n',
        '        \n',
        '        obs = self._flatten_obs(time_step.observation)\n',
        '        reward = time_step.reward if time_step.reward is not None else 0.0\n',
        '        done = time_step.last()\n',
        '        info = {"discount": time_step.discount}\n',
        '        \n',
        '        return obs, reward, done, info\n',
        '    \n',
        '    def sample_action(self) -> np.ndarray:\n',
        '        """Sample random action."""\n',
        '        return np.random.uniform(\n',
        '            self._action_min, \n',
        '            self._action_max, \n',
        '            size=self._action_dim\n',
        '        ).astype(np.float32)\n',
        '    \n',
        '    def close(self):\n',
        '        """Close environment."""\n',
        '        if hasattr(self._env, "close"):\n',
        '            self._env.close()\n\n\n',
        '# Test wrapper\n',
        'if DM_CONTROL_AVAILABLE:\n',
        '    print("Testing DMControlWrapper...")\n',
        '    test_env = DMControlWrapper("walker", "walk", seed=42)\n',
        '    print(f"  Walker-walk obs_dim: {test_env.obs_dim}")\n',
        '    print(f"  Walker-walk action_dim: {test_env.action_dim}")\n',
        '    obs = test_env.reset()\n',
        '    print(f"  Initial obs shape: {obs.shape}")\n',
        '    action = test_env.sample_action()\n',
        '    next_obs, reward, done, info = test_env.step(action)\n',
        '    print(f"  Step result - reward: {reward:.4f}, done: {done}")\n',
        '    test_env.close()\n',
        '    print("DMControlWrapper test passed!")\n',
        'else:\n',
        '    print("Skipping wrapper test - dm_control not available")\n',
    ]),

    md_cell("---\n## 3. Environment Specifications"),
    code_cell([
        '"""\n',
        'Cell: Environment Specifications\n',
        'Purpose: Document all DMControl environments\n',
        '"""\n\n',
        'DMCONTROL_ENVS = {\n',
        '    "walker_walk": {\n',
        '        "domain": "walker",\n',
        '        "task": "walk",\n',
        '        "obs_dim": 24,\n',
        '        "action_dim": 6,\n',
        '        "description": "Bipedal walker learning to walk",\n',
        '    },\n',
        '    "cheetah_run": {\n',
        '        "domain": "cheetah",\n',
        '        "task": "run",\n',
        '        "obs_dim": 17,\n',
        '        "action_dim": 6,\n',
        '        "description": "Cheetah learning to run fast",\n',
        '    },\n',
        '    "reacher_easy": {\n',
        '        "domain": "reacher",\n',
        '        "task": "easy",\n',
        '        "obs_dim": 6,\n',
        '        "action_dim": 2,\n',
        '        "description": "Arm reaching to easy targets",\n',
        '    },\n',
        '    "reacher_hard": {\n',
        '        "domain": "reacher",\n',
        '        "task": "hard",\n',
        '        "obs_dim": 6,\n',
        '        "action_dim": 2,\n',
        '        "description": "Arm reaching to hard targets",\n',
        '    },\n',
        '}\n\n',
        '# Display specifications\n',
        'print("DMControl Environment Specifications:")\n',
        'print("-" * 60)\n',
        'for env_name, spec in DMCONTROL_ENVS.items():\n',
        '    print(f"{env_name:15s}: obs_dim={spec[\"obs_dim\"]:2d}, action_dim={spec[\"action_dim\"]}")\n',
    ]),

    md_cell("---\n## 4. Episode Data Structure and Collection"),
    code_cell([
        '"""\n',
        'Cell: Episode Data Structure\n',
        'Purpose: Data structure for storing episode trajectories\n',
        '"""\n\n',
        '@dataclass\n',
        'class Episode:\n',
        '    """Container for episode data."""\n',
        '    observations: np.ndarray  # [T, obs_dim]\n',
        '    actions: np.ndarray       # [T, action_dim]\n',
        '    rewards: np.ndarray       # [T]\n',
        '    dones: np.ndarray         # [T]\n',
        '    \n',
        '    def __len__(self):\n',
        '        return len(self.observations)\n',
        '    \n',
        '    @property\n',
        '    def total_reward(self):\n',
        '        return self.rewards.sum()\n\n\n',
        'def collect_episode(env, policy="random", max_steps=1000):\n',
        '    """Collect a single episode from environment."""\n',
        '    observations, actions, rewards, dones = [], [], [], []\n',
        '    obs = env.reset()\n',
        '    \n',
        '    for _ in range(max_steps):\n',
        '        observations.append(obs)\n',
        '        action = env.sample_action()\n',
        '        next_obs, reward, done, info = env.step(action)\n',
        '        actions.append(action)\n',
        '        rewards.append(reward)\n',
        '        dones.append(done)\n',
        '        obs = next_obs\n',
        '        if done:\n',
        '            break\n',
        '    \n',
        '    return Episode(\n',
        '        observations=np.array(observations, dtype=np.float32),\n',
        '        actions=np.array(actions, dtype=np.float32),\n',
        '        rewards=np.array(rewards, dtype=np.float32),\n',
        '        dones=np.array(dones, dtype=np.float32),\n',
        '    )\n\n\n',
        'def collect_episodes(env, num_episodes=100, max_steps=1000, verbose=True):\n',
        '    """Collect multiple episodes."""\n',
        '    episodes = []\n',
        '    total_steps = 0\n',
        '    \n',
        '    iterator = tqdm(range(num_episodes), desc="Collecting") if verbose else range(num_episodes)\n',
        '    for _ in iterator:\n',
        '        episode = collect_episode(env, max_steps=max_steps)\n',
        '        episodes.append(episode)\n',
        '        total_steps += len(episode)\n',
        '    \n',
        '    if verbose:\n',
        '        print(f"Collected {num_episodes} episodes, {total_steps} total steps")\n',
        '        print(f"Average reward: {np.mean([ep.total_reward for ep in episodes]):.2f}")\n',
        '    \n',
        '    return episodes\n\n\n',
        'print("Episode data structures defined.")\n',
    ]),

    md_cell("---\n## 5. Environment Validation"),
    code_cell([
        '"""\n',
        'Cell: Environment Validation\n',
        'Purpose: Validate all environments work correctly\n',
        '"""\n\n',
        'if DM_CONTROL_AVAILABLE:\n',
        '    print("=" * 60)\n',
        '    print("VALIDATING ALL DMCONTROL ENVIRONMENTS")\n',
        '    print("=" * 60)\n',
        '    \n',
        '    for env_name, spec in DMCONTROL_ENVS.items():\n',
        '        print(f"\\n--- {env_name} ---")\n',
        '        try:\n',
        '            env = DMControlWrapper(spec["domain"], spec["task"], seed=42)\n',
        '            episode = collect_episode(env, max_steps=100)\n',
        '            print(f"  Observations shape: {episode.observations.shape}")\n',
        '            print(f"  Actions shape: {episode.actions.shape}")\n',
        '            print(f"  Total reward: {episode.total_reward:.2f}")\n',
        '            print(f"  Status: OK")\n',
        '            env.close()\n',
        '        except Exception as e:\n',
        '            print(f"  Status: FAILED - {e}")\n',
        '    \n',
        '    print("\\n" + "=" * 60)\n',
        '    print("VALIDATION COMPLETE")\n',
        'else:\n',
        '    print("Cannot validate - dm_control not installed")\n',
    ]),

    md_cell("---\n## 6. Summary"),
    code_cell([
        '"""\n',
        'Cell: Summary\n',
        '"""\n\n',
        'print("=" * 60)\n',
        'print("PHASE 2 SETUP COMPLETE")\n',
        'print("=" * 60)\n',
        'print("""\n',
        'Components Created:\n',
        '  1. DMControlWrapper - Gymnasium-like interface\n',
        '  2. Episode dataclass - Container for trajectory data\n',
        '  3. collect_episode/collect_episodes - Data collection\n',
        '\n',
        'Environments Ready:\n',
        '  - Walker-walk (obs=24, act=6)\n',
        '  - Cheetah-run (obs=17, act=6)\n',
        '  - Reacher-easy/hard (obs=6, act=2)\n',
        '\n',
        'Next: 02_walker_experiments.ipynb\n',
        '""")\n',
    ]),
]

nb1 = create_notebook(nb1_cells)
with open(os.path.join(base_path, "01_dmcontrol_setup.ipynb"), "w", encoding="utf-8") as f:
    json.dump(nb1, f, indent=1)
print("Created: 01_dmcontrol_setup.ipynb")
